{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prefix-Tuning 实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\transformers\\utils\\hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'input', 'instruction'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk(\"../data/alpaca_data_zh/\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': '4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。',\n",
       " 'input': '输入：4/16',\n",
       " 'instruction': '解释为什么以下分数等同于1/4'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': ['以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。',\n",
       "  '4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。',\n",
       "  '朱利叶斯·凯撒，又称尤利乌斯·恺撒（Julius Caesar）是古罗马的政治家、军事家和作家。他于公元前44年3月15日被刺杀。 \\n\\n根据历史记载，当时罗马元老院里一些参议员联合起来策划了对恺撒的刺杀行动，因为他们担心恺撒的统治将给罗马共和制带来威胁。在公元前44年3月15日（又称“3月的艾达之日”），恺撒去参加元老院会议时，被一群参议员包围并被攻击致死。据记载，他身中23刀，其中一刀最终致命。'],\n",
       " 'input': ['', '输入：4/16', ''],\n",
       " 'instruction': ['保持健康的三个提示。', '解释为什么以下分数等同于1/4', '朱利叶斯·凯撒是如何死亡的？']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='C:/Pretrained_model/LLM-Research/Llama-3___2-3B-Instruct', vocab_size=128000, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t128000: AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128001: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128004: AddedToken(\"<|finetune_right_pad_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128005: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128008: AddedToken(\"<|eom_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128010: AddedToken(\"<|python_tag|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128011: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128012: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128013: AddedToken(\"<|reserved_special_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128014: AddedToken(\"<|reserved_special_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128015: AddedToken(\"<|reserved_special_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128016: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128017: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128018: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128019: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128020: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128021: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128022: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128023: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128024: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128025: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128026: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128027: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128028: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128029: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128030: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128031: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128032: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128033: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128034: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128035: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128036: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128037: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128038: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128039: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128040: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128041: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128042: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128043: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128044: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128045: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128046: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128047: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128048: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128049: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128050: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128051: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128052: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128053: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128054: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128055: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128056: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128057: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128058: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128059: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128060: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128061: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128062: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128063: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128064: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128065: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128066: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128067: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128068: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128069: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128070: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128071: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128072: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128073: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128074: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128075: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128076: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128077: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128078: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128079: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128080: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128081: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128082: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128083: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128084: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128085: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128086: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128087: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128088: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128089: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128090: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128091: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128092: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128093: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128094: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128095: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128096: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128097: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128098: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128099: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128100: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128101: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128102: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128103: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128104: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128105: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128106: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128107: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128108: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128109: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128110: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128111: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128112: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128113: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128114: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128115: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128116: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128117: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128118: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128119: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128120: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128121: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128122: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128123: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128124: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128125: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128126: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128127: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128128: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128129: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128130: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128131: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128132: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128133: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128134: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128135: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128136: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128137: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128138: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128139: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128140: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128141: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128142: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128143: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128144: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128145: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128146: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128147: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128148: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128149: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128150: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128151: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128152: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128153: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128154: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128155: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128156: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128157: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128158: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128159: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128160: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128161: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128162: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128163: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128164: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128165: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128166: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128167: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128168: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128169: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128170: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128171: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128172: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128173: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128174: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128175: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128176: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128177: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128178: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128179: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128180: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128181: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128182: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128183: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128184: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128185: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128186: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128187: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128188: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128189: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128190: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128191: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128192: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128193: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128194: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128195: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128196: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128197: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128198: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128199: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128200: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128201: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128202: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128203: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128204: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128205: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128206: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128207: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128208: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128209: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128210: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128211: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128212: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128213: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128214: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128215: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128216: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128217: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128218: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128219: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128220: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128221: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128222: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128223: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128224: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128225: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128226: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128227: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128228: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128229: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128230: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128231: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128232: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128233: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128234: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128235: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128236: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128237: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128238: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128239: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128240: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128241: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128242: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128243: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128244: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128245: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128246: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128247: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128248: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128249: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128250: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128251: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128252: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128253: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128254: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128255: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"C:/Pretrained_model/LLM-Research/Llama-3___2-3B-Instruct\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.added_tokens_decoder[128004]\n",
    "\n",
    "# tokenizer.pad_token=tokenizer.added_tokens_decoder[128004]\n",
    "tokenizer.pad_token_id = 128004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "     # Step 1: Get the instruction, input, and output from the example\n",
    "    instruction = example['instruction']  # Instruction field (e.g., '解释为什么以下分数等同于1/4')\n",
    "    input_data = example['input']  # Input field (e.g., '4/16')\n",
    "    output_data = example['output']  # Output field (e.g., explanation for 4/16)\n",
    "\n",
    "    # Step 2: Construct the final prompt in the required format\n",
    "    final_prompt = (\n",
    "        f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"{instruction}\\nQuestion: {input_data}<|eot_id|>\\n\"\n",
    "        f\"<|start_header_id|>assistant<|end_header_id|>\\n\\nAnswer:\"\n",
    "    )\n",
    "    tokenized_prompt = tokenizer(final_prompt,add_special_tokens=False)\n",
    "    tokenized_response = tokenizer(output_data,add_special_tokens=False)\n",
    "    input_ids = tokenized_prompt[\"input_ids\"]+tokenized_response[\"input_ids\"]\n",
    "    attention_mask = tokenized_prompt[\"attention_mask\"] + tokenized_response[\"attention_mask\"]\n",
    "    labels = [-100] * len(tokenized_prompt[\"input_ids\"]) + tokenized_response[\"input_ids\"]\n",
    "\n",
    "\n",
    "    MAX_LENGTH = 384\n",
    "    # input_ids, attention_mask, labels = [], [], []\n",
    "    # instruction = tokenizer(\"\\n\".join([\"Human: \" + example[\"instruction\"], example[\"input\"]]).strip() + \"\\n\\nAssistant: \",add_special_tokens=False)\n",
    "    # response = tokenizer(example[\"output\"] ,add_special_tokens=False)\n",
    "    # input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]+ [tokenizer.eos_token_id]\n",
    "    # attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]+[1]\n",
    "    # labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"]+ [tokenizer.eos_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128004"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.eos_token_id\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128006,\n",
       "  882,\n",
       "  128007,\n",
       "  271,\n",
       "  118551,\n",
       "  113614,\n",
       "  9554,\n",
       "  126524,\n",
       "  46239,\n",
       "  9174,\n",
       "  14924,\n",
       "  25,\n",
       "  220,\n",
       "  128009,\n",
       "  198,\n",
       "  128006,\n",
       "  78191,\n",
       "  128007,\n",
       "  271,\n",
       "  16533,\n",
       "  25,\n",
       "  88852,\n",
       "  21043,\n",
       "  118551,\n",
       "  113614,\n",
       "  9554,\n",
       "  126524,\n",
       "  46239,\n",
       "  49543,\n",
       "  16,\n",
       "  13,\n",
       "  111505,\n",
       "  69978,\n",
       "  111006,\n",
       "  108726,\n",
       "  1811,\n",
       "  74257,\n",
       "  36827,\n",
       "  102210,\n",
       "  108562,\n",
       "  40265,\n",
       "  9554,\n",
       "  111006,\n",
       "  114253,\n",
       "  116051,\n",
       "  107471,\n",
       "  65782,\n",
       "  5486,\n",
       "  110774,\n",
       "  65782,\n",
       "  58291,\n",
       "  83994,\n",
       "  126503,\n",
       "  3922,\n",
       "  27327,\n",
       "  113096,\n",
       "  42399,\n",
       "  64209,\n",
       "  104473,\n",
       "  36651,\n",
       "  113614,\n",
       "  3922,\n",
       "  50285,\n",
       "  103229,\n",
       "  120044,\n",
       "  107079,\n",
       "  120772,\n",
       "  91495,\n",
       "  19361,\n",
       "  103129,\n",
       "  35304,\n",
       "  111689,\n",
       "  83747,\n",
       "  33014,\n",
       "  30358,\n",
       "  3490,\n",
       "  17,\n",
       "  13,\n",
       "  111020,\n",
       "  229,\n",
       "  120383,\n",
       "  120522,\n",
       "  102456,\n",
       "  1811,\n",
       "  74257,\n",
       "  36827,\n",
       "  102456,\n",
       "  11883,\n",
       "  17039,\n",
       "  118882,\n",
       "  9554,\n",
       "  107139,\n",
       "  105,\n",
       "  108171,\n",
       "  5486,\n",
       "  53610,\n",
       "  28873,\n",
       "  5486,\n",
       "  37087,\n",
       "  104858,\n",
       "  53953,\n",
       "  34208,\n",
       "  121496,\n",
       "  57942,\n",
       "  103,\n",
       "  96412,\n",
       "  33857,\n",
       "  103167,\n",
       "  9554,\n",
       "  111678,\n",
       "  101828,\n",
       "  103706,\n",
       "  102456,\n",
       "  53953,\n",
       "  3922,\n",
       "  111098,\n",
       "  103048,\n",
       "  45736,\n",
       "  117587,\n",
       "  122603,\n",
       "  121496,\n",
       "  57942,\n",
       "  103,\n",
       "  34208,\n",
       "  117041,\n",
       "  119008,\n",
       "  105610,\n",
       "  118551,\n",
       "  113614,\n",
       "  9554,\n",
       "  120522,\n",
       "  102456,\n",
       "  105369,\n",
       "  33565,\n",
       "  107,\n",
       "  3490,\n",
       "  18,\n",
       "  13,\n",
       "  124022,\n",
       "  94,\n",
       "  120379,\n",
       "  105843,\n",
       "  102780,\n",
       "  1811,\n",
       "  113136,\n",
       "  120379,\n",
       "  33764,\n",
       "  17792,\n",
       "  33014,\n",
       "  113614,\n",
       "  57237,\n",
       "  30356,\n",
       "  107693,\n",
       "  3922,\n",
       "  13153,\n",
       "  8107,\n",
       "  17792,\n",
       "  74257,\n",
       "  36827,\n",
       "  51611,\n",
       "  123076,\n",
       "  220,\n",
       "  22,\n",
       "  12,\n",
       "  23,\n",
       "  103036,\n",
       "  13646,\n",
       "  9554,\n",
       "  113136,\n",
       "  120379,\n",
       "  1811,\n",
       "  103424,\n",
       "  110085,\n",
       "  113136,\n",
       "  120379,\n",
       "  19361,\n",
       "  103129,\n",
       "  35304,\n",
       "  111689,\n",
       "  106196,\n",
       "  106208,\n",
       "  48634,\n",
       "  3922,\n",
       "  113096,\n",
       "  42399,\n",
       "  111006,\n",
       "  123843,\n",
       "  59464,\n",
       "  91495,\n",
       "  115890,\n",
       "  61633,\n",
       "  48634,\n",
       "  34208,\n",
       "  41914,\n",
       "  124920,\n",
       "  48634,\n",
       "  1811],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  88852,\n",
       "  21043,\n",
       "  118551,\n",
       "  113614,\n",
       "  9554,\n",
       "  126524,\n",
       "  46239,\n",
       "  49543,\n",
       "  16,\n",
       "  13,\n",
       "  111505,\n",
       "  69978,\n",
       "  111006,\n",
       "  108726,\n",
       "  1811,\n",
       "  74257,\n",
       "  36827,\n",
       "  102210,\n",
       "  108562,\n",
       "  40265,\n",
       "  9554,\n",
       "  111006,\n",
       "  114253,\n",
       "  116051,\n",
       "  107471,\n",
       "  65782,\n",
       "  5486,\n",
       "  110774,\n",
       "  65782,\n",
       "  58291,\n",
       "  83994,\n",
       "  126503,\n",
       "  3922,\n",
       "  27327,\n",
       "  113096,\n",
       "  42399,\n",
       "  64209,\n",
       "  104473,\n",
       "  36651,\n",
       "  113614,\n",
       "  3922,\n",
       "  50285,\n",
       "  103229,\n",
       "  120044,\n",
       "  107079,\n",
       "  120772,\n",
       "  91495,\n",
       "  19361,\n",
       "  103129,\n",
       "  35304,\n",
       "  111689,\n",
       "  83747,\n",
       "  33014,\n",
       "  30358,\n",
       "  3490,\n",
       "  17,\n",
       "  13,\n",
       "  111020,\n",
       "  229,\n",
       "  120383,\n",
       "  120522,\n",
       "  102456,\n",
       "  1811,\n",
       "  74257,\n",
       "  36827,\n",
       "  102456,\n",
       "  11883,\n",
       "  17039,\n",
       "  118882,\n",
       "  9554,\n",
       "  107139,\n",
       "  105,\n",
       "  108171,\n",
       "  5486,\n",
       "  53610,\n",
       "  28873,\n",
       "  5486,\n",
       "  37087,\n",
       "  104858,\n",
       "  53953,\n",
       "  34208,\n",
       "  121496,\n",
       "  57942,\n",
       "  103,\n",
       "  96412,\n",
       "  33857,\n",
       "  103167,\n",
       "  9554,\n",
       "  111678,\n",
       "  101828,\n",
       "  103706,\n",
       "  102456,\n",
       "  53953,\n",
       "  3922,\n",
       "  111098,\n",
       "  103048,\n",
       "  45736,\n",
       "  117587,\n",
       "  122603,\n",
       "  121496,\n",
       "  57942,\n",
       "  103,\n",
       "  34208,\n",
       "  117041,\n",
       "  119008,\n",
       "  105610,\n",
       "  118551,\n",
       "  113614,\n",
       "  9554,\n",
       "  120522,\n",
       "  102456,\n",
       "  105369,\n",
       "  33565,\n",
       "  107,\n",
       "  3490,\n",
       "  18,\n",
       "  13,\n",
       "  124022,\n",
       "  94,\n",
       "  120379,\n",
       "  105843,\n",
       "  102780,\n",
       "  1811,\n",
       "  113136,\n",
       "  120379,\n",
       "  33764,\n",
       "  17792,\n",
       "  33014,\n",
       "  113614,\n",
       "  57237,\n",
       "  30356,\n",
       "  107693,\n",
       "  3922,\n",
       "  13153,\n",
       "  8107,\n",
       "  17792,\n",
       "  74257,\n",
       "  36827,\n",
       "  51611,\n",
       "  123076,\n",
       "  220,\n",
       "  22,\n",
       "  12,\n",
       "  23,\n",
       "  103036,\n",
       "  13646,\n",
       "  9554,\n",
       "  113136,\n",
       "  120379,\n",
       "  1811,\n",
       "  103424,\n",
       "  110085,\n",
       "  113136,\n",
       "  120379,\n",
       "  19361,\n",
       "  103129,\n",
       "  35304,\n",
       "  111689,\n",
       "  106196,\n",
       "  106208,\n",
       "  48634,\n",
       "  3922,\n",
       "  113096,\n",
       "  42399,\n",
       "  111006,\n",
       "  123843,\n",
       "  59464,\n",
       "  91495,\n",
       "  115890,\n",
       "  61633,\n",
       "  48634,\n",
       "  34208,\n",
       "  41914,\n",
       "  124920,\n",
       "  48634,\n",
       "  1811]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_header_id|>user<|end_header_id|>\\n\\n保持健康的三个提示。\\nQuestion: <|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\nAnswer:以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_ds[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128006, 882, 128007, 271, 118551, 113614, 9554, 126524, 46239, 9174, 14924, 25, 220, 128009, 198, 128006, 78191, 128007, 271, 16533, 25, 88852, 21043, 118551, 113614, 9554, 126524, 46239, 49543, 16, 13, 111505, 69978, 111006, 108726, 1811, 74257, 36827, 102210, 108562, 40265, 9554, 111006, 114253, 116051, 107471, 65782, 5486, 110774, 65782, 58291, 83994, 126503, 3922, 27327, 113096, 42399, 64209, 104473, 36651, 113614, 3922, 50285, 103229, 120044, 107079, 120772, 91495, 19361, 103129, 35304, 111689, 83747, 33014, 30358, 3490, 17, 13, 111020, 229, 120383, 120522, 102456, 1811, 74257, 36827, 102456, 11883, 17039, 118882, 9554, 107139, 105, 108171, 5486, 53610, 28873, 5486, 37087, 104858, 53953, 34208, 121496, 57942, 103, 96412, 33857, 103167, 9554, 111678, 101828, 103706, 102456, 53953, 3922, 111098, 103048, 45736, 117587, 122603, 121496, 57942, 103, 34208, 117041, 119008, 105610, 118551, 113614, 9554, 120522, 102456, 105369, 33565, 107, 3490, 18, 13, 124022, 94, 120379, 105843, 102780, 1811, 113136, 120379, 33764, 17792, 33014, 113614, 57237, 30356, 107693, 3922, 13153, 8107, 17792, 74257, 36827, 51611, 123076, 220, 22, 12, 23, 103036, 13646, 9554, 113136, 120379, 1811, 103424, 110085, 113136, 120379, 19361, 103129, 35304, 111689, 106196, 106208, 48634, 3922, 113096, 42399, 111006, 123843, 59464, 91495, 115890, 61633, 48634, 34208, 41914, 124920, 48634, 1811], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 88852, 21043, 118551, 113614, 9554, 126524, 46239, 49543, 16, 13, 111505, 69978, 111006, 108726, 1811, 74257, 36827, 102210, 108562, 40265, 9554, 111006, 114253, 116051, 107471, 65782, 5486, 110774, 65782, 58291, 83994, 126503, 3922, 27327, 113096, 42399, 64209, 104473, 36651, 113614, 3922, 50285, 103229, 120044, 107079, 120772, 91495, 19361, 103129, 35304, 111689, 83747, 33014, 30358, 3490, 17, 13, 111020, 229, 120383, 120522, 102456, 1811, 74257, 36827, 102456, 11883, 17039, 118882, 9554, 107139, 105, 108171, 5486, 53610, 28873, 5486, 37087, 104858, 53953, 34208, 121496, 57942, 103, 96412, 33857, 103167, 9554, 111678, 101828, 103706, 102456, 53953, 3922, 111098, 103048, 45736, 117587, 122603, 121496, 57942, 103, 34208, 117041, 119008, 105610, 118551, 113614, 9554, 120522, 102456, 105369, 33565, 107, 3490, 18, 13, 124022, 94, 120379, 105843, 102780, 1811, 113136, 120379, 33764, 17792, 33014, 113614, 57237, 30356, 107693, 3922, 13153, 8107, 17792, 74257, 36827, 51611, 123076, 220, 22, 12, 23, 103036, 13646, 9554, 113136, 120379, 1811, 103424, 110085, 113136, 120379, 19361, 103129, 35304, 111689, 106196, 106208, 48634, 3922, 113096, 42399, 111006, 123843, 59464, 91495, 115890, 61633, 48634, 34208, 41914, 124920, 48634, 1811]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116.dll\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2b0d7d8dc34264a36ef882dffe4cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (16): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (17): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (20): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (21): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (26): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (27): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\"C:/Pretrained_model/LLM-Research/Llama-3___2-3B-Instruct\",low_cpu_mem_usage=True,torch_dtype=torch.half,device_map={'':torch.cuda.current_device()},load_in_8bit=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 3072]) torch.float16\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.0.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.1.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.2.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.3.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.4.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.5.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.6.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.7.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.8.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.9.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.10.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.11.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.12.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.13.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.14.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.15.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.16.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.17.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.18.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.19.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.20.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.21.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.22.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.23.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.24.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.25.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.26.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([1024, 3072]) torch.int8\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([3072, 3072]) torch.int8\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([8192, 3072]) torch.int8\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([3072, 8192]) torch.int8\n",
      "model.layers.27.input_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([3072]) torch.float16\n",
      "model.norm.weight torch.Size([3072]) torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name,para in model.named_parameters():\n",
    "    print(name,para.shape,para.dtype)\n",
    "    # 因为8bit 用于矩阵乘法， 所以全连接层是没有改成int8的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Step1 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Step2 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.base_model.model.model.embed_tokens.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.0.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.0.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.0.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.0.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.0.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.0.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.0.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.1.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.1.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.1.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.1.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.1.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.1.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.1.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.2.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.2.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.2.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.2.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.2.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.2.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.2.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.3.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.3.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.3.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.3.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.3.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.3.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.3.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.4.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.4.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.4.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.4.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.4.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.4.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.4.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.5.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.5.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.5.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.5.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.5.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.5.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.5.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.6.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.6.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.6.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.6.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.6.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.6.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.6.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.7.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.7.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.7.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.7.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.7.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.7.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.7.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.8.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.8.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.8.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.8.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.8.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.8.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.8.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.9.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.9.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.9.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.9.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.9.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.9.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.9.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.10.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.10.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.10.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.10.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.10.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.10.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.10.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.11.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.11.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.11.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.11.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.11.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.11.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.11.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.12.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.12.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.12.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.12.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.12.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.12.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.12.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.13.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.13.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.13.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.13.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.13.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.13.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.13.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.14.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.14.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.14.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.14.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.14.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.14.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.14.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.15.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.15.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.15.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.15.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.15.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.15.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.15.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.16.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.16.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.16.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.16.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.16.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.16.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.16.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.17.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.17.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.17.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.17.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.17.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.17.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.17.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.18.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.18.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.18.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.18.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.18.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.18.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.18.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.19.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.19.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.19.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.19.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.19.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.19.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.19.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.20.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.20.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.20.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.20.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.20.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.20.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.20.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.21.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.21.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.21.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.21.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.21.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.21.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.21.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.22.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.22.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.22.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.22.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.22.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.22.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.22.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.23.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.23.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.23.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.23.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.23.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.23.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.23.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.24.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.24.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.24.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.24.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.24.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.24.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.24.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.25.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.25.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.25.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.25.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.25.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.25.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.25.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.26.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.26.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.26.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.26.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.26.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.26.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.26.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.27.self_attn.k_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.base_model.model.model.layers.27.self_attn.o_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.27.mlp.gate_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.27.mlp.up_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.27.mlp.down_proj.weight torch.int8\n",
      "base_model.model.base_model.model.model.layers.27.input_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.layers.27.post_attention_layernorm.weight torch.float16\n",
      "base_model.model.base_model.model.model.norm.weight torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name,para in model.named_parameters():\n",
    "    print(name,para.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads() # 开启梯度检查点时，要执行该方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (1): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (2): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (3): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (4): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (5): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (6): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (7): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (8): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (9): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (10): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (11): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (12): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (13): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (14): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (15): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (16): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (17): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (18): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (19): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (20): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (21): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (22): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (23): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (24): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (25): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (26): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (27): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(tokenized_ds, batch_size=2, collate_fn=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipt = next(enumerate(dl))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 35075,     25, 111505,  69978, 113614,   9554, 126524,  46239,   3490,\n",
       "          72803,     25,    220,  88852,  21043, 118551, 113614,   9554, 126524,\n",
       "          46239,  49543,     16,     13, 111505,  69978, 111006, 108726,   1811,\n",
       "          74257,  36827, 102210, 108562,  40265,   9554, 111006, 114253, 116051,\n",
       "         107471,  65782,   5486, 110774,  65782,  58291,  83994, 126503,   3922,\n",
       "          27327, 113096,  42399,  64209, 104473,  36651, 113614,   3922,  50285,\n",
       "         103229, 120044, 107079, 120772,  91495,  19361, 103129,  35304, 111689,\n",
       "          83747,  33014,  30358,   3490,     17,     13, 111020,    229, 120383,\n",
       "         120522, 102456,   1811,  74257,  36827, 102456,  11883,  17039, 118882,\n",
       "           9554, 107139,    105, 108171,   5486,  53610,  28873,   5486,  37087,\n",
       "         104858,  53953,  34208, 121496,  57942,    103,  96412,  33857, 103167,\n",
       "           9554, 111678, 101828, 103706, 102456,  53953,   3922, 111098, 103048,\n",
       "          45736, 117587, 122603, 121496,  57942,    103,  34208, 117041, 119008,\n",
       "         105610, 118551, 113614,   9554, 120522, 102456, 105369,  33565,    107,\n",
       "           3490,     18,     13, 124022,     94, 120379, 105843, 102780,   1811,\n",
       "         113136, 120379,  33764,  17792,  33014, 113614,  57237,  30356, 107693,\n",
       "           3922,  13153,   8107,  17792,  74257,  36827,  51611, 123076,    220,\n",
       "             22,     12,     23, 103036,  13646,   9554, 113136, 120379,   1811,\n",
       "         103424, 110085, 113136, 120379,  19361, 103129,  35304, 111689, 106196,\n",
       "         106208,  48634,   3922, 113096,  42399, 111006, 123843,  59464,  91495,\n",
       "         115890,  61633,  48634,  34208,  41914, 124920,  48634,   1811, 128009],\n",
       "        [ 35075,     25,  86222,  69962, 113221,  88852,  17620,   9039,  50667,\n",
       "          42016,  35304,     16,     14,     19,    198,  32296,   5232,     19,\n",
       "             14,    845,    271,  72803,     25,    220,     19,     14,    845,\n",
       "          50667,  35304,     16,     14,     19,  21043, 104514,  98739,  74770,\n",
       "          95337,  17620,  17620,  45829,  17620, 103760,  72368,  21418,  23897,\n",
       "         104563,   9554, 112366,  35417,  95337,   9039,     19,   3922, 113885,\n",
       "          10110,     19, 123052,     19,   7705,     14,    320,    845, 123052,\n",
       "             19,   7705,     28,     16,     14,     19,   1811,  17620,   9039,\n",
       "           9554,  95337,  17620,  21043,  11883,  17620,  45829,  34208,  17620,\n",
       "         103760,  21418,  23897, 123025,   9554,  66776, 110260,  64531,   9039,\n",
       "           3922,  37507,  52563,  17620,   9039, 124434, 123025,   9554,  26592,\n",
       "         103138, 104514,  17620,   9039, 115827,  17905,  52563,  35287,  17620,\n",
       "          45829,  21418,  23897,  17620, 103760, 108905,  92776,  33655, 110835,\n",
       "           9039,  92672,  21418,  23897,  42016,  48044,  66776, 110260,  64531,\n",
       "           9039,   3922,  17620,   9039,   9554,  26592,  75863, 107774, 125992,\n",
       "           1811, 105123,     19,     14,    845,  59243,     16,     14,     19,\n",
       "          21043,  78640,  87502, 126644,  91386,  62543, 115707, 102378, 127150,\n",
       "           9554,  26592,  50021,  50667,   1811, 128009, 128004, 128004, 128004,\n",
       "         128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004,\n",
       "         128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004,\n",
       "         128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004, 128004]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,  88852,  21043, 118551, 113614,   9554, 126524,\n",
       "          46239,  49543,     16,     13, 111505,  69978, 111006, 108726,   1811,\n",
       "          74257,  36827, 102210, 108562,  40265,   9554, 111006, 114253, 116051,\n",
       "         107471,  65782,   5486, 110774,  65782,  58291,  83994, 126503,   3922,\n",
       "          27327, 113096,  42399,  64209, 104473,  36651, 113614,   3922,  50285,\n",
       "         103229, 120044, 107079, 120772,  91495,  19361, 103129,  35304, 111689,\n",
       "          83747,  33014,  30358,   3490,     17,     13, 111020,    229, 120383,\n",
       "         120522, 102456,   1811,  74257,  36827, 102456,  11883,  17039, 118882,\n",
       "           9554, 107139,    105, 108171,   5486,  53610,  28873,   5486,  37087,\n",
       "         104858,  53953,  34208, 121496,  57942,    103,  96412,  33857, 103167,\n",
       "           9554, 111678, 101828, 103706, 102456,  53953,   3922, 111098, 103048,\n",
       "          45736, 117587, 122603, 121496,  57942,    103,  34208, 117041, 119008,\n",
       "         105610, 118551, 113614,   9554, 120522, 102456, 105369,  33565,    107,\n",
       "           3490,     18,     13, 124022,     94, 120379, 105843, 102780,   1811,\n",
       "         113136, 120379,  33764,  17792,  33014, 113614,  57237,  30356, 107693,\n",
       "           3922,  13153,   8107,  17792,  74257,  36827,  51611, 123076,    220,\n",
       "             22,     12,     23, 103036,  13646,   9554, 113136, 120379,   1811,\n",
       "         103424, 110085, 113136, 120379,  19361, 103129,  35304, 111689, 106196,\n",
       "         106208,  48634,   3922, 113096,  42399, 111006, 123843,  59464,  91495,\n",
       "         115890,  61633,  48634,  34208,  41914, 124920,  48634,   1811, 128009],\n",
       "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,     19,     14,    845,\n",
       "          50667,  35304,     16,     14,     19,  21043, 104514,  98739,  74770,\n",
       "          95337,  17620,  17620,  45829,  17620, 103760,  72368,  21418,  23897,\n",
       "         104563,   9554, 112366,  35417,  95337,   9039,     19,   3922, 113885,\n",
       "          10110,     19, 123052,     19,   7705,     14,    320,    845, 123052,\n",
       "             19,   7705,     28,     16,     14,     19,   1811,  17620,   9039,\n",
       "           9554,  95337,  17620,  21043,  11883,  17620,  45829,  34208,  17620,\n",
       "         103760,  21418,  23897, 123025,   9554,  66776, 110260,  64531,   9039,\n",
       "           3922,  37507,  52563,  17620,   9039, 124434, 123025,   9554,  26592,\n",
       "         103138, 104514,  17620,   9039, 115827,  17905,  52563,  35287,  17620,\n",
       "          45829,  21418,  23897,  17620, 103760, 108905,  92776,  33655, 110835,\n",
       "           9039,  92672,  21418,  23897,  42016,  48044,  66776, 110260,  64531,\n",
       "           9039,   3922,  17620,   9039,   9554,  26592,  75863, 107774, 125992,\n",
       "           1811, 105123,     19,     14,    845,  59243,     16,     14,     19,\n",
       "          21043,  78640,  87502, 126644,  91386,  62543, 115707, 102378, 127150,\n",
       "           9554,  26592,  50021,  50667,   1811, 128009,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=tensor(1.5794, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[ 0.5571,  1.0928,  3.9297,  ..., -1.9951, -1.9951, -1.9951],\n",
       "         [-0.3359, -0.1638, -0.0285,  ..., -6.9531, -6.9531, -6.9531],\n",
       "         [ 1.3213,  2.5742,  0.8296,  ..., -2.0332, -2.0332, -2.0332],\n",
       "         ...,\n",
       "         [ 4.6133,  4.9062,  2.1191,  ...,  0.6650,  0.6650,  0.6650],\n",
       "         [ 2.7559,  2.3516,  2.9180,  ..., -2.4141, -2.4160, -2.4160],\n",
       "         [-0.8667, -1.5869, -4.8164,  ...,  2.5117,  2.5117,  2.5117]],\n",
       "\n",
       "        [[ 0.5571,  1.0928,  3.9297,  ..., -1.9951, -1.9951, -1.9951],\n",
       "         [-0.3359, -0.1638, -0.0285,  ..., -6.9531, -6.9531, -6.9531],\n",
       "         [ 2.3262,  2.6504,  0.1075,  ..., -1.0654, -1.0645, -1.0645],\n",
       "         ...,\n",
       "         [ 2.5469,  4.4570,  2.0723,  ..., -0.5947, -0.5952, -0.5952],\n",
       "         [ 2.7051,  4.2344,  1.6777,  ..., -0.1199, -0.1201, -0.1203],\n",
       "         [ 2.7812,  4.2031,  1.7471,  ...,  0.2922,  0.2920,  0.2920]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[ 7.2109,  3.5352,  1.6113,  ...,  1.8086, -1.0781, -1.4893],\n",
       "          [ 2.3398,  2.1660,  0.9492,  ...,  0.9644, -0.0422, -1.3291],\n",
       "          [-3.6934, -2.0840, -1.6836,  ...,  1.5527, -1.3535, -0.9424],\n",
       "          ...,\n",
       "          [-4.9258,  1.1436,  1.3633,  ...,  1.7715, -0.9458, -1.0977],\n",
       "          [-0.8145,  0.8560,  1.7070,  ...,  0.7388, -0.7793, -0.8452],\n",
       "          [ 6.0703, -3.1289,  2.2090,  ...,  0.6553, -1.2988, -0.8901]],\n",
       "\n",
       "         [[-1.1709,  1.0371,  1.0107,  ...,  1.5273, -2.0117,  1.1523],\n",
       "          [ 0.2450, -0.6963,  0.3381,  ...,  1.9004, -1.5264,  0.2334],\n",
       "          [ 3.4883, -3.0820,  1.8105,  ...,  1.6660, -1.0127,  1.0283],\n",
       "          ...,\n",
       "          [-2.0488, -1.2539, -2.8516,  ...,  1.3262, -1.2520,  0.5210],\n",
       "          [-0.1209, -0.2949, -0.2489,  ...,  1.2695, -0.8794,  0.0157],\n",
       "          [-1.6338, -1.9199, -1.1367,  ...,  1.3271, -1.2822,  1.4736]],\n",
       "\n",
       "         [[-0.2068, -0.1317,  0.6992,  ...,  0.2487, -1.0127, -0.4932],\n",
       "          [-6.8672,  3.1387,  5.0938,  ...,  2.2598, -2.4043,  0.7900],\n",
       "          [-1.0244, -0.4370,  0.4287,  ...,  0.4995, -0.8271, -1.2178],\n",
       "          ...,\n",
       "          [ 0.8916, -0.2114, -0.8438,  ...,  0.2773, -2.0957, -1.0400],\n",
       "          [ 5.1094, -1.0098, -2.8789,  ...,  1.0029, -1.1650,  0.9697],\n",
       "          [ 0.6680, -1.5723,  0.0918,  ...,  1.0430, -1.0234, -1.1738]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.9268,  2.0156,  1.4355,  ..., -1.5469,  1.2598, -0.7944],\n",
       "          [-2.2363,  3.0117,  1.9668,  ..., -0.2443,  0.6128, -1.6514],\n",
       "          [-0.5205,  0.0000,  1.3105,  ..., -1.7070,  1.2891, -1.0488],\n",
       "          ...,\n",
       "          [ 1.1201,  0.8306, -1.5869,  ..., -1.9893,  1.2314, -0.7363],\n",
       "          [ 1.5381,  0.9312, -1.7100,  ...,  0.2520,  0.9141, -0.8608],\n",
       "          [-0.0764, -0.6465, -0.9014,  ..., -1.1104,  1.4775, -1.6445]],\n",
       "\n",
       "         [[-5.7852, -3.8672,  3.9121,  ..., -2.3340,  1.2900, -1.8467],\n",
       "          [-0.7480, -0.6729,  0.9775,  ..., -1.7520,  1.7432, -2.1719],\n",
       "          [ 3.8008, -0.1406,  5.3906,  ..., -0.3826,  2.8145, -1.0566],\n",
       "          ...,\n",
       "          [ 4.6133, -3.2266, -5.4531,  ..., -1.2559,  2.5410, -1.5654],\n",
       "          [ 0.3225,  0.0111, -0.4941,  ..., -0.8105,  1.3252, -1.8662],\n",
       "          [-6.1758,  3.3398, -0.8926,  ..., -1.7705,  0.9346, -1.6904]],\n",
       "\n",
       "         [[ 3.2520, -3.1680,  2.0215,  ..., -1.0713, -0.6323,  1.6348],\n",
       "          [ 0.7695, -2.1855, -1.1113,  ..., -0.5659,  0.1197, -1.0693],\n",
       "          [ 1.9316,  0.8608, -0.0498,  ..., -1.0781, -0.4072,  1.5889],\n",
       "          ...,\n",
       "          [-5.3555, -2.5332, -0.9395,  ..., -1.2334, -0.8735,  1.4805],\n",
       "          [ 0.0609, -0.3113,  0.6226,  ..., -0.1628,  2.1289, -0.8735],\n",
       "          [ 2.5898,  2.7637,  2.5547,  ..., -1.6885,  0.1390,  3.0645]]],\n",
       "\n",
       "\n",
       "        [[[ 7.2109,  3.5352,  1.6113,  ...,  1.8086, -1.0781, -1.4893],\n",
       "          [ 2.3398,  2.1660,  0.9492,  ...,  0.9644, -0.0422, -1.3291],\n",
       "          [-4.0938, -1.7559, -1.6562,  ...,  1.3174, -0.9600, -1.0557],\n",
       "          ...,\n",
       "          [-4.7031,  2.0391,  0.8423,  ...,  0.8394, -0.8955, -0.4990],\n",
       "          [ 2.0586, -0.3765,  0.8804,  ...,  0.8394, -0.8955, -0.4990],\n",
       "          [ 6.9336, -2.5527,  0.5449,  ...,  0.8394, -0.8955, -0.4990]],\n",
       "\n",
       "         [[-1.1709,  1.0371,  1.0107,  ...,  1.5273, -2.0117,  1.1523],\n",
       "          [ 0.2450, -0.6963,  0.3381,  ...,  1.9004, -1.5264,  0.2334],\n",
       "          [ 3.9102, -3.0742,  1.8643,  ...,  0.9375, -1.0713, -0.9512],\n",
       "          ...,\n",
       "          [-2.7207, -1.3535, -4.2930,  ...,  0.4243, -0.6475,  1.4307],\n",
       "          [-5.1992, -3.5605, -3.6699,  ...,  0.4243, -0.6475,  1.4307],\n",
       "          [-2.8965, -3.5332, -1.4941,  ...,  0.4243, -0.6475,  1.4307]],\n",
       "\n",
       "         [[-0.2068, -0.1317,  0.6992,  ...,  0.2487, -1.0127, -0.4932],\n",
       "          [-6.8672,  3.1387,  5.0938,  ...,  2.2598, -2.4043,  0.7900],\n",
       "          [-0.1100,  0.0555, -0.1299,  ..., -0.0527, -1.0049, -1.8066],\n",
       "          ...,\n",
       "          [-0.2712, -1.9863,  0.9023,  ...,  0.0685, -0.3542, -2.8789],\n",
       "          [ 0.1990, -0.3469,  1.4102,  ...,  0.0685, -0.3542, -2.8789],\n",
       "          [ 0.4863,  1.5088,  1.3193,  ...,  0.0685, -0.3542, -2.8789]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.9268,  2.0156,  1.4355,  ..., -1.5469,  1.2598, -0.7944],\n",
       "          [-2.2363,  3.0117,  1.9668,  ..., -0.2443,  0.6128, -1.6514],\n",
       "          [-0.2422, -0.0583,  1.1338,  ..., -1.6377,  1.0127, -0.8887],\n",
       "          ...,\n",
       "          [ 0.7432, -0.0408, -1.3242,  ..., -1.4678,  1.3936, -0.6260],\n",
       "          [-0.2371, -0.3933, -0.8291,  ..., -1.4678,  1.3936, -0.6260],\n",
       "          [-1.0000, -0.4990,  0.0181,  ..., -1.4678,  1.3936, -0.6260]],\n",
       "\n",
       "         [[-5.7852, -3.8672,  3.9121,  ..., -2.3340,  1.2900, -1.8467],\n",
       "          [-0.7480, -0.6729,  0.9775,  ..., -1.7520,  1.7432, -2.1719],\n",
       "          [ 3.9082, -0.9150,  5.3750,  ..., -1.0264,  1.9951, -0.7642],\n",
       "          ...,\n",
       "          [ 6.2930, -4.7539, -8.4297,  ..., -1.2881,  0.9780, -1.3037],\n",
       "          [-2.1875, -0.9995, -6.1914,  ..., -1.2881,  0.9780, -1.3037],\n",
       "          [-8.6641,  3.3789, -1.3281,  ..., -1.2881,  0.9780, -1.3037]],\n",
       "\n",
       "         [[ 3.2520, -3.1680,  2.0215,  ..., -1.0713, -0.6323,  1.6348],\n",
       "          [ 0.7695, -2.1855, -1.1113,  ..., -0.5659,  0.1197, -1.0693],\n",
       "          [ 1.9541,  0.9575,  0.0625,  ..., -1.0020, -0.3647,  1.6953],\n",
       "          ...,\n",
       "          [-7.2891, -0.8740, -2.0039,  ..., -1.3750, -0.1290,  4.0703],\n",
       "          [-4.1016,  1.9141, -0.0490,  ..., -1.3750, -0.1290,  4.0703],\n",
       "          [ 2.8594,  3.5020,  1.9258,  ..., -1.3750, -0.1290,  4.0703]]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), tensor([[[[ 6.1493e-02,  3.2654e-02, -2.4261e-02,  ...,  3.9368e-02,\n",
       "           -3.9062e-02, -4.3671e-02],\n",
       "          [-1.1078e-02,  3.0945e-02,  4.7729e-02,  ...,  3.1250e-02,\n",
       "            8.2474e-03,  3.8879e-02],\n",
       "          [-9.5154e-02, -4.2389e-02, -1.1032e-02,  ..., -9.1476e-03,\n",
       "            4.4464e-02,  3.4332e-03],\n",
       "          ...,\n",
       "          [ 1.2154e-02, -3.7750e-02,  3.6011e-02,  ...,  3.0151e-02,\n",
       "            1.8494e-02, -1.0461e-01],\n",
       "          [-2.0340e-02,  3.4088e-02,  2.2766e-02,  ..., -1.2032e-02,\n",
       "           -2.8381e-02,  1.5442e-01],\n",
       "          [ 4.3579e-02,  1.7456e-02, -5.3406e-02,  ...,  3.2959e-02,\n",
       "            3.3203e-02,  8.7158e-02]],\n",
       "\n",
       "         [[ 6.9763e-02,  4.6295e-02,  1.9974e-02,  ...,  4.0131e-02,\n",
       "           -6.6895e-02, -8.8867e-02],\n",
       "          [ 9.5749e-03,  1.5511e-02, -1.8494e-02,  ..., -2.0538e-02,\n",
       "            8.3084e-03, -8.4305e-03],\n",
       "          [-5.3467e-02, -4.8737e-02, -1.1420e-01,  ..., -5.9662e-02,\n",
       "           -4.0863e-02,  3.5645e-02],\n",
       "          ...,\n",
       "          [ 2.5452e-02,  1.1670e-01, -3.7231e-02,  ..., -4.6997e-02,\n",
       "           -3.7415e-02, -1.1505e-01],\n",
       "          [-2.9312e-02, -1.0414e-02,  2.4319e-04,  ..., -6.9141e-04,\n",
       "            2.6108e-02,  2.6398e-02],\n",
       "          [-3.9825e-02, -1.2901e-02, -5.6213e-02,  ..., -7.8369e-02,\n",
       "           -3.5339e-02,  1.9043e-02]],\n",
       "\n",
       "         [[ 4.0710e-02, -5.5847e-02,  2.0599e-03,  ..., -4.1718e-02,\n",
       "            1.9012e-02, -2.2156e-02],\n",
       "          [-1.3268e-02, -9.4223e-03,  7.5035e-03,  ...,  7.2594e-03,\n",
       "           -8.5678e-03, -7.0068e-02],\n",
       "          [ 2.7542e-02,  2.0416e-02,  2.6154e-02,  ...,  1.9104e-02,\n",
       "           -3.4332e-02,  1.2276e-02],\n",
       "          ...,\n",
       "          [ 1.6037e-02, -2.5955e-02, -9.1934e-03,  ...,  1.7029e-02,\n",
       "           -8.1787e-02, -2.5406e-02],\n",
       "          [-5.7770e-02, -3.1067e-02,  1.7380e-02,  ..., -8.3069e-02,\n",
       "           -3.5980e-02,  4.1656e-02],\n",
       "          [ 2.4490e-02,  1.3809e-02, -7.1350e-02,  ..., -5.5756e-02,\n",
       "           -2.8809e-02, -2.6627e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.9785e-02, -1.0353e-02,  5.4398e-03,  ...,  6.1607e-03,\n",
       "            2.5208e-02,  2.3102e-02],\n",
       "          [-2.1881e-02,  1.4206e-02,  1.0056e-02,  ...,  1.9236e-03,\n",
       "            3.0160e-04, -6.6162e-02],\n",
       "          [-2.3087e-02, -2.1332e-02,  2.1179e-02,  ...,  8.7891e-03,\n",
       "           -1.9211e-02, -1.2474e-02],\n",
       "          ...,\n",
       "          [-1.4946e-02,  2.3331e-02,  1.2383e-02,  ..., -4.4495e-02,\n",
       "            2.9358e-02, -2.0905e-02],\n",
       "          [ 1.4290e-02,  4.8645e-02, -9.5673e-03,  ...,  1.8806e-03,\n",
       "            2.7161e-03, -1.7424e-03],\n",
       "          [ 1.4122e-02, -4.5532e-02, -1.5625e-02,  ...,  2.4368e-02,\n",
       "            1.0840e-01,  2.4597e-02]],\n",
       "\n",
       "         [[-8.1360e-02,  2.0630e-02, -3.8361e-02,  ...,  6.0913e-02,\n",
       "            8.2275e-02,  9.1934e-03],\n",
       "          [ 1.5396e-02, -1.3649e-02, -5.0545e-04,  ..., -5.4092e-03,\n",
       "           -6.5994e-03, -2.2568e-02],\n",
       "          [-6.9214e-02, -1.5417e-01,  5.5145e-02,  ...,  1.3969e-02,\n",
       "           -1.6537e-03,  7.1526e-03],\n",
       "          ...,\n",
       "          [-4.6326e-02, -1.1360e-02, -6.3171e-02,  ...,  3.7384e-02,\n",
       "           -7.3547e-02,  9.6436e-02],\n",
       "          [ 5.9570e-02,  2.5139e-03,  3.1471e-04,  ...,  3.2444e-03,\n",
       "            3.0609e-02,  1.6098e-02],\n",
       "          [-8.5266e-02, -1.8921e-02, -4.8676e-02,  ...,  3.2867e-02,\n",
       "            3.9917e-02, -4.0497e-02]],\n",
       "\n",
       "         [[-9.3536e-03, -2.2430e-02, -2.0248e-02,  ..., -6.0081e-03,\n",
       "           -4.2725e-03, -4.7699e-02],\n",
       "          [-6.7863e-03, -1.0544e-02,  4.4830e-02,  ..., -3.7415e-02,\n",
       "            1.5228e-02,  1.0941e-02],\n",
       "          [ 5.1147e-02, -2.1545e-02,  3.1433e-02,  ..., -2.6642e-02,\n",
       "           -6.3934e-03,  3.2227e-02],\n",
       "          ...,\n",
       "          [-2.8366e-02,  2.6794e-02, -2.8946e-02,  ...,  9.1614e-02,\n",
       "            7.1960e-02,  2.8885e-02],\n",
       "          [ 4.0474e-03,  8.4305e-03,  4.3701e-02,  ..., -1.2543e-02,\n",
       "           -1.7105e-02,  8.5413e-05],\n",
       "          [ 1.9012e-02,  1.1658e-02,  4.4434e-02,  ..., -2.4048e-02,\n",
       "           -3.5583e-02,  3.2196e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 6.1493e-02,  3.2654e-02, -2.4261e-02,  ...,  3.9368e-02,\n",
       "           -3.9062e-02, -4.3671e-02],\n",
       "          [-1.1078e-02,  3.0945e-02,  4.7729e-02,  ...,  3.1250e-02,\n",
       "            8.2474e-03,  3.8879e-02],\n",
       "          [-3.4302e-02,  7.4081e-03,  8.1665e-02,  ...,  4.8637e-03,\n",
       "            1.0669e-01,  1.5345e-03],\n",
       "          ...,\n",
       "          [-1.6785e-02, -8.0643e-03,  6.2927e-02,  ...,  4.4174e-03,\n",
       "            7.8354e-03,  3.6713e-02],\n",
       "          [-1.6785e-02, -8.0643e-03,  6.2927e-02,  ...,  4.4174e-03,\n",
       "            7.8354e-03,  3.6713e-02],\n",
       "          [-1.6785e-02, -8.0643e-03,  6.2927e-02,  ...,  4.4174e-03,\n",
       "            7.8354e-03,  3.6713e-02]],\n",
       "\n",
       "         [[ 6.9763e-02,  4.6295e-02,  1.9974e-02,  ...,  4.0131e-02,\n",
       "           -6.6895e-02, -8.8867e-02],\n",
       "          [ 9.5749e-03,  1.5511e-02, -1.8494e-02,  ..., -2.0538e-02,\n",
       "            8.3084e-03, -8.4305e-03],\n",
       "          [ 6.0394e-02, -5.5176e-02,  3.3417e-02,  ...,  3.3478e-02,\n",
       "           -3.4363e-02,  9.1309e-02],\n",
       "          ...,\n",
       "          [ 3.6896e-02,  3.4302e-02, -3.7262e-02,  ..., -8.0261e-03,\n",
       "            3.5763e-03,  2.3788e-02],\n",
       "          [ 3.6896e-02,  3.4302e-02, -3.7262e-02,  ..., -8.0261e-03,\n",
       "            3.5763e-03,  2.3788e-02],\n",
       "          [ 3.6896e-02,  3.4302e-02, -3.7262e-02,  ..., -8.0261e-03,\n",
       "            3.5763e-03,  2.3788e-02]],\n",
       "\n",
       "         [[ 4.0710e-02, -5.5847e-02,  2.0599e-03,  ..., -4.1718e-02,\n",
       "            1.9012e-02, -2.2156e-02],\n",
       "          [-1.3268e-02, -9.4223e-03,  7.5035e-03,  ...,  7.2594e-03,\n",
       "           -8.5678e-03, -7.0068e-02],\n",
       "          [-8.6365e-03,  7.2815e-02,  3.5492e-02,  ..., -1.2283e-02,\n",
       "           -8.6731e-02,  1.3618e-02],\n",
       "          ...,\n",
       "          [-5.7411e-03, -1.0176e-03, -3.3684e-03,  ..., -1.4473e-02,\n",
       "           -1.5411e-02, -1.2115e-02],\n",
       "          [-5.7411e-03, -1.0176e-03, -3.3684e-03,  ..., -1.4473e-02,\n",
       "           -1.5411e-02, -1.2115e-02],\n",
       "          [-5.7411e-03, -1.0176e-03, -3.3684e-03,  ..., -1.4473e-02,\n",
       "           -1.5411e-02, -1.2115e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.9785e-02, -1.0353e-02,  5.4398e-03,  ...,  6.1607e-03,\n",
       "            2.5208e-02,  2.3102e-02],\n",
       "          [-2.1881e-02,  1.4206e-02,  1.0056e-02,  ...,  1.9236e-03,\n",
       "            3.0160e-04, -6.6162e-02],\n",
       "          [-1.9272e-02,  3.0991e-02,  1.3180e-03,  ...,  1.4145e-02,\n",
       "           -7.7858e-03, -3.5095e-02],\n",
       "          ...,\n",
       "          [-3.4046e-03, -3.0441e-02, -2.1301e-02,  ...,  1.4629e-03,\n",
       "            7.6790e-03,  3.5553e-03],\n",
       "          [-3.4046e-03, -3.0441e-02, -2.1301e-02,  ...,  1.4629e-03,\n",
       "            7.6790e-03,  3.5553e-03],\n",
       "          [-3.4046e-03, -3.0441e-02, -2.1301e-02,  ...,  1.4629e-03,\n",
       "            7.6790e-03,  3.5553e-03]],\n",
       "\n",
       "         [[-8.1360e-02,  2.0630e-02, -3.8361e-02,  ...,  6.0913e-02,\n",
       "            8.2275e-02,  9.1934e-03],\n",
       "          [ 1.5396e-02, -1.3649e-02, -5.0545e-04,  ..., -5.4092e-03,\n",
       "           -6.5994e-03, -2.2568e-02],\n",
       "          [-1.0516e-01, -1.2854e-01,  8.7402e-02,  ..., -8.1421e-02,\n",
       "            5.7373e-02,  5.5145e-02],\n",
       "          ...,\n",
       "          [-2.3178e-02, -1.2825e-02, -3.5583e-02,  ..., -7.6256e-03,\n",
       "            8.3237e-03, -3.1891e-02],\n",
       "          [-2.3178e-02, -1.2825e-02, -3.5583e-02,  ..., -7.6256e-03,\n",
       "            8.3237e-03, -3.1891e-02],\n",
       "          [-2.3178e-02, -1.2825e-02, -3.5583e-02,  ..., -7.6256e-03,\n",
       "            8.3237e-03, -3.1891e-02]],\n",
       "\n",
       "         [[-9.3536e-03, -2.2430e-02, -2.0248e-02,  ..., -6.0081e-03,\n",
       "           -4.2725e-03, -4.7699e-02],\n",
       "          [-6.7863e-03, -1.0544e-02,  4.4830e-02,  ..., -3.7415e-02,\n",
       "            1.5228e-02,  1.0941e-02],\n",
       "          [-1.0345e-02, -4.6143e-02, -6.2408e-03,  ...,  3.4302e-02,\n",
       "           -6.8130e-03,  1.0841e-02],\n",
       "          ...,\n",
       "          [ 4.8401e-02, -5.6091e-02, -3.3325e-02,  ...,  2.6608e-04,\n",
       "            4.0558e-02,  6.9046e-03],\n",
       "          [ 4.8401e-02, -5.6091e-02, -3.3325e-02,  ...,  2.6608e-04,\n",
       "            4.0558e-02,  6.9046e-03],\n",
       "          [ 4.8401e-02, -5.6091e-02, -3.3325e-02,  ...,  2.6608e-04,\n",
       "            4.0558e-02,  6.9046e-03]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 8.4531e+00,  2.4727e+00,  2.8047e+00,  ..., -1.5491e-01,\n",
       "            5.0439e-01, -8.4961e-01],\n",
       "          [ 1.3203e+00,  1.4150e+00,  1.6934e+00,  ...,  7.9102e-01,\n",
       "           -9.1357e-01,  1.4026e-01],\n",
       "          [-3.6035e+00,  3.0859e+00,  2.9980e+00,  ...,  7.0117e-01,\n",
       "           -9.6240e-01, -5.6592e-01],\n",
       "          ...,\n",
       "          [-5.9062e+00,  3.7344e+00, -3.0410e+00,  ...,  5.1709e-01,\n",
       "           -4.3701e-01, -1.6328e+00],\n",
       "          [-2.4102e+00,  2.3027e+00, -1.8320e+00,  ...,  1.3171e-01,\n",
       "           -6.1865e-01, -8.3203e-01],\n",
       "          [ 1.8223e+00,  1.2959e+00, -9.1602e-01,  ...,  9.6094e-01,\n",
       "           -3.1738e-01, -8.4082e-01]],\n",
       "\n",
       "         [[-2.9258e+00, -5.2422e+00,  2.4199e+00,  ..., -2.7393e-01,\n",
       "            2.2266e+00,  1.4404e+00],\n",
       "          [-2.1211e+00,  8.5400e-01,  1.0645e+00,  ...,  1.4368e-01,\n",
       "            9.7314e-01,  5.4413e-02],\n",
       "          [-4.8438e+00,  5.1133e+00,  2.6426e+00,  ...,  9.0186e-01,\n",
       "            2.0605e+00,  5.4297e-01],\n",
       "          ...,\n",
       "          [ 9.0547e+00, -1.7520e+00, -5.4805e+00,  ...,  8.9258e-01,\n",
       "            2.5605e+00, -1.5808e-01],\n",
       "          [ 5.1758e+00,  2.1484e+00, -2.8945e+00,  ..., -1.1993e-01,\n",
       "            2.2051e+00,  2.0020e+00],\n",
       "          [ 2.2598e+00,  3.9141e+00, -8.3203e-01,  ...,  1.2781e-01,\n",
       "            2.4023e+00,  6.4355e-01]],\n",
       "\n",
       "         [[-1.6377e+00, -2.6973e+00, -2.0176e+00,  ..., -1.5283e+00,\n",
       "            1.7861e+00,  2.1021e-01],\n",
       "          [-7.3145e-01,  7.1289e-01, -2.8589e-01,  ..., -2.4866e-01,\n",
       "            4.9072e-01, -4.2603e-01],\n",
       "          [-1.1660e+00,  2.8008e+00, -2.3438e-02,  ..., -7.1338e-01,\n",
       "            9.7705e-01, -7.1350e-02],\n",
       "          ...,\n",
       "          [ 3.0352e+00, -6.5723e-01,  8.4521e-01,  ..., -1.2549e+00,\n",
       "            8.6963e-01, -8.5889e-01],\n",
       "          [ 5.1221e-01,  4.0063e-01, -1.9836e-03,  ..., -2.8809e-01,\n",
       "            1.3076e+00, -8.5938e-01],\n",
       "          [-2.8735e-01,  7.8027e-01, -1.3037e+00,  ..., -1.2627e+00,\n",
       "            7.8760e-01, -2.3230e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.8047e+00, -7.9541e-01,  2.2363e+00,  ...,  1.1846e+00,\n",
       "            4.6143e-01, -2.2285e+00],\n",
       "          [ 3.8086e-02,  1.0195e+00, -3.5693e-01,  ...,  7.9004e-01,\n",
       "            3.1006e-02, -9.2383e-01],\n",
       "          [-5.4062e+00,  3.2793e+00, -2.2656e+00,  ...,  6.8408e-01,\n",
       "           -1.0439e+00, -2.0410e+00],\n",
       "          ...,\n",
       "          [-2.4746e+00,  2.0664e+00,  1.5049e+00,  ...,  1.3696e-01,\n",
       "            1.6388e-02, -2.2383e+00],\n",
       "          [ 2.6758e+00,  3.5742e+00,  3.2266e+00,  ...,  1.5820e+00,\n",
       "            1.3416e-01, -2.0000e+00],\n",
       "          [ 6.2969e+00,  2.8223e+00,  3.5547e+00,  ...,  2.4023e-01,\n",
       "           -9.6045e-01, -2.2656e+00]],\n",
       "\n",
       "         [[ 6.5352e+00,  7.2852e-01,  2.9395e+00,  ..., -2.9810e-01,\n",
       "           -2.2715e+00,  5.1904e-01],\n",
       "          [ 7.3486e-01,  9.3311e-01,  6.9580e-01,  ...,  4.2389e-02,\n",
       "           -9.4385e-01, -3.3374e-01],\n",
       "          [-1.4648e+00,  2.5527e+00,  1.1309e+00,  ...,  2.4902e-01,\n",
       "           -2.0098e+00,  1.9043e-01],\n",
       "          ...,\n",
       "          [-4.0469e+00,  2.8809e+00, -4.3652e-01,  ...,  3.0884e-01,\n",
       "           -2.5234e+00,  3.7573e-01],\n",
       "          [-2.7783e-01,  2.5352e+00, -1.1993e-02,  ...,  5.5084e-03,\n",
       "           -1.9990e+00, -5.3857e-01],\n",
       "          [ 3.8301e+00,  2.5820e+00,  2.0352e+00,  ...,  7.4707e-01,\n",
       "           -2.3379e+00, -4.2334e-01]],\n",
       "\n",
       "         [[ 6.8477e+00, -2.7012e+00, -5.6406e+00,  ...,  4.2822e-01,\n",
       "            9.1992e-01,  2.9346e-01],\n",
       "          [ 3.1250e-02, -4.6411e-01, -7.0312e-01,  ...,  6.7285e-01,\n",
       "           -5.4395e-01, -3.9209e-01],\n",
       "          [-2.8008e+00,  1.4492e+00, -1.8105e+00,  ...,  1.2871e+00,\n",
       "           -2.1252e-01,  4.5947e-01],\n",
       "          ...,\n",
       "          [-3.3633e+00, -2.2188e+00,  3.3477e+00,  ...,  2.1328e+00,\n",
       "            1.1436e+00, -9.3555e-01],\n",
       "          [-3.1128e-03, -2.3242e-01,  3.1396e-01,  ...,  1.8438e+00,\n",
       "           -7.6172e-01, -7.7490e-01],\n",
       "          [ 1.9209e+00,  1.3486e+00,  5.7861e-01,  ...,  2.0488e+00,\n",
       "           -1.0078e+00,  2.6294e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 8.4531e+00,  2.4727e+00,  2.8047e+00,  ..., -1.5491e-01,\n",
       "            5.0439e-01, -8.4961e-01],\n",
       "          [ 1.3203e+00,  1.4150e+00,  1.6934e+00,  ...,  7.9102e-01,\n",
       "           -9.1357e-01,  1.4026e-01],\n",
       "          [-4.3320e+00,  2.6758e+00,  4.1719e+00,  ...,  7.0947e-01,\n",
       "           -1.2480e+00, -1.1406e+00],\n",
       "          ...,\n",
       "          [-4.6055e+00,  3.9844e+00, -3.0723e+00,  ...,  6.3721e-01,\n",
       "           -1.6284e-01, -6.9238e-01],\n",
       "          [ 5.4980e-01,  3.4258e+00, -1.8604e+00,  ...,  6.8018e-01,\n",
       "           -8.2336e-02, -7.3584e-01],\n",
       "          [ 5.3438e+00,  6.3672e-01,  1.6016e-01,  ...,  6.1133e-01,\n",
       "           -5.6458e-02, -7.5830e-01]],\n",
       "\n",
       "         [[-2.9258e+00, -5.2422e+00,  2.4199e+00,  ..., -2.7393e-01,\n",
       "            2.2266e+00,  1.4404e+00],\n",
       "          [-2.1211e+00,  8.5400e-01,  1.0645e+00,  ...,  1.4368e-01,\n",
       "            9.7314e-01,  5.4413e-02],\n",
       "          [-5.2031e+00,  4.8047e+00,  3.2363e+00,  ..., -5.9668e-01,\n",
       "            2.1484e+00,  1.3984e+00],\n",
       "          ...,\n",
       "          [ 4.0078e+00,  3.3594e-01, -2.7715e+00,  ...,  8.9453e-01,\n",
       "            1.8730e+00, -8.7061e-01],\n",
       "          [ 6.0820e+00,  3.9551e+00, -2.0312e+00,  ...,  9.4434e-01,\n",
       "            1.9258e+00, -8.7109e-01],\n",
       "          [ 2.4551e+00,  5.2422e+00, -3.0078e-01,  ...,  1.0371e+00,\n",
       "            1.9551e+00, -1.0020e+00]],\n",
       "\n",
       "         [[-1.6377e+00, -2.6973e+00, -2.0176e+00,  ..., -1.5283e+00,\n",
       "            1.7861e+00,  2.1021e-01],\n",
       "          [-7.3145e-01,  7.1289e-01, -2.8589e-01,  ..., -2.4866e-01,\n",
       "            4.9072e-01, -4.2603e-01],\n",
       "          [-1.3291e+00,  2.3359e+00,  2.9053e-01,  ..., -1.0811e+00,\n",
       "            7.0020e-01, -2.6416e-01],\n",
       "          ...,\n",
       "          [ 1.9375e+00,  5.0879e-01,  8.6230e-01,  ...,  3.1860e-01,\n",
       "            1.7761e-01, -8.8330e-01],\n",
       "          [ 1.7246e+00,  2.1816e+00, -1.9141e-01,  ...,  2.6562e-01,\n",
       "            2.4561e-01, -8.6182e-01],\n",
       "          [-1.4355e-01,  2.5273e+00, -1.2236e+00,  ...,  2.5537e-01,\n",
       "            2.8979e-01, -7.3779e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.8047e+00, -7.9541e-01,  2.2363e+00,  ...,  1.1846e+00,\n",
       "            4.6143e-01, -2.2285e+00],\n",
       "          [ 3.8086e-02,  1.0195e+00, -3.5693e-01,  ...,  7.9004e-01,\n",
       "            3.1006e-02, -9.2383e-01],\n",
       "          [-5.6406e+00,  3.5469e+00, -2.4766e+00,  ...,  1.1318e+00,\n",
       "           -1.3389e+00, -2.0977e+00],\n",
       "          ...,\n",
       "          [-2.7930e+00,  2.8906e+00,  1.5000e+00,  ...,  1.3702e-02,\n",
       "            3.4668e-02, -1.9316e+00],\n",
       "          [ 3.2578e+00,  3.3203e+00,  2.6836e+00,  ...,  2.1381e-03,\n",
       "            5.7983e-02, -1.9883e+00],\n",
       "          [ 6.4531e+00,  1.6582e+00,  2.7148e+00,  ..., -1.0590e-02,\n",
       "            1.0419e-01, -2.0254e+00]],\n",
       "\n",
       "         [[ 6.5352e+00,  7.2852e-01,  2.9395e+00,  ..., -2.9810e-01,\n",
       "           -2.2715e+00,  5.1904e-01],\n",
       "          [ 7.3486e-01,  9.3311e-01,  6.9580e-01,  ...,  4.2389e-02,\n",
       "           -9.4385e-01, -3.3374e-01],\n",
       "          [-2.3750e+00,  2.9941e+00,  3.9722e-01,  ...,  3.1021e-02,\n",
       "           -1.9990e+00, -6.3184e-01],\n",
       "          ...,\n",
       "          [-3.2109e+00,  3.5156e+00, -2.0176e+00,  ...,  8.8574e-01,\n",
       "           -1.9023e+00, -8.7744e-01],\n",
       "          [-3.8159e-01,  3.8086e+00, -9.2676e-01,  ...,  7.6270e-01,\n",
       "           -1.9775e+00, -1.0117e+00],\n",
       "          [ 3.0156e+00,  1.6016e+00,  6.2158e-01,  ...,  6.9678e-01,\n",
       "           -2.0098e+00, -1.0381e+00]],\n",
       "\n",
       "         [[ 6.8477e+00, -2.7012e+00, -5.6406e+00,  ...,  4.2822e-01,\n",
       "            9.1992e-01,  2.9346e-01],\n",
       "          [ 3.1250e-02, -4.6411e-01, -7.0312e-01,  ...,  6.7285e-01,\n",
       "           -5.4395e-01, -3.9209e-01],\n",
       "          [-3.5020e+00,  1.9531e+00, -2.2871e+00,  ...,  1.3184e+00,\n",
       "           -6.2842e-01, -4.2505e-01],\n",
       "          ...,\n",
       "          [-3.3145e+00, -1.4951e+00,  1.9863e+00,  ...,  1.8662e+00,\n",
       "            3.4351e-01, -7.0117e-01],\n",
       "          [ 1.9824e-01,  3.5400e-01,  8.9014e-01,  ...,  1.9834e+00,\n",
       "            4.3091e-01, -6.7090e-01],\n",
       "          [ 3.7129e+00,  2.0762e+00, -6.8555e-01,  ...,  1.9980e+00,\n",
       "            3.7231e-01, -6.7676e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 8.3618e-02, -1.7834e-01,  3.9612e-02,  ..., -7.1167e-02,\n",
       "            1.8689e-01,  1.0272e-01],\n",
       "          [ 3.3966e-02,  6.1760e-03,  3.2104e-02,  ...,  3.2043e-03,\n",
       "            1.7441e-02,  3.8910e-02],\n",
       "          [-9.0698e-02,  1.4168e-02, -6.9336e-02,  ..., -5.1117e-02,\n",
       "            2.5952e-01,  1.5894e-01],\n",
       "          ...,\n",
       "          [ 3.3752e-02,  9.3323e-02,  6.7566e-02,  ..., -7.3547e-02,\n",
       "           -1.3708e-01,  2.2049e-02],\n",
       "          [-9.9121e-02,  2.7512e-02,  2.5009e-02,  ..., -1.4258e-01,\n",
       "           -3.3630e-02,  5.8044e-02],\n",
       "          [ 8.1658e-06,  7.7515e-02,  2.0630e-01,  ..., -1.1804e-01,\n",
       "           -1.1456e-04,  6.1981e-02]],\n",
       "\n",
       "         [[ 7.7133e-03, -6.3416e-02,  5.8502e-02,  ..., -1.3831e-01,\n",
       "           -1.0605e-02, -3.3234e-02],\n",
       "          [-3.4302e-02, -3.7842e-02,  1.6510e-02,  ..., -3.2166e-02,\n",
       "           -1.2848e-02, -2.8934e-03],\n",
       "          [ 1.4209e-01,  8.2458e-02, -1.3013e-01,  ..., -2.0538e-02,\n",
       "            1.6675e-01, -9.2651e-02],\n",
       "          ...,\n",
       "          [-1.3660e-01, -6.4941e-02,  5.0306e-04,  ...,  2.9526e-02,\n",
       "           -4.4250e-02,  1.7853e-02],\n",
       "          [-3.3817e-03,  3.1799e-02,  9.0942e-03,  ...,  8.8013e-02,\n",
       "            5.1971e-02,  2.7939e-02],\n",
       "          [ 2.9564e-03,  1.1084e-01, -1.9678e-01,  ...,  2.1000e-03,\n",
       "           -7.4585e-02,  1.3794e-01]],\n",
       "\n",
       "         [[ 2.9370e-01,  4.8279e-02,  7.1045e-02,  ..., -7.3853e-02,\n",
       "            3.2593e-02, -7.2327e-02],\n",
       "          [ 7.5012e-02,  1.2741e-02, -4.1351e-03,  ..., -2.4612e-02,\n",
       "           -2.0386e-02,  4.4067e-02],\n",
       "          [-1.8034e-03, -2.5520e-03, -2.4139e-02,  ..., -3.9642e-02,\n",
       "            2.3193e-03, -4.1626e-02],\n",
       "          ...,\n",
       "          [ 1.9543e-01,  1.2978e-02,  7.0801e-02,  ...,  2.4521e-02,\n",
       "           -8.8379e-02,  2.3010e-02],\n",
       "          [-2.7435e-02, -1.1932e-01,  1.1658e-01,  ..., -1.1725e-01,\n",
       "           -9.8328e-02, -2.7390e-02],\n",
       "          [ 1.0181e-01, -2.0789e-01, -2.0984e-01,  ..., -1.2439e-01,\n",
       "           -1.8539e-02,  1.2408e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.8737e-02, -8.4961e-02, -3.9368e-03,  ..., -3.5828e-02,\n",
       "           -5.8563e-02, -3.5400e-02],\n",
       "          [-3.7964e-02,  6.2927e-02, -3.5309e-02,  ..., -1.8585e-02,\n",
       "            3.3569e-02, -1.0353e-02],\n",
       "          [ 6.8298e-02, -2.3056e-02,  2.3315e-02,  ..., -3.4599e-03,\n",
       "            2.1252e-01,  4.0283e-02],\n",
       "          ...,\n",
       "          [ 3.0487e-02,  1.3513e-01,  3.9276e-02,  ...,  1.0211e-01,\n",
       "           -1.5808e-02, -3.1311e-02],\n",
       "          [ 5.1849e-02,  8.6792e-02, -1.0216e-02,  ...,  4.8584e-02,\n",
       "            5.9113e-02, -9.3140e-02],\n",
       "          [-7.1838e-02,  5.0018e-02, -5.7098e-02,  ..., -7.9346e-03,\n",
       "            1.4966e-01, -5.8899e-02]],\n",
       "\n",
       "         [[ 8.3160e-03, -1.2512e-01,  3.3112e-02,  ..., -6.1401e-02,\n",
       "            1.9250e-01,  1.3542e-02],\n",
       "          [-7.1907e-03, -5.0079e-02, -2.4185e-02,  ...,  1.8101e-03,\n",
       "            2.4002e-02, -2.1469e-02],\n",
       "          [-1.2733e-02, -2.9517e-01,  4.5441e-02,  ..., -6.3171e-02,\n",
       "            3.3374e-01,  5.0079e-02],\n",
       "          ...,\n",
       "          [ 9.5276e-02, -8.3130e-02,  1.1255e-01,  ...,  1.2195e-01,\n",
       "            1.9177e-01,  9.7198e-03],\n",
       "          [-9.9854e-02,  9.2773e-03, -1.1139e-01,  ..., -8.1055e-02,\n",
       "           -1.4404e-01,  9.1553e-02],\n",
       "          [ 2.9572e-02, -1.1377e-01,  4.3221e-03,  ..., -9.5032e-02,\n",
       "            1.5823e-02,  7.2021e-02]],\n",
       "\n",
       "         [[ 1.2018e-01, -4.7699e-02,  7.9102e-02,  ...,  4.2786e-02,\n",
       "            3.4363e-02,  1.4429e-01],\n",
       "          [-3.6621e-02,  2.6054e-03,  2.0721e-02,  ..., -6.7825e-03,\n",
       "           -4.7333e-02, -5.8105e-02],\n",
       "          [-5.2795e-02,  4.2816e-02, -1.8921e-02,  ...,  1.1520e-02,\n",
       "           -2.4854e-01, -2.0386e-02],\n",
       "          ...,\n",
       "          [ 2.5366e-01,  4.2145e-02,  2.7075e-01,  ..., -3.6530e-02,\n",
       "           -1.0992e-01,  3.3643e-01],\n",
       "          [-8.9569e-03, -4.0436e-02,  9.2651e-02,  ...,  1.5527e-01,\n",
       "           -1.5373e-02,  1.5857e-01],\n",
       "          [ 1.1670e-01,  9.5520e-02, -8.6914e-02,  ...,  1.4478e-01,\n",
       "           -4.5074e-02, -2.6047e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 8.3618e-02, -1.7834e-01,  3.9612e-02,  ..., -7.1167e-02,\n",
       "            1.8689e-01,  1.0272e-01],\n",
       "          [ 3.3966e-02,  6.1760e-03,  3.2104e-02,  ...,  3.2043e-03,\n",
       "            1.7441e-02,  3.8910e-02],\n",
       "          [ 9.9365e-02, -1.7685e-02, -6.8237e-02,  ..., -5.8655e-02,\n",
       "            1.4111e-01, -7.5928e-02],\n",
       "          ...,\n",
       "          [-4.6631e-02, -1.1646e-01, -4.4708e-03,  ...,  1.4984e-02,\n",
       "            1.2445e-01, -7.7148e-02],\n",
       "          [-4.1199e-02, -8.7646e-02, -1.7670e-02,  ...,  1.3329e-02,\n",
       "            1.2347e-01, -5.7465e-02],\n",
       "          [-4.9957e-02, -1.1450e-01, -1.7990e-02,  ...,  2.6581e-02,\n",
       "            2.0032e-01, -5.5328e-02]],\n",
       "\n",
       "         [[ 7.7133e-03, -6.3416e-02,  5.8502e-02,  ..., -1.3831e-01,\n",
       "           -1.0605e-02, -3.3234e-02],\n",
       "          [-3.4302e-02, -3.7842e-02,  1.6510e-02,  ..., -3.2166e-02,\n",
       "           -1.2848e-02, -2.8934e-03],\n",
       "          [-1.1505e-01, -1.3901e-02,  9.6558e-02,  ..., -1.6260e-01,\n",
       "            1.9507e-01,  1.1823e-01],\n",
       "          ...,\n",
       "          [-2.9846e-02, -5.1994e-03, -4.0344e-02,  ..., -2.4887e-02,\n",
       "           -5.0690e-02,  3.2288e-02],\n",
       "          [-2.8763e-02, -2.4811e-02, -4.3243e-02,  ..., -2.7802e-02,\n",
       "           -7.1228e-02,  3.9429e-02],\n",
       "          [-3.3325e-02,  2.1286e-03, -2.9495e-02,  ..., -2.8168e-02,\n",
       "           -8.3801e-02,  6.7902e-03]],\n",
       "\n",
       "         [[ 2.9370e-01,  4.8279e-02,  7.1045e-02,  ..., -7.3853e-02,\n",
       "            3.2593e-02, -7.2327e-02],\n",
       "          [ 7.5012e-02,  1.2741e-02, -4.1351e-03,  ..., -2.4612e-02,\n",
       "           -2.0386e-02,  4.4067e-02],\n",
       "          [-9.9182e-02, -2.3096e-01,  1.1798e-01,  ...,  1.4307e-01,\n",
       "            1.3452e-01, -6.7200e-02],\n",
       "          ...,\n",
       "          [ 3.0273e-02, -2.7808e-01,  8.1863e-03,  ..., -2.0752e-02,\n",
       "            6.1874e-03,  1.8280e-02],\n",
       "          [-6.0081e-03, -2.6367e-01,  1.7838e-02,  ..., -2.5024e-02,\n",
       "            2.7130e-02,  9.5215e-03],\n",
       "          [-7.8201e-03, -2.6807e-01,  8.5831e-03,  ..., -5.0964e-02,\n",
       "            2.3499e-02,  1.4946e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.8737e-02, -8.4961e-02, -3.9368e-03,  ..., -3.5828e-02,\n",
       "           -5.8563e-02, -3.5400e-02],\n",
       "          [-3.7964e-02,  6.2927e-02, -3.5309e-02,  ..., -1.8585e-02,\n",
       "            3.3569e-02, -1.0353e-02],\n",
       "          [-9.6142e-05, -5.3101e-02,  2.5772e-02,  ..., -7.5623e-02,\n",
       "            1.5320e-01,  6.8604e-02],\n",
       "          ...,\n",
       "          [-5.0621e-03, -2.4872e-03, -6.6528e-02,  ..., -4.6753e-02,\n",
       "            5.3680e-02, -1.1591e-01],\n",
       "          [-1.4091e-02, -1.9669e-02, -9.5764e-02,  ..., -3.6469e-02,\n",
       "            5.3497e-02, -1.0651e-01],\n",
       "          [-1.6953e-02, -2.6993e-02, -1.2598e-01,  ..., -4.7516e-02,\n",
       "            2.9327e-02, -1.2756e-01]],\n",
       "\n",
       "         [[ 8.3160e-03, -1.2512e-01,  3.3112e-02,  ..., -6.1401e-02,\n",
       "            1.9250e-01,  1.3542e-02],\n",
       "          [-7.1907e-03, -5.0079e-02, -2.4185e-02,  ...,  1.8101e-03,\n",
       "            2.4002e-02, -2.1469e-02],\n",
       "          [ 2.7771e-03,  1.3914e-03, -4.3976e-02,  ...,  1.8311e-02,\n",
       "            1.7236e-01, -1.4355e-01],\n",
       "          ...,\n",
       "          [-1.3110e-01, -1.4694e-02, -8.2214e-02,  ..., -5.9998e-02,\n",
       "            4.1797e-01, -1.4259e-02],\n",
       "          [-1.2079e-01, -3.5217e-02, -9.6924e-02,  ..., -9.6130e-02,\n",
       "            4.1016e-01, -2.9633e-02],\n",
       "          [-9.6558e-02, -5.9326e-02, -1.1981e-01,  ..., -1.1407e-01,\n",
       "            4.2676e-01, -3.5950e-02]],\n",
       "\n",
       "         [[ 1.2018e-01, -4.7699e-02,  7.9102e-02,  ...,  4.2786e-02,\n",
       "            3.4363e-02,  1.4429e-01],\n",
       "          [-3.6621e-02,  2.6054e-03,  2.0721e-02,  ..., -6.7825e-03,\n",
       "           -4.7333e-02, -5.8105e-02],\n",
       "          [-4.4830e-02, -7.5623e-02,  1.6223e-01,  ..., -4.1229e-02,\n",
       "           -8.2153e-02, -1.2036e-01],\n",
       "          ...,\n",
       "          [-3.5370e-02,  1.5350e-02,  5.7434e-02,  ...,  1.3025e-01,\n",
       "            6.4758e-02,  1.1523e-01],\n",
       "          [-3.8239e-02,  1.3107e-02,  7.3730e-02,  ...,  1.4380e-01,\n",
       "            4.7760e-02,  1.2634e-01],\n",
       "          [-2.4033e-02,  9.5291e-03,  9.2407e-02,  ...,  1.5906e-01,\n",
       "            4.1443e-02,  1.3220e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.2848e-02,  3.9124e-02,  1.7731e-02,  ..., -1.4844e-01,\n",
       "           -1.9189e-01,  1.0029e+00],\n",
       "          [-3.0137e+00, -1.8340e+00, -1.1934e+00,  ...,  7.2754e-01,\n",
       "           -9.6008e-02, -4.1250e+00],\n",
       "          [ 2.0410e-01, -2.6660e+00, -3.4473e+00,  ..., -7.8027e-01,\n",
       "           -4.1284e-01, -4.2773e+00],\n",
       "          ...,\n",
       "          [ 3.9180e+00, -2.9297e+00,  3.9453e+00,  ...,  4.2065e-01,\n",
       "            7.5439e-01, -4.0195e+00],\n",
       "          [ 8.3008e-03, -8.6816e-01,  1.9746e+00,  ..., -3.1323e-01,\n",
       "            1.1357e+00, -4.1992e+00],\n",
       "          [-1.0830e+00, -1.4160e-02,  1.5254e+00,  ..., -4.5361e-01,\n",
       "           -1.3809e+00, -3.7422e+00]],\n",
       "\n",
       "         [[-6.2943e-03,  1.7014e-02, -2.7466e-04,  ...,  7.9575e-03,\n",
       "           -2.6794e-02,  4.2603e-02],\n",
       "          [ 2.3496e+00,  3.7549e-01, -6.5479e-01,  ..., -1.1182e+00,\n",
       "            1.6201e+00,  1.7529e-01],\n",
       "          [ 1.4746e+00, -9.7559e-01, -1.6553e+00,  ..., -6.4941e-01,\n",
       "            1.3457e+00,  8.7939e-01],\n",
       "          ...,\n",
       "          [-3.0098e+00,  6.5918e-01,  2.4414e+00,  ..., -4.4287e-01,\n",
       "            4.4092e-01,  7.7197e-01],\n",
       "          [-1.0742e+00, -1.3984e+00, -1.0938e-01,  ..., -1.4343e-01,\n",
       "            2.4438e-01,  1.3757e-01],\n",
       "          [-3.1689e-01, -1.9365e+00, -7.1143e-01,  ..., -2.2736e-02,\n",
       "           -7.8760e-01,  8.8440e-02]],\n",
       "\n",
       "         [[ 2.1179e-02, -1.2169e-02,  4.7150e-03,  ...,  1.0498e-01,\n",
       "           -8.5266e-02, -2.5854e-01],\n",
       "          [ 3.6367e+00,  1.4238e+00,  6.7383e-02,  ..., -7.8271e-01,\n",
       "            5.6396e-02,  1.0088e+00],\n",
       "          [-8.5742e-01,  2.5156e+00, -2.9648e+00,  ..., -9.8828e-01,\n",
       "           -4.1309e-01,  1.9619e+00],\n",
       "          ...,\n",
       "          [-3.7402e+00,  2.7051e-01,  7.3633e-01,  ..., -6.8604e-01,\n",
       "           -7.2852e-01,  2.7002e-01],\n",
       "          [-1.0674e+00,  2.9668e+00,  7.0850e-01,  ..., -9.3652e-01,\n",
       "            2.1301e-01,  1.4092e+00],\n",
       "          [ 1.6348e+00,  1.9795e+00,  2.0430e+00,  ..., -1.0371e+00,\n",
       "            5.9473e-01,  1.9043e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.1463e-03, -1.1002e-02, -1.5564e-02,  ...,  3.0322e-01,\n",
       "           -5.9912e-01,  4.6387e-01],\n",
       "          [ 5.4297e-01, -4.3848e-01,  4.3945e-01,  ...,  1.3594e+00,\n",
       "            4.8906e+00, -2.7617e+00],\n",
       "          [ 3.6865e-01,  4.1089e-01,  6.9482e-01,  ...,  2.5293e+00,\n",
       "            2.0059e+00, -5.3047e+00],\n",
       "          ...,\n",
       "          [-2.2791e-01, -9.6069e-02, -2.2717e-01,  ...,  1.1396e+00,\n",
       "            3.7441e+00, -3.0200e-01],\n",
       "          [-2.0227e-01,  2.9419e-01, -1.9922e-01,  ..., -3.4180e+00,\n",
       "            3.8887e+00, -3.9062e+00],\n",
       "          [ 2.0044e-01,  7.1973e-01, -6.6699e-01,  ..., -2.8750e+00,\n",
       "            2.5879e+00, -3.6367e+00]],\n",
       "\n",
       "         [[-1.9165e-02,  3.7766e-04, -5.8746e-03,  ...,  7.2852e-01,\n",
       "           -1.3892e-01, -8.3618e-02],\n",
       "          [ 9.1992e-01,  1.1279e-01,  1.3193e+00,  ..., -1.5645e+00,\n",
       "           -7.3633e-01,  3.1177e-01],\n",
       "          [ 4.2627e-01, -1.4380e-01, -3.4937e-01,  ..., -1.6445e+00,\n",
       "           -2.1074e+00,  5.3223e-01],\n",
       "          ...,\n",
       "          [-9.8096e-01,  6.6895e-01, -1.2686e+00,  ..., -2.5117e+00,\n",
       "           -1.8057e+00, -3.3008e+00],\n",
       "          [-4.1235e-01,  3.0225e-01,  1.4392e-01,  ..., -1.8838e+00,\n",
       "           -2.4590e+00, -3.6777e+00],\n",
       "          [ 7.0801e-01, -8.1641e-01,  1.1543e+00,  ..., -2.7305e+00,\n",
       "            3.9917e-01,  1.1328e+00]],\n",
       "\n",
       "         [[-1.0437e-02,  3.8361e-02,  1.6632e-03,  ..., -3.9575e-01,\n",
       "           -5.1208e-02, -2.8503e-02],\n",
       "          [ 2.1562e+00, -1.0488e+00,  2.3066e+00,  ..., -2.0972e-01,\n",
       "           -7.3438e-01, -1.4941e-01],\n",
       "          [ 4.4688e+00, -3.0039e+00,  3.3867e+00,  ...,  6.0791e-01,\n",
       "           -7.8955e-01,  3.9917e-02],\n",
       "          ...,\n",
       "          [-2.8398e+00, -1.2588e+00, -3.7656e+00,  ...,  9.1553e-01,\n",
       "           -4.7559e-01, -3.4180e-01],\n",
       "          [-1.0684e+00, -1.3340e+00, -1.2217e+00,  ...,  9.0820e-01,\n",
       "           -8.0200e-02,  4.3408e-01],\n",
       "          [-1.9639e+00, -2.0430e+00, -1.3887e+00,  ..., -7.8711e-01,\n",
       "            1.1909e-02,  3.5938e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.2848e-02,  3.9124e-02,  1.7731e-02,  ..., -1.4844e-01,\n",
       "           -1.9189e-01,  1.0029e+00],\n",
       "          [-3.0137e+00, -1.8340e+00, -1.1934e+00,  ...,  7.2754e-01,\n",
       "           -9.6008e-02, -4.1250e+00],\n",
       "          [ 1.4355e-01, -2.8594e+00, -3.3164e+00,  ...,  9.6973e-01,\n",
       "           -7.5830e-01, -4.4883e+00],\n",
       "          ...,\n",
       "          [ 2.9844e+00, -1.5098e+00,  2.7617e+00,  ..., -1.7603e-01,\n",
       "           -1.8945e+00, -2.5801e+00],\n",
       "          [ 1.3232e+00, -1.4941e+00,  1.9785e+00,  ..., -2.2791e-01,\n",
       "           -2.0000e+00, -2.5781e+00],\n",
       "          [-1.5684e+00, -5.2637e-01,  2.6660e-01,  ..., -2.2510e-01,\n",
       "           -2.0488e+00, -2.5254e+00]],\n",
       "\n",
       "         [[-6.2943e-03,  1.7014e-02, -2.7466e-04,  ...,  7.9575e-03,\n",
       "           -2.6794e-02,  4.2603e-02],\n",
       "          [ 2.3496e+00,  3.7549e-01, -6.5479e-01,  ..., -1.1182e+00,\n",
       "            1.6201e+00,  1.7529e-01],\n",
       "          [ 1.0283e+00, -1.0859e+00, -1.9014e+00,  ..., -1.3794e-01,\n",
       "            2.3657e-01,  8.2617e-01],\n",
       "          ...,\n",
       "          [-1.3965e+00,  5.4590e-01,  5.7520e-01,  ..., -4.5288e-01,\n",
       "            1.7554e-01,  5.7373e-01],\n",
       "          [-2.3340e+00, -9.3262e-01, -2.2083e-01,  ..., -4.9951e-01,\n",
       "            8.7036e-02,  5.5273e-01],\n",
       "          [-1.0684e+00, -1.8770e+00, -9.8926e-01,  ..., -5.4443e-01,\n",
       "            2.4268e-01,  5.5078e-01]],\n",
       "\n",
       "         [[ 2.1179e-02, -1.2169e-02,  4.7150e-03,  ...,  1.0498e-01,\n",
       "           -8.5266e-02, -2.5854e-01],\n",
       "          [ 3.6367e+00,  1.4238e+00,  6.7383e-02,  ..., -7.8271e-01,\n",
       "            5.6396e-02,  1.0088e+00],\n",
       "          [-1.4121e+00,  2.6641e+00, -2.4766e+00,  ..., -2.1211e+00,\n",
       "           -9.6924e-01,  1.0859e+00],\n",
       "          ...,\n",
       "          [-3.4082e+00,  2.1191e-01,  9.4873e-01,  ..., -9.0918e-01,\n",
       "           -1.4072e+00, -4.8877e-01],\n",
       "          [-1.3281e+00,  2.5996e+00,  1.9336e+00,  ..., -8.7695e-01,\n",
       "           -1.3486e+00, -3.3301e-01],\n",
       "          [ 2.0938e+00,  3.3086e+00,  2.2617e+00,  ..., -8.6621e-01,\n",
       "           -1.2822e+00, -4.3262e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.1463e-03, -1.1002e-02, -1.5564e-02,  ...,  3.0322e-01,\n",
       "           -5.9912e-01,  4.6387e-01],\n",
       "          [ 5.4297e-01, -4.3848e-01,  4.3945e-01,  ...,  1.3594e+00,\n",
       "            4.8906e+00, -2.7617e+00],\n",
       "          [ 1.9043e-01,  5.3223e-01,  8.3447e-01,  ...,  1.2129e+00,\n",
       "            2.4863e+00, -4.0820e+00],\n",
       "          ...,\n",
       "          [-5.4541e-01, -3.3789e-01, -3.5425e-01,  ..., -9.5154e-02,\n",
       "            2.5195e+00, -8.9453e-01],\n",
       "          [-3.2520e-01,  2.9541e-01, -5.6055e-01,  ..., -1.4136e-01,\n",
       "            2.3340e+00, -9.8242e-01],\n",
       "          [ 1.6797e-01,  7.7637e-01, -5.2148e-01,  ..., -1.7993e-01,\n",
       "            2.3438e+00, -1.1240e+00]],\n",
       "\n",
       "         [[-1.9165e-02,  3.7766e-04, -5.8746e-03,  ...,  7.2852e-01,\n",
       "           -1.3892e-01, -8.3618e-02],\n",
       "          [ 9.1992e-01,  1.1279e-01,  1.3193e+00,  ..., -1.5645e+00,\n",
       "           -7.3633e-01,  3.1177e-01],\n",
       "          [ 6.8311e-01, -3.5522e-02, -4.3042e-01,  ..., -2.2852e+00,\n",
       "           -1.6309e+00,  8.8965e-01],\n",
       "          ...,\n",
       "          [-5.2734e-01,  9.3994e-01, -2.1973e-03,  ..., -1.5205e+00,\n",
       "            7.9980e-01,  3.8794e-01],\n",
       "          [-5.5908e-01,  7.1045e-02,  4.3652e-01,  ..., -1.4873e+00,\n",
       "            7.1484e-01,  2.6880e-01],\n",
       "          [-4.5898e-02, -9.4385e-01,  7.7637e-01,  ..., -1.3926e+00,\n",
       "            6.3574e-01,  2.3718e-01]],\n",
       "\n",
       "         [[-1.0437e-02,  3.8361e-02,  1.6632e-03,  ..., -3.9575e-01,\n",
       "           -5.1208e-02, -2.8503e-02],\n",
       "          [ 2.1562e+00, -1.0488e+00,  2.3066e+00,  ..., -2.0972e-01,\n",
       "           -7.3438e-01, -1.4941e-01],\n",
       "          [ 4.5625e+00, -3.0879e+00,  3.4375e+00,  ...,  5.0000e-01,\n",
       "           -1.8176e-01, -2.2715e+00],\n",
       "          ...,\n",
       "          [-1.3262e+00, -7.1777e-01, -2.3242e+00,  ..., -5.7617e-01,\n",
       "           -5.2051e-01, -2.8613e-01],\n",
       "          [-2.8652e+00, -1.7812e+00, -1.6172e+00,  ..., -6.3867e-01,\n",
       "           -5.8740e-01, -2.4341e-01],\n",
       "          [-1.9980e+00, -1.7236e+00, -1.6309e-01,  ..., -6.3135e-01,\n",
       "           -6.4209e-01, -3.4961e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-3.3741e-03, -2.9163e-03, -4.2992e-03,  ..., -4.7836e-03,\n",
       "            3.8910e-04,  1.7176e-03],\n",
       "          [ 9.8938e-02, -2.6636e-01,  6.8970e-03,  ..., -1.1896e-01,\n",
       "            8.7952e-02, -3.2056e-01],\n",
       "          [ 4.3579e-01, -3.9697e-01,  1.2915e-01,  ..., -4.2114e-02,\n",
       "            1.7041e-01, -5.9540e-02],\n",
       "          ...,\n",
       "          [ 9.9365e-02,  4.2603e-02, -1.3708e-01,  ..., -7.8796e-02,\n",
       "            2.0950e-02,  1.8555e-01],\n",
       "          [ 1.7847e-01,  1.3684e-01, -1.0126e-01,  ...,  2.7588e-01,\n",
       "            1.4392e-01, -7.7026e-02],\n",
       "          [ 1.3025e-01,  2.6538e-01,  8.5327e-02,  ...,  2.7246e-01,\n",
       "           -1.7615e-01, -1.1597e-01]],\n",
       "\n",
       "         [[ 7.6866e-04, -3.0251e-03,  1.4091e-02,  ...,  6.5422e-03,\n",
       "            2.3327e-03, -7.9250e-04],\n",
       "          [-1.5808e-01,  1.2390e-01, -1.7334e-01,  ...,  1.1731e-01,\n",
       "           -1.9812e-01, -1.1279e-01],\n",
       "          [ 3.2178e-01, -2.7197e-01, -7.3914e-02,  ..., -1.2964e-01,\n",
       "           -3.2684e-02, -1.3989e-01],\n",
       "          ...,\n",
       "          [-9.4482e-02,  1.4514e-01,  4.1406e-01,  ...,  8.7219e-02,\n",
       "           -5.4395e-01,  3.5205e-01],\n",
       "          [ 1.7383e-01, -8.6365e-02,  6.8420e-02,  ..., -2.4329e-01,\n",
       "            2.5391e-01, -1.1475e-01],\n",
       "          [-1.1914e-01,  2.1393e-02, -6.6504e-01,  ...,  3.2251e-01,\n",
       "            5.8398e-01,  1.0327e-01]],\n",
       "\n",
       "         [[ 1.7567e-03, -3.3798e-03,  4.4060e-03,  ..., -3.4294e-03,\n",
       "           -4.6301e-04,  1.6022e-03],\n",
       "          [-1.4526e-01,  2.2180e-01,  1.2219e-01,  ...,  2.0312e-01,\n",
       "            3.7170e-02, -3.5400e-01],\n",
       "          [-1.2964e-01,  1.0114e-01, -1.1157e-01,  ...,  2.5464e-01,\n",
       "           -1.1200e-01, -1.6785e-03],\n",
       "          ...,\n",
       "          [-2.2632e-01, -6.7520e-03, -9.9365e-02,  ...,  1.0791e-01,\n",
       "            1.3947e-02,  3.1158e-02],\n",
       "          [ 1.3660e-01, -7.5623e-02, -8.0933e-02,  ...,  1.2134e-01,\n",
       "           -1.0925e-01,  8.6914e-02],\n",
       "          [-2.0459e-01,  7.4707e-02,  6.2500e-02,  ...,  3.0777e-02,\n",
       "            1.9760e-02,  1.8018e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.1471e-05,  1.5297e-03, -7.5006e-04,  ..., -7.4673e-04,\n",
       "            5.0468e-03, -5.0735e-04],\n",
       "          [-1.3257e-01,  3.0884e-01,  4.0588e-02,  ..., -1.4331e-01,\n",
       "            7.6965e-02, -7.3181e-02],\n",
       "          [-1.3110e-01,  2.2232e-02, -6.0883e-03,  ...,  5.8044e-02,\n",
       "            1.1151e-01,  9.5032e-02],\n",
       "          ...,\n",
       "          [-1.5637e-01,  2.8275e-02, -2.2552e-02,  ...,  3.0319e-02,\n",
       "           -4.7949e-01,  1.2372e-01],\n",
       "          [-3.5522e-02, -3.3997e-02,  1.3647e-01,  ...,  1.2183e-01,\n",
       "           -2.9346e-01, -9.1675e-02],\n",
       "          [ 7.4768e-02,  1.0907e-01, -1.1292e-02,  ...,  9.3994e-02,\n",
       "            2.8662e-01,  1.5308e-01]],\n",
       "\n",
       "         [[-3.6850e-03, -5.3558e-03,  4.3335e-03,  ..., -3.1700e-03,\n",
       "            9.4223e-04,  4.9591e-04],\n",
       "          [-1.0791e-01,  1.3367e-01,  1.2170e-01,  ...,  2.2003e-02,\n",
       "           -1.7163e-01, -2.2510e-01],\n",
       "          [ 3.1647e-02,  3.9062e-02, -2.1741e-01,  ..., -1.8286e-01,\n",
       "           -3.0884e-01,  7.5378e-03],\n",
       "          ...,\n",
       "          [ 4.2651e-01, -1.5526e-03,  2.1069e-01,  ...,  2.4756e-01,\n",
       "           -2.1179e-02, -1.5527e-01],\n",
       "          [-1.6541e-01,  4.9530e-02,  1.1584e-01,  ..., -4.0283e-03,\n",
       "           -6.0043e-03,  1.6388e-02],\n",
       "          [-3.6646e-01,  2.3914e-01, -9.7778e-02,  ...,  5.5481e-02,\n",
       "           -1.5051e-01,  1.9073e-02]],\n",
       "\n",
       "         [[-1.3628e-03, -9.3460e-04,  1.0271e-03,  ...,  2.0027e-03,\n",
       "           -2.2907e-03,  3.4161e-03],\n",
       "          [-1.1609e-01,  1.5369e-01,  2.2095e-01,  ..., -1.8677e-02,\n",
       "            1.3013e-01, -5.1147e-02],\n",
       "          [-1.7578e-02,  3.9368e-02,  3.2031e-01,  ..., -5.0598e-02,\n",
       "            6.1279e-02,  1.1279e-01],\n",
       "          ...,\n",
       "          [ 1.0150e-01, -1.2769e-01, -3.6475e-01,  ..., -2.6758e-01,\n",
       "           -2.1033e-01,  1.8750e-01],\n",
       "          [ 2.1179e-01,  8.0688e-02, -6.0852e-02,  ...,  2.6782e-01,\n",
       "            7.7881e-02,  1.3220e-01],\n",
       "          [ 2.1838e-01,  8.7280e-02, -6.7505e-02,  ...,  1.4679e-02,\n",
       "           -1.7603e-01,  1.4075e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.3741e-03, -2.9163e-03, -4.2992e-03,  ..., -4.7836e-03,\n",
       "            3.8910e-04,  1.7176e-03],\n",
       "          [ 9.8938e-02, -2.6636e-01,  6.8970e-03,  ..., -1.1896e-01,\n",
       "            8.7952e-02, -3.2056e-01],\n",
       "          [ 5.7312e-02, -1.1804e-01,  1.2671e-01,  ...,  9.8511e-02,\n",
       "           -2.4512e-01,  5.2429e-02],\n",
       "          ...,\n",
       "          [-8.3374e-02, -2.5921e-03, -1.9385e-01,  ..., -2.4316e-01,\n",
       "           -1.3477e-01, -5.5908e-02],\n",
       "          [-6.3293e-02,  1.5381e-02, -1.4587e-01,  ..., -2.5415e-01,\n",
       "           -1.9690e-01, -2.5375e-02],\n",
       "          [-7.0007e-02,  1.3901e-02, -1.2115e-01,  ..., -1.9934e-01,\n",
       "           -1.2854e-01, -2.9175e-02]],\n",
       "\n",
       "         [[ 7.6866e-04, -3.0251e-03,  1.4091e-02,  ...,  6.5422e-03,\n",
       "            2.3327e-03, -7.9250e-04],\n",
       "          [-1.5808e-01,  1.2390e-01, -1.7334e-01,  ...,  1.1731e-01,\n",
       "           -1.9812e-01, -1.1279e-01],\n",
       "          [ 3.4277e-01, -2.6538e-01, -2.6703e-02,  ..., -1.5030e-02,\n",
       "           -1.1383e-01,  1.0706e-01],\n",
       "          ...,\n",
       "          [ 1.4233e-01, -3.5474e-01, -1.0748e-01,  ...,  3.5181e-01,\n",
       "            1.9666e-01, -6.1798e-03],\n",
       "          [ 1.2244e-01, -3.6304e-01, -1.2939e-01,  ...,  3.8599e-01,\n",
       "            1.9006e-01, -2.1179e-02],\n",
       "          [ 1.2036e-01, -3.1348e-01, -1.0870e-01,  ...,  3.9795e-01,\n",
       "            1.8958e-01, -1.7502e-02]],\n",
       "\n",
       "         [[ 1.7567e-03, -3.3798e-03,  4.4060e-03,  ..., -3.4294e-03,\n",
       "           -4.6301e-04,  1.6022e-03],\n",
       "          [-1.4526e-01,  2.2180e-01,  1.2219e-01,  ...,  2.0312e-01,\n",
       "            3.7170e-02, -3.5400e-01],\n",
       "          [-2.6270e-01,  2.0129e-01,  7.5195e-02,  ...,  2.1899e-01,\n",
       "           -1.1188e-01, -5.4169e-04],\n",
       "          ...,\n",
       "          [ 1.5137e-01,  1.1035e-01,  8.4900e-02,  ..., -1.9165e-02,\n",
       "           -9.2712e-02, -1.6754e-02],\n",
       "          [ 7.9102e-02,  9.0393e-02,  9.6375e-02,  ...,  1.3527e-02,\n",
       "           -9.7351e-02, -2.5497e-02],\n",
       "          [ 3.8879e-02,  1.2494e-01,  1.0193e-01,  ...,  3.1982e-02,\n",
       "           -1.3062e-01, -3.5767e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.1471e-05,  1.5297e-03, -7.5006e-04,  ..., -7.4673e-04,\n",
       "            5.0468e-03, -5.0735e-04],\n",
       "          [-1.3257e-01,  3.0884e-01,  4.0588e-02,  ..., -1.4331e-01,\n",
       "            7.6965e-02, -7.3181e-02],\n",
       "          [-1.8652e-01, -4.9500e-02,  3.5034e-02,  ...,  7.3547e-02,\n",
       "            1.3818e-01,  2.3407e-02],\n",
       "          ...,\n",
       "          [ 7.9285e-02, -1.4270e-01, -2.5558e-03,  ...,  2.0248e-02,\n",
       "            1.2733e-02,  7.6599e-02],\n",
       "          [ 7.0557e-02, -1.1975e-01, -5.7602e-03,  ...,  3.7506e-02,\n",
       "           -2.8122e-02,  8.6670e-02],\n",
       "          [ 9.1003e-02, -1.3477e-01, -8.3084e-03,  ...,  3.0090e-02,\n",
       "           -7.7271e-02,  1.1963e-01]],\n",
       "\n",
       "         [[-3.6850e-03, -5.3558e-03,  4.3335e-03,  ..., -3.1700e-03,\n",
       "            9.4223e-04,  4.9591e-04],\n",
       "          [-1.0791e-01,  1.3367e-01,  1.2170e-01,  ...,  2.2003e-02,\n",
       "           -1.7163e-01, -2.2510e-01],\n",
       "          [ 1.3428e-01,  8.5205e-02,  4.4556e-03,  ...,  2.7405e-02,\n",
       "           -1.3867e-01,  2.1606e-02],\n",
       "          ...,\n",
       "          [-3.5156e-01,  1.5576e-01, -1.8103e-01,  ..., -4.4220e-02,\n",
       "           -2.7441e-01,  2.3468e-02],\n",
       "          [-3.1787e-01,  1.3635e-01, -2.0789e-01,  ..., -7.6294e-02,\n",
       "           -2.5757e-01,  4.4708e-02],\n",
       "          [-3.0200e-01,  1.6162e-01, -2.0288e-01,  ..., -1.2292e-01,\n",
       "           -2.0251e-01,  3.0243e-02]],\n",
       "\n",
       "         [[-1.3628e-03, -9.3460e-04,  1.0271e-03,  ...,  2.0027e-03,\n",
       "           -2.2907e-03,  3.4161e-03],\n",
       "          [-1.1609e-01,  1.5369e-01,  2.2095e-01,  ..., -1.8677e-02,\n",
       "            1.3013e-01, -5.1147e-02],\n",
       "          [-1.4075e-01, -1.3599e-01, -4.7943e-02,  ..., -2.6074e-01,\n",
       "            7.2144e-02,  8.1299e-02],\n",
       "          ...,\n",
       "          [ 5.3436e-02, -9.9365e-02, -1.5417e-01,  ...,  8.4717e-02,\n",
       "           -2.5909e-02,  1.0583e-01],\n",
       "          [ 2.0248e-02, -1.2769e-01, -1.4026e-01,  ...,  3.3264e-02,\n",
       "           -1.8433e-02,  1.2891e-01],\n",
       "          [-6.9008e-03, -1.0608e-01, -1.7883e-01,  ...,  2.4796e-05,\n",
       "           -8.9874e-03,  1.2683e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.3573e-02, -7.7438e-03, -1.8539e-03,  ...,  2.1875e-01,\n",
       "           -4.6729e-01,  8.3069e-02],\n",
       "          [-5.7422e-01, -8.7646e-01, -7.7148e-01,  ...,  2.0703e-01,\n",
       "           -3.5522e-01,  4.0283e-01],\n",
       "          [ 2.2266e+00, -1.9570e+00, -2.0312e-01,  ...,  4.2236e-01,\n",
       "            7.0215e-01, -5.9033e-01],\n",
       "          ...,\n",
       "          [ 1.8301e+00, -6.9727e-01,  1.4785e+00,  ...,  1.6309e+00,\n",
       "           -4.5508e-01, -2.3987e-01],\n",
       "          [-2.5146e-01, -4.3359e-01, -1.2793e-01,  ...,  3.0713e-01,\n",
       "            4.0210e-01, -2.2637e+00],\n",
       "          [-1.3740e+00, -3.2007e-01, -8.6523e-01,  ...,  2.6294e-01,\n",
       "            4.3823e-01, -1.2900e+00]],\n",
       "\n",
       "         [[-9.0866e-03, -1.7029e-02, -2.5497e-02,  ..., -1.1658e-01,\n",
       "           -4.7150e-02, -2.1149e-02],\n",
       "          [-1.7188e+00,  1.2598e+00, -1.1172e+00,  ...,  1.8628e-01,\n",
       "           -1.6621e+00,  6.8726e-02],\n",
       "          [-5.1914e+00,  3.3945e+00,  4.3604e-01,  ...,  9.6973e-01,\n",
       "            8.8477e-01,  5.2246e-01],\n",
       "          ...,\n",
       "          [ 2.9180e+00, -1.7578e-02, -1.4600e+00,  ..., -2.2510e-01,\n",
       "           -1.2754e+00, -4.3823e-02],\n",
       "          [ 3.2617e+00,  2.1328e+00, -1.7334e+00,  ...,  6.9238e-01,\n",
       "           -5.8740e-01, -5.2295e-01],\n",
       "          [ 2.2891e+00,  3.5508e+00, -3.5234e+00,  ...,  2.4512e-01,\n",
       "           -5.5811e-01,  4.5508e-01]],\n",
       "\n",
       "         [[ 2.1683e-02, -9.0179e-03, -3.0727e-03,  ...,  5.2686e-01,\n",
       "           -5.1758e-01, -1.2561e-01],\n",
       "          [ 4.1328e+00,  2.5830e-01,  2.3652e+00,  ..., -2.0691e-02,\n",
       "            5.4108e-02,  1.1182e+00],\n",
       "          [-5.4590e-01,  2.1328e+00, -1.6035e+00,  ..., -4.7021e-01,\n",
       "           -2.1179e-01, -4.9023e-01],\n",
       "          ...,\n",
       "          [-2.7734e+00, -2.4531e+00, -2.3359e+00,  ..., -1.8417e-02,\n",
       "            7.2607e-01, -2.1899e-01],\n",
       "          [-1.5732e+00,  1.0537e+00, -1.6377e+00,  ...,  7.6807e-01,\n",
       "           -5.0635e-01, -3.9575e-01],\n",
       "          [ 2.1738e+00,  1.0049e+00,  5.3516e-01,  ..., -1.2461e+00,\n",
       "           -4.1809e-02, -3.3203e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.8000e-03, -3.4714e-03,  1.2131e-02,  ...,  4.0161e-01,\n",
       "           -1.4392e-01,  1.6582e+00],\n",
       "          [-1.0254e+00,  4.7656e-01,  7.9736e-01,  ...,  4.5239e-01,\n",
       "           -3.4375e-01, -2.5938e+00],\n",
       "          [-1.7598e+00,  2.0654e-01,  2.0273e+00,  ...,  5.4297e-01,\n",
       "            7.8076e-01, -3.1699e+00],\n",
       "          ...,\n",
       "          [ 1.2080e+00,  1.3955e+00, -9.8242e-01,  ...,  1.4150e+00,\n",
       "           -1.1611e+00, -3.2793e+00],\n",
       "          [ 1.7944e-01, -1.1292e-01, -7.3584e-01,  ...,  1.8027e+00,\n",
       "            5.9961e-01, -3.0098e+00],\n",
       "          [ 2.2998e-01, -1.0742e+00, -1.0986e+00,  ...,  1.4248e+00,\n",
       "            7.6709e-01, -3.4766e+00]],\n",
       "\n",
       "         [[ 5.0354e-03, -1.0710e-03, -6.4888e-03,  ..., -1.2537e-01,\n",
       "           -3.1421e-01,  7.1680e-01],\n",
       "          [-3.2930e+00, -9.2139e-01,  2.8730e+00,  ...,  8.3252e-02,\n",
       "            3.0249e-01,  5.1416e-01],\n",
       "          [-2.4082e+00, -5.4883e-01, -3.4521e-01,  ...,  9.6582e-01,\n",
       "           -5.8258e-02,  1.0381e+00],\n",
       "          ...,\n",
       "          [ 1.6963e+00, -1.5156e+00,  2.0923e-01,  ..., -1.8877e+00,\n",
       "           -1.9861e-01,  1.4551e+00],\n",
       "          [ 1.3467e+00,  4.1455e-01, -1.1094e+00,  ...,  1.2197e+00,\n",
       "           -8.9307e-01,  1.0381e+00],\n",
       "          [-1.6855e+00,  1.9971e+00,  1.8135e+00,  ...,  7.1045e-01,\n",
       "           -6.5137e-01,  4.6826e-01]],\n",
       "\n",
       "         [[-1.7548e-02,  5.5389e-03, -1.2650e-02,  ..., -2.0126e-02,\n",
       "           -2.8931e-01,  3.2007e-01],\n",
       "          [ 7.1924e-01, -9.4434e-01,  3.3496e-01,  ...,  6.4453e-01,\n",
       "            8.6621e-01, -6.3843e-02],\n",
       "          [ 3.3750e+00, -2.7734e+00,  2.6484e+00,  ...,  1.1484e+00,\n",
       "           -8.9893e-01,  1.2817e-01],\n",
       "          ...,\n",
       "          [-2.0723e+00, -6.1475e-01, -7.9492e-01,  ..., -1.3262e+00,\n",
       "            2.2207e+00,  2.1113e+00],\n",
       "          [-1.0147e-03, -1.1895e+00, -3.2471e-01,  ..., -5.7324e-01,\n",
       "            9.7559e-01, -7.1680e-01],\n",
       "          [ 1.8896e-01, -1.3340e+00,  4.8828e-03,  ..., -5.6152e-01,\n",
       "           -8.5205e-02,  4.7729e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3573e-02, -7.7438e-03, -1.8539e-03,  ...,  2.1875e-01,\n",
       "           -4.6729e-01,  8.3069e-02],\n",
       "          [-5.7422e-01, -8.7646e-01, -7.7148e-01,  ...,  2.0703e-01,\n",
       "           -3.5522e-01,  4.0283e-01],\n",
       "          [ 1.9102e+00, -2.2363e+00, -9.5020e-01,  ..., -3.7109e-01,\n",
       "            2.8809e-01, -1.9714e-01],\n",
       "          ...,\n",
       "          [ 6.4160e-01, -7.8906e-01,  4.9902e-01,  ...,  1.5693e+00,\n",
       "           -8.4668e-01,  4.6729e-01],\n",
       "          [-7.3926e-01, -1.8066e-01, -1.1725e-01,  ...,  1.3887e+00,\n",
       "           -5.5957e-01,  5.6787e-01],\n",
       "          [-1.4102e+00,  5.5957e-01, -6.3818e-01,  ...,  1.3408e+00,\n",
       "           -5.3955e-01,  5.4932e-01]],\n",
       "\n",
       "         [[-9.0866e-03, -1.7029e-02, -2.5497e-02,  ..., -1.1658e-01,\n",
       "           -4.7150e-02, -2.1149e-02],\n",
       "          [-1.7188e+00,  1.2598e+00, -1.1172e+00,  ...,  1.8628e-01,\n",
       "           -1.6621e+00,  6.8726e-02],\n",
       "          [-5.0117e+00,  3.1133e+00,  3.8867e-01,  ...,  2.7979e-01,\n",
       "            9.9707e-01, -1.8848e-01],\n",
       "          ...,\n",
       "          [ 1.1895e+00, -6.3477e-02,  6.3477e-03,  ..., -1.2490e+00,\n",
       "            4.2212e-01,  1.5674e+00],\n",
       "          [ 3.2285e+00,  2.0625e+00, -1.0723e+00,  ..., -1.3896e+00,\n",
       "            6.3281e-01,  1.4561e+00],\n",
       "          [ 2.4492e+00,  2.8379e+00, -1.7139e+00,  ..., -1.3213e+00,\n",
       "            6.6211e-01,  1.5293e+00]],\n",
       "\n",
       "         [[ 2.1683e-02, -9.0179e-03, -3.0727e-03,  ...,  5.2686e-01,\n",
       "           -5.1758e-01, -1.2561e-01],\n",
       "          [ 4.1328e+00,  2.5830e-01,  2.3652e+00,  ..., -2.0691e-02,\n",
       "            5.4108e-02,  1.1182e+00],\n",
       "          [-4.9219e-01,  2.4238e+00, -6.7725e-01,  ..., -6.1768e-01,\n",
       "            6.7993e-02, -1.5625e-01],\n",
       "          ...,\n",
       "          [-3.2070e+00,  9.7998e-01, -1.7451e+00,  ...,  2.0679e-01,\n",
       "            2.9761e-01, -1.7017e-01],\n",
       "          [-1.6885e+00,  2.0996e+00, -1.3105e+00,  ...,  1.7261e-01,\n",
       "            4.4727e-01,  4.4556e-02],\n",
       "          [ 1.4492e+00,  1.8555e+00, -4.9805e-02,  ...,  2.2046e-01,\n",
       "            4.9414e-01, -9.0881e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.8000e-03, -3.4714e-03,  1.2131e-02,  ...,  4.0161e-01,\n",
       "           -1.4392e-01,  1.6582e+00],\n",
       "          [-1.0254e+00,  4.7656e-01,  7.9736e-01,  ...,  4.5239e-01,\n",
       "           -3.4375e-01, -2.5938e+00],\n",
       "          [-1.6523e+00,  2.6953e-01,  2.2090e+00,  ...,  6.0059e-01,\n",
       "            4.2163e-01, -3.5879e+00],\n",
       "          ...,\n",
       "          [ 1.3730e+00,  1.3193e+00,  1.4148e-01,  ...,  3.0664e-01,\n",
       "            2.5820e+00, -2.1797e+00],\n",
       "          [ 1.4805e+00, -2.4414e-02, -8.8379e-02,  ...,  2.8296e-01,\n",
       "            2.5664e+00, -2.1953e+00],\n",
       "          [ 3.4399e-01, -1.3740e+00, -4.1406e-01,  ...,  3.5181e-01,\n",
       "            2.6621e+00, -2.1895e+00]],\n",
       "\n",
       "         [[ 5.0354e-03, -1.0710e-03, -6.4888e-03,  ..., -1.2537e-01,\n",
       "           -3.1421e-01,  7.1680e-01],\n",
       "          [-3.2930e+00, -9.2139e-01,  2.8730e+00,  ...,  8.3252e-02,\n",
       "            3.0249e-01,  5.1416e-01],\n",
       "          [-2.8828e+00, -7.3828e-01,  1.6382e-01,  ..., -2.0801e-01,\n",
       "           -2.1082e-01,  1.3379e+00],\n",
       "          ...,\n",
       "          [ 2.2520e+00, -2.2852e+00,  9.1016e-01,  ...,  3.6841e-01,\n",
       "           -7.8223e-01,  8.3154e-01],\n",
       "          [ 1.2061e+00, -8.5010e-01,  1.9453e+00,  ...,  6.2158e-01,\n",
       "           -7.3291e-01,  1.0508e+00],\n",
       "          [-1.0273e+00,  9.9170e-01,  2.1641e+00,  ...,  6.8018e-01,\n",
       "           -6.4307e-01,  1.1221e+00]],\n",
       "\n",
       "         [[-1.7548e-02,  5.5389e-03, -1.2650e-02,  ..., -2.0126e-02,\n",
       "           -2.8931e-01,  3.2007e-01],\n",
       "          [ 7.1924e-01, -9.4434e-01,  3.3496e-01,  ...,  6.4453e-01,\n",
       "            8.6621e-01, -6.3843e-02],\n",
       "          [ 3.3008e+00, -2.6406e+00,  2.4707e+00,  ...,  5.8154e-01,\n",
       "           -7.3535e-01, -1.6797e-01],\n",
       "          ...,\n",
       "          [-1.3984e+00, -1.0898e+00, -1.7266e+00,  ...,  4.2017e-01,\n",
       "            1.1267e-01, -4.5532e-01],\n",
       "          [-2.0215e+00, -1.7832e+00, -1.7480e+00,  ...,  4.1968e-01,\n",
       "            1.4978e-01, -5.1709e-01],\n",
       "          [-6.8652e-01, -1.3203e+00, -1.1777e+00,  ...,  4.9365e-01,\n",
       "            2.5269e-01, -4.4971e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 9.1705e-03, -1.6203e-03, -2.0123e-03,  ..., -1.2741e-03,\n",
       "           -3.3512e-03,  6.5498e-03],\n",
       "          [ 1.0620e-01,  1.3318e-01, -6.6406e-02,  ..., -2.7930e-01,\n",
       "            4.3237e-01, -2.3779e-01],\n",
       "          [-1.4807e-01,  8.9050e-02, -2.6074e-01,  ..., -2.0129e-01,\n",
       "            1.6907e-01, -7.7576e-02],\n",
       "          ...,\n",
       "          [ 1.4001e-01, -5.8936e-01,  4.3945e-02,  ..., -2.8979e-01,\n",
       "           -4.7070e-01, -1.1865e-01],\n",
       "          [ 5.1956e-03, -9.3506e-02,  3.8550e-01,  ...,  3.3960e-01,\n",
       "            1.4783e-01, -3.0176e-01],\n",
       "          [-1.3770e-01, -8.3679e-02, -3.6914e-01,  ...,  7.4707e-02,\n",
       "           -6.9336e-02,  1.3412e-02]],\n",
       "\n",
       "         [[ 1.4677e-03,  1.3123e-02,  1.8406e-03,  ...,  5.9013e-03,\n",
       "            8.0795e-03,  6.6681e-03],\n",
       "          [ 4.8126e-02,  1.2006e-01, -7.2876e-02,  ...,  3.8184e-01,\n",
       "            1.2067e-01,  6.2012e-01],\n",
       "          [ 5.5225e-01, -1.6370e-01,  9.6008e-02,  ..., -3.1592e-01,\n",
       "            3.3081e-01, -1.7017e-01],\n",
       "          ...,\n",
       "          [-1.8372e-01,  3.4155e-01, -2.1143e-01,  ..., -3.9111e-01,\n",
       "           -4.6851e-01,  3.2739e-01],\n",
       "          [-2.7808e-01,  1.0902e-02, -5.7861e-01,  ..., -2.9956e-01,\n",
       "           -7.5867e-02,  1.4233e-01],\n",
       "          [ 1.2622e-01, -3.0835e-01,  2.5269e-01,  ...,  2.0493e-02,\n",
       "            2.2510e-01,  3.3350e-01]],\n",
       "\n",
       "         [[ 3.0918e-03, -3.8815e-03, -4.5357e-03,  ...,  3.0327e-04,\n",
       "           -4.1046e-03, -2.2781e-02],\n",
       "          [ 1.8494e-02, -1.7227e-02, -2.6929e-01,  ...,  2.4426e-01,\n",
       "           -9.8938e-02,  2.2729e-01],\n",
       "          [-1.5405e-01,  4.9042e-02, -3.6694e-01,  ...,  2.4384e-02,\n",
       "           -3.9746e-01,  8.0029e-01],\n",
       "          ...,\n",
       "          [ 3.3641e-04, -2.9053e-01, -2.5781e-01,  ...,  5.4102e-01,\n",
       "           -1.1450e-01,  7.1094e-01],\n",
       "          [ 2.3853e-01, -3.2379e-02,  8.5876e-02,  ..., -5.7324e-01,\n",
       "           -1.1511e-01, -6.0010e-01],\n",
       "          [ 2.5238e-02, -3.8452e-01,  6.0669e-02,  ..., -3.4863e-01,\n",
       "           -2.8223e-01,  3.1689e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.6556e-02,  1.5650e-03, -3.1796e-03,  ...,  3.8738e-03,\n",
       "           -3.8743e-05,  7.0801e-03],\n",
       "          [-2.2400e-01, -2.0935e-01,  1.7310e-01,  ..., -4.6802e-01,\n",
       "            2.7657e-03,  8.1205e-04],\n",
       "          [-2.6221e-01, -1.7822e-04, -2.3657e-01,  ...,  7.1228e-02,\n",
       "            1.5869e-02, -4.5312e-01],\n",
       "          ...,\n",
       "          [ 2.8223e-01,  1.2268e-01, -1.6528e-01,  ..., -3.5620e-01,\n",
       "            4.5410e-01, -4.1748e-01],\n",
       "          [ 2.0630e-01, -2.0886e-01,  2.3181e-01,  ...,  1.3062e-01,\n",
       "            3.4448e-01, -5.9723e-02],\n",
       "          [-1.4099e-01, -3.0933e-01, -9.5093e-02,  ..., -3.8208e-01,\n",
       "           -1.9482e-01,  2.9587e-02]],\n",
       "\n",
       "         [[-3.8414e-03, -1.0368e-02,  3.9673e-03,  ..., -6.0425e-03,\n",
       "           -1.0857e-02, -8.5306e-04],\n",
       "          [-2.2864e-01,  1.3806e-01,  5.4199e-02,  ..., -3.0103e-01,\n",
       "           -6.1914e-01,  5.4395e-01],\n",
       "          [ 1.2061e-01, -1.3525e-01,  3.4814e-01,  ...,  3.7170e-02,\n",
       "            1.7139e-01,  1.5289e-02],\n",
       "          ...,\n",
       "          [ 1.3293e-01,  6.6016e-01,  9.0088e-02,  ..., -4.5746e-02,\n",
       "           -1.7071e-03, -1.3428e-01],\n",
       "          [ 2.4683e-01,  1.5723e-01,  2.4646e-01,  ..., -8.1665e-02,\n",
       "           -3.9337e-02, -1.7896e-01],\n",
       "          [ 2.9205e-02,  1.1572e-01,  6.4880e-02,  ..., -8.1116e-02,\n",
       "           -5.7434e-02, -1.4587e-01]],\n",
       "\n",
       "         [[-1.2865e-03,  2.9602e-03,  5.7030e-03,  ..., -6.0234e-03,\n",
       "           -5.0354e-03, -3.4809e-03],\n",
       "          [ 4.5435e-01,  7.3425e-02,  1.6150e-01,  ...,  2.2476e-02,\n",
       "            3.7628e-02,  1.0150e-01],\n",
       "          [ 1.4725e-03, -4.5135e-02, -1.0925e-02,  ..., -2.9956e-01,\n",
       "           -5.3589e-02,  3.7524e-01],\n",
       "          ...,\n",
       "          [ 1.2390e-01, -3.7451e-01,  1.6943e-01,  ..., -3.8892e-01,\n",
       "           -1.8433e-01,  3.7598e-02],\n",
       "          [-9.5886e-02,  2.0581e-01,  2.1667e-01,  ..., -4.9820e-03,\n",
       "            2.0154e-01, -3.7939e-01],\n",
       "          [-2.1521e-01,  3.0396e-01, -2.0251e-01,  ..., -3.3960e-01,\n",
       "           -9.1064e-02, -9.6497e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 9.1705e-03, -1.6203e-03, -2.0123e-03,  ..., -1.2741e-03,\n",
       "           -3.3512e-03,  6.5498e-03],\n",
       "          [ 1.0620e-01,  1.3318e-01, -6.6406e-02,  ..., -2.7930e-01,\n",
       "            4.3237e-01, -2.3779e-01],\n",
       "          [-1.2091e-01,  2.8589e-01,  2.3270e-02,  ...,  2.0483e-01,\n",
       "            2.2510e-01,  8.2703e-02],\n",
       "          ...,\n",
       "          [ 4.6387e-02,  5.6610e-02, -4.4971e-01,  ...,  2.8369e-01,\n",
       "           -1.0449e-01,  3.8379e-01],\n",
       "          [ 9.5032e-02,  1.6708e-02, -4.0698e-01,  ...,  2.8711e-01,\n",
       "           -9.9365e-02,  3.6255e-01],\n",
       "          [ 9.4238e-02, -2.1851e-02, -3.8477e-01,  ...,  3.1641e-01,\n",
       "           -1.1273e-01,  3.1616e-01]],\n",
       "\n",
       "         [[ 1.4677e-03,  1.3123e-02,  1.8406e-03,  ...,  5.9013e-03,\n",
       "            8.0795e-03,  6.6681e-03],\n",
       "          [ 4.8126e-02,  1.2006e-01, -7.2876e-02,  ...,  3.8184e-01,\n",
       "            1.2067e-01,  6.2012e-01],\n",
       "          [ 3.1464e-02,  2.8671e-02, -1.9817e-03,  ..., -3.0859e-01,\n",
       "            1.8127e-01, -4.9756e-01],\n",
       "          ...,\n",
       "          [ 1.8933e-01,  1.1188e-01,  3.7451e-01,  ..., -3.2318e-02,\n",
       "            3.5522e-01,  4.4861e-02],\n",
       "          [ 1.9678e-01,  8.1482e-02,  3.8403e-01,  ...,  3.0487e-02,\n",
       "            3.3179e-01,  6.3171e-02],\n",
       "          [ 2.0984e-01,  1.2891e-01,  3.4863e-01,  ...,  4.4403e-02,\n",
       "            3.0444e-01,  1.1157e-01]],\n",
       "\n",
       "         [[ 3.0918e-03, -3.8815e-03, -4.5357e-03,  ...,  3.0327e-04,\n",
       "           -4.1046e-03, -2.2781e-02],\n",
       "          [ 1.8494e-02, -1.7227e-02, -2.6929e-01,  ...,  2.4426e-01,\n",
       "           -9.8938e-02,  2.2729e-01],\n",
       "          [-1.8469e-01,  2.9068e-02, -2.5635e-01,  ..., -2.8101e-01,\n",
       "            1.7126e-01,  7.5977e-01],\n",
       "          ...,\n",
       "          [ 2.0386e-02, -3.4009e-01,  1.0754e-01,  ..., -4.6069e-01,\n",
       "            8.1482e-02,  5.5518e-01],\n",
       "          [ 1.9577e-02, -3.3398e-01,  1.3611e-01,  ..., -4.8145e-01,\n",
       "            7.8674e-02,  5.1758e-01],\n",
       "          [-5.5389e-03, -3.5864e-01,  3.9551e-02,  ..., -4.5142e-01,\n",
       "            1.1578e-01,  5.5176e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.6556e-02,  1.5650e-03, -3.1796e-03,  ...,  3.8738e-03,\n",
       "           -3.8743e-05,  7.0801e-03],\n",
       "          [-2.2400e-01, -2.0935e-01,  1.7310e-01,  ..., -4.6802e-01,\n",
       "            2.7657e-03,  8.1205e-04],\n",
       "          [-3.9355e-01,  1.3940e-01,  6.0028e-02,  ..., -1.0730e-01,\n",
       "            2.6413e-02, -2.5610e-01],\n",
       "          ...,\n",
       "          [-2.3401e-01, -2.8946e-02, -3.4326e-01,  ..., -2.9224e-01,\n",
       "           -5.8545e-01,  5.3955e-01],\n",
       "          [-2.7051e-01, -5.0354e-02, -3.3716e-01,  ..., -3.1470e-01,\n",
       "           -6.2939e-01,  6.0840e-01],\n",
       "          [-2.8247e-01,  6.9214e-02, -2.8296e-01,  ..., -2.8540e-01,\n",
       "           -6.0498e-01,  5.5078e-01]],\n",
       "\n",
       "         [[-3.8414e-03, -1.0368e-02,  3.9673e-03,  ..., -6.0425e-03,\n",
       "           -1.0857e-02, -8.5306e-04],\n",
       "          [-2.2864e-01,  1.3806e-01,  5.4199e-02,  ..., -3.0103e-01,\n",
       "           -6.1914e-01,  5.4395e-01],\n",
       "          [-4.5807e-02,  1.0669e-01, -6.8054e-02,  ..., -2.2546e-01,\n",
       "            2.0569e-01,  4.8193e-01],\n",
       "          ...,\n",
       "          [ 8.5907e-03,  1.2671e-01,  5.1300e-02,  ..., -1.3123e-01,\n",
       "           -1.7822e-01, -8.5754e-02],\n",
       "          [ 2.4200e-02,  8.1177e-02,  8.5876e-02,  ..., -1.1511e-01,\n",
       "           -1.7639e-01, -1.2433e-01],\n",
       "          [ 3.5877e-03,  8.1604e-02,  8.5205e-02,  ..., -1.1188e-01,\n",
       "           -2.1387e-01, -1.6162e-01]],\n",
       "\n",
       "         [[-1.2865e-03,  2.9602e-03,  5.7030e-03,  ..., -6.0234e-03,\n",
       "           -5.0354e-03, -3.4809e-03],\n",
       "          [ 4.5435e-01,  7.3425e-02,  1.6150e-01,  ...,  2.2476e-02,\n",
       "            3.7628e-02,  1.0150e-01],\n",
       "          [-1.0205e-01, -2.7969e-02, -6.4270e-02,  ..., -2.3364e-01,\n",
       "           -2.1252e-01,  2.8662e-01],\n",
       "          ...,\n",
       "          [-1.4267e-02,  1.1255e-01,  1.1609e-01,  ..., -2.6367e-01,\n",
       "           -6.8848e-02, -2.8641e-02],\n",
       "          [-2.3163e-02,  1.3477e-01,  1.4404e-01,  ..., -2.7808e-01,\n",
       "           -4.2389e-02,  1.5732e-02],\n",
       "          [-8.3237e-03,  1.9177e-01,  1.4563e-01,  ..., -2.9297e-01,\n",
       "           -6.1401e-02,  4.7211e-02]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.4458e-02, -2.6550e-02,  3.8624e-05,  ...,  5.1086e-02,\n",
       "            6.5369e-02, -3.0347e-01],\n",
       "          [ 2.1836e+00,  1.1367e+00,  7.6270e-01,  ...,  8.2666e-01,\n",
       "            9.4287e-01, -1.1445e+00],\n",
       "          [ 1.9941e+00,  2.4688e+00,  1.2588e+00,  ..., -6.7627e-01,\n",
       "           -1.6504e-01, -2.8735e-01],\n",
       "          ...,\n",
       "          [-2.4141e+00,  4.8926e-01, -4.4629e-01,  ...,  2.0300e-01,\n",
       "            1.2617e+00, -2.7368e-01],\n",
       "          [-3.1714e-01,  5.6982e-01, -5.0293e-01,  ..., -1.6572e+00,\n",
       "           -1.5723e+00, -2.2852e-01],\n",
       "          [ 7.1826e-01,  2.5195e+00, -8.2959e-01,  ...,  4.0497e-02,\n",
       "           -1.3213e+00, -4.3091e-01]],\n",
       "\n",
       "         [[ 3.5461e-02, -1.7252e-03,  5.3864e-03,  ..., -8.3105e-01,\n",
       "            3.9160e-01, -1.8518e-01],\n",
       "          [ 1.3174e+00, -7.5684e-02, -4.2334e-01,  ..., -4.9683e-01,\n",
       "            1.8188e-01,  2.2385e-02],\n",
       "          [-1.0518e+00, -2.1348e+00, -1.9629e+00,  ..., -2.2012e+00,\n",
       "           -1.1127e-01,  1.3076e+00],\n",
       "          ...,\n",
       "          [-1.6973e+00, -1.7053e-01,  5.3613e-01,  ...,  3.3423e-01,\n",
       "           -1.0801e+00, -1.2764e+00],\n",
       "          [ 5.2686e-01, -7.5586e-01,  1.7646e+00,  ...,  1.8076e+00,\n",
       "           -5.7080e-01,  6.6846e-01],\n",
       "          [ 3.8887e+00, -1.2529e+00,  1.8965e+00,  ...,  9.9219e-01,\n",
       "           -1.3486e+00, -1.8447e+00]],\n",
       "\n",
       "         [[ 2.5711e-02,  6.7520e-03,  1.6083e-02,  ..., -2.7954e-01,\n",
       "           -2.5049e-01,  3.3667e-01],\n",
       "          [ 1.0898e+00,  7.2266e-01,  1.1494e+00,  ..., -2.0762e+00,\n",
       "           -7.1680e-01,  5.0140e-02],\n",
       "          [-6.1865e-01, -1.1963e+00,  2.5122e-01,  ...,  6.4880e-02,\n",
       "           -4.9561e-01, -3.0838e-02],\n",
       "          ...,\n",
       "          [-1.4375e+00,  6.4404e-01, -1.1270e+00,  ..., -1.0068e+00,\n",
       "           -4.6631e-01, -8.7305e-01],\n",
       "          [-4.2773e-01, -2.6416e-01, -9.1309e-02,  ..., -5.4590e-01,\n",
       "           -4.6021e-01, -1.3818e+00],\n",
       "          [ 1.2578e+00, -1.3633e+00,  1.4590e+00,  ..., -5.8838e-01,\n",
       "           -7.2266e-02, -7.9688e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.7678e-03,  1.3762e-03,  1.2047e-02,  ...,  5.9863e-01,\n",
       "           -1.9629e-01,  4.3921e-01],\n",
       "          [-1.0566e+00,  7.5830e-01,  5.6152e-02,  ..., -7.7002e-01,\n",
       "            2.6172e-01,  3.2788e-01],\n",
       "          [-4.3711e+00,  2.1211e+00,  1.1553e+00,  ..., -4.2554e-01,\n",
       "           -4.2505e-01,  1.2715e+00],\n",
       "          ...,\n",
       "          [ 1.4834e+00,  4.7949e-01, -7.3340e-01,  ...,  4.6729e-01,\n",
       "           -3.1519e-01,  4.6069e-01],\n",
       "          [ 4.6265e-02,  3.2617e-01,  3.1555e-02,  ...,  5.1074e-01,\n",
       "            6.4648e-01,  4.4556e-01],\n",
       "          [ 1.5979e-01,  3.4277e-01, -1.4922e+00,  ...,  2.2229e-01,\n",
       "           -4.1675e-01,  1.0571e-01]],\n",
       "\n",
       "         [[-4.5898e-02, -9.3613e-03, -7.5264e-03,  ...,  2.2253e-01,\n",
       "            1.6382e-01,  7.8918e-02],\n",
       "          [-1.5479e+00, -1.6943e+00, -1.7627e+00,  ..., -4.6533e-01,\n",
       "           -1.9153e-01, -6.0498e-01],\n",
       "          [ 5.3076e-01, -5.0293e-01, -1.9521e+00,  ...,  4.8828e-01,\n",
       "           -3.1787e-01,  2.5464e-01],\n",
       "          ...,\n",
       "          [ 3.4258e+00,  1.2773e+00,  2.9062e+00,  ...,  2.7148e-01,\n",
       "           -4.4995e-01,  2.7783e-01],\n",
       "          [ 3.9893e-01, -1.2178e+00,  8.4619e-01,  ..., -1.0479e+00,\n",
       "            4.7852e-01,  2.9614e-01],\n",
       "          [-2.1914e+00,  9.0625e-01,  7.1924e-01,  ..., -9.2822e-01,\n",
       "            1.3291e+00,  7.6025e-01]],\n",
       "\n",
       "         [[-1.4465e-02,  1.8326e-02, -2.9434e-02,  ..., -3.5059e-01,\n",
       "            4.2798e-01, -2.4768e-01],\n",
       "          [-1.3301e+00,  1.4795e+00, -1.6016e+00,  ...,  1.7932e-01,\n",
       "            1.0371e+00,  1.5684e+00],\n",
       "          [ 3.9395e+00,  2.0938e+00, -1.6191e+00,  ...,  7.0312e-01,\n",
       "           -9.9658e-01,  9.6387e-01],\n",
       "          ...,\n",
       "          [ 1.5898e+00,  1.4873e+00,  4.3242e+00,  ..., -8.0566e-01,\n",
       "            1.3877e+00, -6.3184e-01],\n",
       "          [-3.9673e-02,  1.5156e+00,  4.9744e-02,  ..., -7.7942e-02,\n",
       "           -4.3921e-01, -5.5908e-01],\n",
       "          [-1.1787e+00,  1.4941e+00, -1.2207e+00,  ...,  7.1143e-01,\n",
       "           -6.5625e-01, -1.4795e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4458e-02, -2.6550e-02,  3.8624e-05,  ...,  5.1086e-02,\n",
       "            6.5369e-02, -3.0347e-01],\n",
       "          [ 2.1836e+00,  1.1367e+00,  7.6270e-01,  ...,  8.2666e-01,\n",
       "            9.4287e-01, -1.1445e+00],\n",
       "          [ 2.8574e+00,  2.4609e+00,  9.9268e-01,  ..., -3.6060e-01,\n",
       "            3.1030e-01, -1.1511e-01],\n",
       "          ...,\n",
       "          [-1.2305e+00,  2.7441e-01, -1.5322e+00,  ..., -4.9390e-01,\n",
       "            3.8721e-01, -5.8154e-01],\n",
       "          [-2.0938e+00,  1.9248e+00, -1.5586e+00,  ..., -4.4092e-01,\n",
       "            3.7939e-01, -4.9341e-01],\n",
       "          [-1.0195e+00,  2.3711e+00, -8.9844e-01,  ..., -5.7910e-01,\n",
       "            7.3145e-01, -5.9961e-01]],\n",
       "\n",
       "         [[ 3.5461e-02, -1.7252e-03,  5.3864e-03,  ..., -8.3105e-01,\n",
       "            3.9160e-01, -1.8518e-01],\n",
       "          [ 1.3174e+00, -7.5684e-02, -4.2334e-01,  ..., -4.9683e-01,\n",
       "            1.8188e-01,  2.2385e-02],\n",
       "          [-1.6084e+00, -2.2578e+00, -2.0996e+00,  ..., -1.0713e+00,\n",
       "           -3.8037e-01,  1.2520e+00],\n",
       "          ...,\n",
       "          [-3.4414e+00, -1.0732e+00,  1.8115e-01,  ...,  9.7229e-02,\n",
       "           -6.5479e-01, -8.7744e-01],\n",
       "          [-7.1289e-02, -1.4551e+00,  1.4092e+00,  ...,  1.0602e-01,\n",
       "           -7.5293e-01, -8.3105e-01],\n",
       "          [ 3.4102e+00, -9.5898e-01,  2.1914e+00,  ...,  7.9895e-02,\n",
       "           -7.9346e-01, -8.0176e-01]],\n",
       "\n",
       "         [[ 2.5711e-02,  6.7520e-03,  1.6083e-02,  ..., -2.7954e-01,\n",
       "           -2.5049e-01,  3.3667e-01],\n",
       "          [ 1.0898e+00,  7.2266e-01,  1.1494e+00,  ..., -2.0762e+00,\n",
       "           -7.1680e-01,  5.0140e-02],\n",
       "          [-1.1074e+00, -9.4141e-01,  3.9551e-01,  ..., -3.1372e-02,\n",
       "           -3.9014e-01, -4.4629e-01],\n",
       "          ...,\n",
       "          [-8.1348e-01,  1.0479e+00,  6.8994e-01,  ..., -2.1530e-02,\n",
       "           -9.6497e-02, -9.9414e-01],\n",
       "          [ 6.4941e-01, -1.0876e-01,  1.6250e+00,  ...,  2.4887e-02,\n",
       "            4.7363e-02, -1.0771e+00],\n",
       "          [ 1.5273e+00, -1.2627e+00,  1.8750e+00,  ...,  1.4612e-01,\n",
       "            6.9946e-02, -1.0127e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.7678e-03,  1.3762e-03,  1.2047e-02,  ...,  5.9863e-01,\n",
       "           -1.9629e-01,  4.3921e-01],\n",
       "          [-1.0566e+00,  7.5830e-01,  5.6152e-02,  ..., -7.7002e-01,\n",
       "            2.6172e-01,  3.2788e-01],\n",
       "          [-4.5234e+00,  1.8721e+00,  1.3379e+00,  ..., -6.1621e-01,\n",
       "            1.1090e-01,  1.2188e+00],\n",
       "          ...,\n",
       "          [ 5.7275e-01, -1.7090e-03,  8.3984e-02,  ..., -9.5312e-01,\n",
       "           -7.8613e-01,  1.9946e-01],\n",
       "          [ 1.0723e+00,  5.3711e-01, -1.1230e+00,  ..., -9.8047e-01,\n",
       "           -7.1387e-01,  2.1924e-01],\n",
       "          [ 5.8008e-01,  8.6328e-01, -2.0410e+00,  ..., -1.0361e+00,\n",
       "           -7.7686e-01,  2.6709e-01]],\n",
       "\n",
       "         [[-4.5898e-02, -9.3613e-03, -7.5264e-03,  ...,  2.2253e-01,\n",
       "            1.6382e-01,  7.8918e-02],\n",
       "          [-1.5479e+00, -1.6943e+00, -1.7627e+00,  ..., -4.6533e-01,\n",
       "           -1.9153e-01, -6.0498e-01],\n",
       "          [ 9.4629e-01, -1.4893e-01, -2.5605e+00,  ...,  4.8779e-01,\n",
       "            5.5145e-02,  3.5938e-01],\n",
       "          ...,\n",
       "          [ 1.4883e+00, -2.7930e+00,  1.3770e+00,  ..., -6.3281e-01,\n",
       "            6.2842e-01,  1.5784e-01],\n",
       "          [ 2.9419e-02, -2.0625e+00,  8.9551e-01,  ..., -5.8984e-01,\n",
       "            5.3271e-01,  2.3560e-01],\n",
       "          [-1.5322e+00,  5.8594e-03,  3.6133e-02,  ..., -6.0693e-01,\n",
       "            4.8779e-01,  2.3901e-01]],\n",
       "\n",
       "         [[-1.4465e-02,  1.8326e-02, -2.9434e-02,  ..., -3.5059e-01,\n",
       "            4.2798e-01, -2.4768e-01],\n",
       "          [-1.3301e+00,  1.4795e+00, -1.6016e+00,  ...,  1.7932e-01,\n",
       "            1.0371e+00,  1.5684e+00],\n",
       "          [ 3.9316e+00,  2.1348e+00, -2.1133e+00,  ...,  1.2520e+00,\n",
       "           -1.0957e+00,  6.0986e-01],\n",
       "          ...,\n",
       "          [ 1.5195e+00,  1.0488e+00,  2.7515e-01,  ...,  1.8770e+00,\n",
       "            2.4121e-01, -1.4355e-01],\n",
       "          [-1.4082e+00,  2.1133e+00, -5.3516e-01,  ...,  1.8848e+00,\n",
       "            2.3901e-01, -1.1620e-02],\n",
       "          [-3.0996e+00,  1.7217e+00, -1.0566e+00,  ...,  1.8828e+00,\n",
       "            3.4180e-01, -2.7161e-02]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-2.8839e-03, -9.6893e-03,  1.3847e-03,  ...,  9.1934e-03,\n",
       "           -3.7079e-03,  4.9820e-03],\n",
       "          [-3.2593e-01, -4.1504e-01, -5.5298e-02,  ...,  3.6401e-01,\n",
       "            2.6880e-01, -4.1565e-02],\n",
       "          [-3.5229e-01, -8.9844e-02, -2.2729e-01,  ..., -1.9897e-01,\n",
       "            2.6343e-01,  6.6040e-02],\n",
       "          ...,\n",
       "          [-4.8730e-01, -3.4546e-01,  1.0431e-01,  ..., -3.1201e-01,\n",
       "           -9.4910e-02,  1.9067e-01],\n",
       "          [-2.1988e-02,  1.9678e-01, -9.4788e-02,  ..., -2.3914e-01,\n",
       "           -2.5366e-01,  5.9863e-01],\n",
       "          [ 8.3801e-02,  2.0203e-01,  1.3985e-02,  ..., -5.5267e-02,\n",
       "            1.5356e-01,  3.6768e-01]],\n",
       "\n",
       "         [[ 2.0332e-03,  4.7913e-03, -4.7035e-03,  ..., -2.3437e-04,\n",
       "            5.1613e-03, -8.7204e-03],\n",
       "          [ 1.9751e-01, -2.3010e-01,  4.5728e-01,  ..., -1.6333e-01,\n",
       "            7.3303e-02, -2.5903e-01],\n",
       "          [ 8.3313e-02,  3.4058e-01,  8.7952e-02,  ..., -5.4230e-02,\n",
       "            3.2080e-01, -2.5806e-01],\n",
       "          ...,\n",
       "          [-3.7671e-01,  1.8774e-01,  3.4277e-01,  ...,  6.7139e-02,\n",
       "            4.7070e-01, -4.1406e-01],\n",
       "          [-1.3281e-01,  3.7793e-01, -1.1163e-01,  ..., -4.3640e-02,\n",
       "           -1.2457e-01, -4.7302e-02],\n",
       "          [ 2.2217e-02, -2.3962e-01, -1.0571e-01,  ..., -1.6455e-01,\n",
       "            2.9510e-02, -1.6724e-01]],\n",
       "\n",
       "         [[ 8.8501e-04, -5.7526e-03, -3.4161e-03,  ..., -3.7117e-03,\n",
       "           -9.6436e-03,  1.1797e-03],\n",
       "          [ 8.3313e-02,  2.1021e-01, -3.7207e-01,  ..., -3.0457e-02,\n",
       "            2.0203e-01,  2.3267e-01],\n",
       "          [-2.2314e-01, -6.5186e-02,  1.4526e-01,  ..., -4.9243e-01,\n",
       "            1.4294e-01, -3.5059e-01],\n",
       "          ...,\n",
       "          [-7.1960e-02,  3.6865e-02,  6.2943e-03,  ..., -4.4189e-01,\n",
       "            2.2778e-01, -1.2244e-01],\n",
       "          [ 2.3560e-01,  2.0190e-01,  1.5175e-02,  ..., -2.8467e-01,\n",
       "            1.1340e-01, -2.3779e-01],\n",
       "          [-1.9485e-02, -5.9448e-02, -6.3354e-02,  ..., -3.1177e-01,\n",
       "           -1.5466e-01, -3.0298e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.0651e-03,  5.6953e-03, -2.0962e-03,  ...,  1.7986e-03,\n",
       "           -5.1928e-04, -7.1335e-04],\n",
       "          [-5.5566e-01,  2.8320e-01,  8.1024e-03,  ...,  6.7749e-02,\n",
       "            1.0395e-03, -2.2937e-01],\n",
       "          [-6.2891e-01,  2.4426e-01, -2.9834e-01,  ...,  2.2693e-01,\n",
       "           -2.4292e-01,  1.0522e-01],\n",
       "          ...,\n",
       "          [-2.2034e-01,  7.2998e-01,  4.3970e-01,  ..., -3.5797e-02,\n",
       "           -3.3203e-01,  6.1798e-02],\n",
       "          [-7.5989e-02, -3.0420e-01,  3.9795e-01,  ..., -3.8477e-01,\n",
       "           -7.2449e-02,  4.6631e-02],\n",
       "          [ 7.0679e-02,  1.3525e-01,  3.8525e-01,  ..., -3.4302e-02,\n",
       "           -3.0518e-01,  2.0471e-01]],\n",
       "\n",
       "         [[-3.3736e-05, -3.0556e-03,  4.2076e-03,  ..., -3.0613e-03,\n",
       "           -8.9874e-03,  5.3978e-03],\n",
       "          [ 1.2183e-01,  9.1797e-02, -4.2212e-01,  ..., -7.6782e-02,\n",
       "            1.7554e-01,  2.0251e-01],\n",
       "          [ 2.7252e-02, -4.9683e-02, -2.7222e-01,  ..., -3.1055e-01,\n",
       "           -1.4246e-01, -2.6318e-01],\n",
       "          ...,\n",
       "          [-2.5708e-01, -1.4380e-01, -1.0889e-01,  ..., -5.6946e-02,\n",
       "            3.5919e-02,  2.2656e-01],\n",
       "          [-6.0028e-02, -8.7830e-02,  2.9785e-01,  ..., -3.3105e-01,\n",
       "           -1.3049e-01, -2.3877e-01],\n",
       "          [-7.2449e-02, -9.6588e-03, -5.6824e-02,  ...,  2.0432e-02,\n",
       "           -1.2939e-01,  3.1470e-01]],\n",
       "\n",
       "         [[-2.3289e-03,  4.3335e-03,  1.3838e-03,  ..., -5.6190e-03,\n",
       "            1.3367e-02, -6.5308e-03],\n",
       "          [ 8.6060e-02, -1.7700e-01,  2.6172e-01,  ...,  1.6443e-01,\n",
       "           -1.8164e-01, -3.6572e-01],\n",
       "          [-1.2695e-01, -8.3130e-02,  7.2876e-02,  ..., -9.7839e-02,\n",
       "            2.6611e-01, -3.4961e-01],\n",
       "          ...,\n",
       "          [-6.0156e-01,  1.8481e-01,  5.9912e-01,  ..., -2.1582e-01,\n",
       "           -2.7786e-02, -3.0786e-01],\n",
       "          [-3.0005e-01,  3.7720e-01,  4.7168e-01,  ..., -6.0760e-02,\n",
       "            1.8091e-01, -2.6416e-01],\n",
       "          [-4.7943e-02,  3.8306e-01, -9.6741e-02,  ..., -1.3696e-01,\n",
       "            3.7061e-01, -2.5830e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.8839e-03, -9.6893e-03,  1.3847e-03,  ...,  9.1934e-03,\n",
       "           -3.7079e-03,  4.9820e-03],\n",
       "          [-3.2593e-01, -4.1504e-01, -5.5298e-02,  ...,  3.6401e-01,\n",
       "            2.6880e-01, -4.1565e-02],\n",
       "          [-1.0461e-01, -2.4622e-01,  3.9246e-02,  ...,  1.3269e-01,\n",
       "            1.8518e-01,  1.1444e-02],\n",
       "          ...,\n",
       "          [ 5.9662e-02, -3.2990e-02,  6.0987e-04,  ...,  6.6650e-02,\n",
       "            1.2781e-01, -5.3310e-04],\n",
       "          [ 1.7807e-02,  5.2929e-04, -1.1353e-02,  ...,  6.5002e-02,\n",
       "            1.1200e-01, -4.0955e-02],\n",
       "          [-5.0568e-02,  3.6583e-03,  4.1199e-02,  ...,  6.1523e-02,\n",
       "            1.1871e-01, -1.9821e-02]],\n",
       "\n",
       "         [[ 2.0332e-03,  4.7913e-03, -4.7035e-03,  ..., -2.3437e-04,\n",
       "            5.1613e-03, -8.7204e-03],\n",
       "          [ 1.9751e-01, -2.3010e-01,  4.5728e-01,  ..., -1.6333e-01,\n",
       "            7.3303e-02, -2.5903e-01],\n",
       "          [-2.0154e-01, -2.3071e-01, -1.7798e-01,  ...,  9.8816e-02,\n",
       "            1.1755e-01,  1.4404e-01],\n",
       "          ...,\n",
       "          [ 6.3293e-02,  5.4932e-03, -1.5906e-01,  ..., -3.1958e-01,\n",
       "            2.0032e-01, -3.4741e-01],\n",
       "          [ 5.5145e-02, -3.1250e-02, -1.5417e-01,  ..., -2.4695e-01,\n",
       "            1.3794e-01, -3.5376e-01],\n",
       "          [-6.7024e-03,  3.5278e-02, -1.2488e-01,  ..., -2.7417e-01,\n",
       "            1.7017e-01, -4.0918e-01]],\n",
       "\n",
       "         [[ 8.8501e-04, -5.7526e-03, -3.4161e-03,  ..., -3.7117e-03,\n",
       "           -9.6436e-03,  1.1797e-03],\n",
       "          [ 8.3313e-02,  2.1021e-01, -3.7207e-01,  ..., -3.0457e-02,\n",
       "            2.0203e-01,  2.3267e-01],\n",
       "          [-1.1664e-01, -2.6489e-01,  8.4290e-02,  ..., -9.9731e-02,\n",
       "            1.8542e-01, -2.2388e-01],\n",
       "          ...,\n",
       "          [-1.0925e-01,  1.9629e-01, -1.5503e-01,  ...,  4.5135e-02,\n",
       "           -1.0883e-01, -1.9397e-01],\n",
       "          [-7.5684e-02,  1.6296e-01, -1.9531e-01,  ...,  1.7502e-02,\n",
       "           -1.4099e-01, -2.0776e-01],\n",
       "          [-4.3121e-02,  2.0764e-01, -1.3855e-01,  ...,  7.8918e-02,\n",
       "           -2.0752e-01, -1.6260e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.0651e-03,  5.6953e-03, -2.0962e-03,  ...,  1.7986e-03,\n",
       "           -5.1928e-04, -7.1335e-04],\n",
       "          [-5.5566e-01,  2.8320e-01,  8.1024e-03,  ...,  6.7749e-02,\n",
       "            1.0395e-03, -2.2937e-01],\n",
       "          [-3.0884e-01, -5.1422e-02, -2.7954e-01,  ...,  5.1483e-02,\n",
       "           -4.0466e-02, -2.1716e-01],\n",
       "          ...,\n",
       "          [ 1.9653e-01,  2.5482e-03, -6.0059e-01,  ...,  2.3059e-01,\n",
       "            5.5756e-02,  1.7041e-01],\n",
       "          [ 1.8799e-01, -1.6479e-02, -5.8740e-01,  ...,  2.3083e-01,\n",
       "            1.4685e-01,  1.8323e-01],\n",
       "          [ 1.6541e-01, -2.6367e-02, -5.2881e-01,  ...,  1.4380e-01,\n",
       "            9.1248e-02,  1.3989e-01]],\n",
       "\n",
       "         [[-3.3736e-05, -3.0556e-03,  4.2076e-03,  ..., -3.0613e-03,\n",
       "           -8.9874e-03,  5.3978e-03],\n",
       "          [ 1.2183e-01,  9.1797e-02, -4.2212e-01,  ..., -7.6782e-02,\n",
       "            1.7554e-01,  2.0251e-01],\n",
       "          [ 1.8896e-01, -1.8286e-01, -2.5391e-01,  ..., -1.8506e-01,\n",
       "            4.3182e-02, -2.1375e-01],\n",
       "          ...,\n",
       "          [-1.4478e-01, -1.7609e-02, -1.8143e-02,  ...,  1.2054e-01,\n",
       "           -4.4647e-02,  4.1870e-02],\n",
       "          [-1.3025e-01,  4.2297e-02,  2.9709e-02,  ...,  1.1072e-01,\n",
       "           -7.2021e-02,  9.1614e-02],\n",
       "          [-1.4075e-01,  3.4943e-02,  5.6763e-02,  ...,  1.3049e-01,\n",
       "           -8.8257e-02,  9.5520e-02]],\n",
       "\n",
       "         [[-2.3289e-03,  4.3335e-03,  1.3838e-03,  ..., -5.6190e-03,\n",
       "            1.3367e-02, -6.5308e-03],\n",
       "          [ 8.6060e-02, -1.7700e-01,  2.6172e-01,  ...,  1.6443e-01,\n",
       "           -1.8164e-01, -3.6572e-01],\n",
       "          [ 1.2091e-01, -9.8633e-02,  6.6345e-02,  ...,  1.4539e-01,\n",
       "           -3.3112e-02, -3.2910e-01],\n",
       "          ...,\n",
       "          [-1.4679e-02,  2.8931e-01, -2.9761e-01,  ..., -7.0496e-02,\n",
       "            7.1350e-02, -2.3331e-02],\n",
       "          [ 6.7558e-03,  3.3398e-01, -3.6963e-01,  ..., -1.3513e-01,\n",
       "            7.4280e-02, -2.6417e-03],\n",
       "          [-3.1433e-02,  3.5718e-01, -3.5400e-01,  ..., -1.8433e-01,\n",
       "            4.4861e-02, -1.5060e-02]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-4.1237e-03,  7.4120e-03,  2.3823e-03,  ..., -8.8623e-02,\n",
       "            1.6956e-01,  2.1118e-01],\n",
       "          [ 3.1680e+00,  1.5361e+00,  8.1934e-01,  ..., -6.0986e-01,\n",
       "            7.3877e-01, -6.5125e-02],\n",
       "          [ 1.9814e+00,  2.8574e+00,  2.3320e+00,  ...,  7.9407e-02,\n",
       "           -3.1860e-01, -5.3809e-01],\n",
       "          ...,\n",
       "          [-2.0391e+00,  2.7148e+00, -2.7832e-01,  ..., -5.7227e-01,\n",
       "           -1.6055e+00, -6.4648e-01],\n",
       "          [-6.9141e-01,  9.3896e-01, -1.3486e+00,  ..., -1.2998e+00,\n",
       "           -1.8301e+00,  7.5928e-01],\n",
       "          [ 6.9189e-01,  3.0884e-02, -1.7676e+00,  ...,  2.2937e-01,\n",
       "           -7.0264e-01, -6.9275e-02]],\n",
       "\n",
       "         [[ 1.6495e-02, -2.1332e-02,  1.5793e-02,  ..., -1.3164e+00,\n",
       "            4.3848e-01, -2.5586e-01],\n",
       "          [ 9.5410e-01, -4.1992e-01,  2.1460e-01,  ...,  2.3750e+00,\n",
       "           -9.7168e-01, -6.9580e-01],\n",
       "          [-4.1431e-01,  8.0566e-01, -1.0811e+00,  ...,  1.7695e+00,\n",
       "            6.0693e-01, -3.5156e-01],\n",
       "          ...,\n",
       "          [-1.0938e+00, -3.1885e-01,  3.8477e-01,  ...,  2.9043e+00,\n",
       "           -1.5869e-01,  2.0566e+00],\n",
       "          [-2.5415e-01, -2.3608e-01, -1.7896e-01,  ...,  8.1152e-01,\n",
       "            1.5293e+00,  1.0146e+00],\n",
       "          [ 3.8062e-01,  2.7051e-01,  4.1357e-01,  ...,  3.7427e-01,\n",
       "            1.7676e+00,  1.4307e+00]],\n",
       "\n",
       "         [[ 3.3783e-02, -9.5062e-03,  5.7297e-03,  ..., -6.8115e-01,\n",
       "            1.6931e-01, -1.1514e+00],\n",
       "          [-4.9438e-01, -5.6152e-02, -7.0264e-01,  ...,  2.3965e+00,\n",
       "            1.4463e+00,  5.8047e+00],\n",
       "          [-1.1566e-02, -6.6956e-02,  5.6396e-02,  ...,  2.8594e+00,\n",
       "           -7.2900e-01,  4.2969e+00],\n",
       "          ...,\n",
       "          [ 5.4102e-01,  1.2854e-01,  5.0928e-01,  ...,  4.6445e+00,\n",
       "            1.7480e+00,  5.9131e-01],\n",
       "          [ 3.1445e-01,  5.0586e-01,  2.7002e-01,  ...,  6.4727e+00,\n",
       "            4.6328e+00, -4.8022e-01],\n",
       "          [ 9.9060e-02, -7.7393e-02,  2.6147e-01,  ...,  3.6250e+00,\n",
       "            9.6533e-01,  9.1406e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.7334e-02,  1.0040e-02, -1.4984e-02,  ..., -6.0107e-01,\n",
       "           -5.7812e-01, -1.1729e+00],\n",
       "          [-9.7461e-01,  2.9736e-01,  9.8975e-01,  ...,  7.8467e-01,\n",
       "            2.6680e+00, -7.7637e-01],\n",
       "          [-2.0293e+00,  1.7266e+00,  1.9863e+00,  ...,  1.1270e+00,\n",
       "            2.0996e+00, -6.9580e-01],\n",
       "          ...,\n",
       "          [ 1.1826e+00,  1.2695e-01, -1.9316e+00,  ..., -1.1729e+00,\n",
       "           -1.3145e+00,  3.0938e+00],\n",
       "          [ 6.7480e-01,  4.4995e-01, -1.1865e+00,  ..., -1.2305e+00,\n",
       "           -4.1602e-01,  1.0615e+00],\n",
       "          [ 6.0742e-01,  7.4023e-01, -5.0391e-01,  ..., -1.2764e+00,\n",
       "            1.1797e+00,  1.1609e-01]],\n",
       "\n",
       "         [[-1.4687e-02,  3.6621e-03,  2.2995e-02,  ...,  2.1594e-01,\n",
       "            6.8652e-01,  1.0625e+00],\n",
       "          [-1.1484e+00, -1.4282e-01,  7.5879e-01,  ...,  9.9304e-02,\n",
       "            5.8105e-01, -3.4375e-01],\n",
       "          [-1.7168e+00, -1.2012e+00,  1.8281e+00,  ..., -9.2676e-01,\n",
       "           -3.6963e-01,  5.9814e-01],\n",
       "          ...,\n",
       "          [ 6.1572e-01, -5.4785e-01, -7.7441e-01,  ..., -4.8523e-02,\n",
       "           -1.7842e+00,  4.4702e-01],\n",
       "          [ 7.8516e-01,  2.1729e-01, -9.0527e-01,  ..., -2.0520e-01,\n",
       "            3.0054e-01,  1.4463e+00],\n",
       "          [ 7.8320e-01, -8.5352e-01, -1.2021e+00,  ..., -7.5000e-01,\n",
       "            6.0547e-02,  4.0576e-01]],\n",
       "\n",
       "         [[-8.3237e-03,  1.4236e-02, -1.0406e-02,  ...,  2.7954e-01,\n",
       "           -1.9446e-01, -2.7344e-01],\n",
       "          [ 2.3672e+00, -1.4990e-01, -1.4248e+00,  ..., -9.6680e-02,\n",
       "            7.0996e-01, -2.9590e-01],\n",
       "          [-2.5391e+00,  3.3555e+00,  2.1367e+00,  ...,  7.2217e-01,\n",
       "           -9.3750e-01, -1.0605e+00],\n",
       "          ...,\n",
       "          [-2.4863e+00, -1.6523e+00,  4.3164e-01,  ..., -8.6279e-01,\n",
       "           -1.6904e+00,  1.2334e+00],\n",
       "          [-6.2500e-01, -2.7393e-01, -1.1780e-01,  ..., -3.4473e-01,\n",
       "            4.7192e-01,  7.1228e-02],\n",
       "          [ 1.7490e+00,  1.3477e+00, -1.5996e+00,  ...,  9.8145e-01,\n",
       "            1.7407e-01, -7.1338e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.1237e-03,  7.4120e-03,  2.3823e-03,  ..., -8.8623e-02,\n",
       "            1.6956e-01,  2.1118e-01],\n",
       "          [ 3.1680e+00,  1.5361e+00,  8.1934e-01,  ..., -6.0986e-01,\n",
       "            7.3877e-01, -6.5125e-02],\n",
       "          [ 2.1738e+00,  2.6855e+00,  2.4395e+00,  ..., -1.2969e+00,\n",
       "           -5.8398e-01, -2.1484e-01],\n",
       "          ...,\n",
       "          [-2.9141e+00,  1.7324e+00, -6.0400e-01,  ...,  7.4609e-01,\n",
       "            1.2939e-01, -4.6216e-01],\n",
       "          [-2.6992e+00,  1.1436e+00, -1.5586e+00,  ...,  6.2891e-01,\n",
       "            8.1604e-02, -5.8643e-01],\n",
       "          [ 1.2109e-01, -2.6660e-01, -1.8457e+00,  ...,  7.0020e-01,\n",
       "            1.4612e-01, -7.3145e-01]],\n",
       "\n",
       "         [[ 1.6495e-02, -2.1332e-02,  1.5793e-02,  ..., -1.3164e+00,\n",
       "            4.3848e-01, -2.5586e-01],\n",
       "          [ 9.5410e-01, -4.1992e-01,  2.1460e-01,  ...,  2.3750e+00,\n",
       "           -9.7168e-01, -6.9580e-01],\n",
       "          [-8.1055e-02,  5.7178e-01, -8.0859e-01,  ...,  2.0293e+00,\n",
       "            6.0107e-01,  3.0811e-01],\n",
       "          ...,\n",
       "          [-2.0156e+00, -1.4277e+00, -3.6328e-01,  ...,  9.8145e-01,\n",
       "            9.3213e-01,  8.3838e-01],\n",
       "          [-1.0811e+00, -9.7351e-02,  8.2959e-01,  ...,  7.3096e-01,\n",
       "            8.6182e-01,  7.1436e-01],\n",
       "          [ 8.4912e-01,  1.2412e+00,  1.6172e+00,  ...,  7.2656e-01,\n",
       "            8.0908e-01,  9.2627e-01]],\n",
       "\n",
       "         [[ 3.3783e-02, -9.5062e-03,  5.7297e-03,  ..., -6.8115e-01,\n",
       "            1.6931e-01, -1.1514e+00],\n",
       "          [-4.9438e-01, -5.6152e-02, -7.0264e-01,  ...,  2.3965e+00,\n",
       "            1.4463e+00,  5.8047e+00],\n",
       "          [ 9.5398e-02, -1.3623e-01,  2.5024e-01,  ...,  3.4062e+00,\n",
       "            1.0211e-01,  4.3281e+00],\n",
       "          ...,\n",
       "          [-4.1797e-01, -6.4648e-01, -3.3594e-01,  ...,  1.4043e+00,\n",
       "           -2.8296e-01,  1.0957e+00],\n",
       "          [ 4.9609e-01, -4.9805e-02,  4.3237e-01,  ...,  1.4307e+00,\n",
       "           -1.8079e-01,  1.0430e+00],\n",
       "          [ 9.4629e-01,  5.4688e-01,  9.6484e-01,  ...,  1.3584e+00,\n",
       "           -1.5454e-01,  1.0215e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.7334e-02,  1.0040e-02, -1.4984e-02,  ..., -6.0107e-01,\n",
       "           -5.7812e-01, -1.1729e+00],\n",
       "          [-9.7461e-01,  2.9736e-01,  9.8975e-01,  ...,  7.8467e-01,\n",
       "            2.6680e+00, -7.7637e-01],\n",
       "          [-1.8838e+00,  1.5645e+00,  1.9180e+00,  ...,  1.4033e+00,\n",
       "           -2.3132e-01, -7.6807e-01],\n",
       "          ...,\n",
       "          [ 5.7812e-01, -5.0537e-01, -1.3672e+00,  ..., -1.1777e+00,\n",
       "            3.1602e+00, -1.1338e+00],\n",
       "          [ 1.2002e+00,  6.8848e-01, -1.3682e+00,  ..., -1.2734e+00,\n",
       "            3.1934e+00, -1.1680e+00],\n",
       "          [ 7.2217e-01,  1.4424e+00, -7.6270e-01,  ..., -1.1758e+00,\n",
       "            3.2617e+00, -1.1982e+00]],\n",
       "\n",
       "         [[-1.4687e-02,  3.6621e-03,  2.2995e-02,  ...,  2.1594e-01,\n",
       "            6.8652e-01,  1.0625e+00],\n",
       "          [-1.1484e+00, -1.4282e-01,  7.5879e-01,  ...,  9.9304e-02,\n",
       "            5.8105e-01, -3.4375e-01],\n",
       "          [-1.6270e+00, -1.1826e+00,  2.0820e+00,  ..., -7.9639e-01,\n",
       "            2.5009e-02,  7.5317e-02],\n",
       "          ...,\n",
       "          [-2.1143e-01,  8.3838e-01, -3.9331e-01,  ..., -1.3672e+00,\n",
       "            3.2251e-01,  8.8818e-01],\n",
       "          [ 9.8486e-01,  1.4905e-01, -1.1270e+00,  ..., -1.3994e+00,\n",
       "            2.7075e-01,  1.0098e+00],\n",
       "          [ 1.3457e+00, -7.7930e-01, -1.4824e+00,  ..., -1.4229e+00,\n",
       "            3.0664e-01,  1.0322e+00]],\n",
       "\n",
       "         [[-8.3237e-03,  1.4236e-02, -1.0406e-02,  ...,  2.7954e-01,\n",
       "           -1.9446e-01, -2.7344e-01],\n",
       "          [ 2.3672e+00, -1.4990e-01, -1.4248e+00,  ..., -9.6680e-02,\n",
       "            7.0996e-01, -2.9590e-01],\n",
       "          [-2.8105e+00,  3.6543e+00,  2.2930e+00,  ...,  9.2090e-01,\n",
       "           -4.0942e-01, -1.6816e+00],\n",
       "          ...,\n",
       "          [-3.0742e+00,  3.2715e-01,  5.5566e-01,  ...,  6.0669e-02,\n",
       "            2.0764e-01, -4.6289e-01],\n",
       "          [ 2.0508e-01,  1.0889e+00, -1.2617e+00,  ..., -7.2289e-03,\n",
       "            4.0796e-01, -5.9766e-01],\n",
       "          [ 3.4414e+00,  1.1865e+00, -2.5801e+00,  ..., -2.0081e-01,\n",
       "            4.1235e-01, -6.9531e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-5.1346e-03,  3.6602e-03,  1.6680e-03,  ...,  2.8419e-03,\n",
       "            1.5478e-03, -2.0866e-03],\n",
       "          [-3.1348e-01, -4.4647e-02,  8.1360e-02,  ..., -1.2488e-01,\n",
       "            1.3782e-01,  7.8247e-02],\n",
       "          [-5.3711e-02,  1.8982e-01,  1.8921e-01,  ..., -1.6211e-01,\n",
       "            3.5950e-02, -1.5845e-01],\n",
       "          ...,\n",
       "          [-3.2166e-02,  4.6814e-02, -2.8198e-01,  ...,  1.4148e-01,\n",
       "            1.0870e-01,  2.3535e-01],\n",
       "          [-3.6108e-01,  6.3293e-02,  6.1798e-02,  ..., -2.3441e-03,\n",
       "           -2.6196e-01, -1.9385e-01],\n",
       "          [-4.1260e-02, -4.5850e-01,  6.1188e-03,  ...,  5.2032e-02,\n",
       "            6.5735e-02,  5.4639e-01]],\n",
       "\n",
       "         [[ 1.2016e-02, -3.8242e-03, -1.6693e-02,  ..., -3.3264e-03,\n",
       "           -2.6417e-03, -3.0441e-03],\n",
       "          [ 1.0809e-01,  1.3660e-01, -7.0763e-03,  ...,  9.8999e-02,\n",
       "           -2.9956e-01,  8.3313e-02],\n",
       "          [ 3.6764e-04,  1.0492e-01,  5.2032e-02,  ..., -9.5154e-02,\n",
       "           -1.4001e-01, -3.4637e-02],\n",
       "          ...,\n",
       "          [ 2.5928e-01, -4.6143e-02,  1.7065e-01,  ..., -1.3892e-01,\n",
       "            1.8750e-01,  3.0347e-01],\n",
       "          [ 3.7256e-01, -8.7402e-02,  9.0790e-03,  ..., -4.4403e-02,\n",
       "           -8.9600e-02, -1.0626e-01],\n",
       "          [ 5.1855e-01, -1.0089e-01,  2.5146e-01,  ...,  2.6794e-02,\n",
       "           -1.2866e-01, -1.0712e-01]],\n",
       "\n",
       "         [[-7.3967e-03,  6.5517e-04,  2.0924e-03,  ...,  7.7209e-03,\n",
       "           -7.4577e-04, -1.4811e-03],\n",
       "          [-3.9380e-01, -2.7115e-02, -9.2850e-03,  ..., -9.4421e-02,\n",
       "            1.0559e-01, -9.1324e-03],\n",
       "          [-3.9819e-01,  1.5857e-01, -4.5215e-01,  ..., -3.3203e-02,\n",
       "            3.4428e-03, -2.8125e-01],\n",
       "          ...,\n",
       "          [ 2.5854e-01, -1.1147e-02,  1.1273e-01,  ..., -1.1853e-01,\n",
       "           -9.6924e-02, -1.2189e-01],\n",
       "          [ 4.5508e-01, -1.0028e-01, -1.0895e-01,  ..., -3.0228e-02,\n",
       "           -2.1448e-01, -2.2385e-02],\n",
       "          [-4.8438e-01, -1.6479e-01, -4.0186e-01,  ..., -3.3057e-01,\n",
       "           -1.1908e-01,  7.2510e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.6627e-03, -1.9817e-03, -3.3073e-03,  ...,  1.5312e-02,\n",
       "            1.1223e-02, -4.8027e-03],\n",
       "          [-1.2769e-01,  1.4795e-01, -1.1505e-01,  ...,  6.4514e-02,\n",
       "            2.1912e-01, -2.6733e-01],\n",
       "          [ 8.1543e-02, -1.4526e-01, -1.6199e-01,  ...,  2.4951e-01,\n",
       "            2.2980e-02, -1.0541e-01],\n",
       "          ...,\n",
       "          [ 4.0161e-01,  3.3264e-02,  2.8183e-02,  ...,  2.5711e-02,\n",
       "            3.0005e-01,  6.1607e-04],\n",
       "          [ 2.4033e-02, -1.9958e-01,  1.4233e-01,  ..., -9.3201e-02,\n",
       "            3.0322e-01, -6.2805e-02],\n",
       "          [-3.0441e-02,  2.2949e-01, -2.5049e-01,  ..., -9.7473e-02,\n",
       "            1.6492e-01,  8.8867e-02]],\n",
       "\n",
       "         [[ 5.9929e-03, -1.4503e-02,  8.7280e-03,  ..., -8.5220e-03,\n",
       "            6.1722e-03,  8.2626e-03],\n",
       "          [ 1.6101e-01,  8.2092e-02, -3.6548e-01,  ...,  2.1960e-01,\n",
       "           -2.0447e-01, -2.4731e-01],\n",
       "          [ 2.9395e-01, -8.8623e-02,  4.2456e-01,  ...,  1.8555e-01,\n",
       "            1.3208e-01, -2.1509e-01],\n",
       "          ...,\n",
       "          [-6.4819e-02,  1.5186e-01, -1.1041e-01,  ...,  1.4917e-01,\n",
       "           -4.7943e-02,  5.6946e-02],\n",
       "          [ 2.9956e-01,  4.9048e-01,  3.4473e-01,  ...,  1.7749e-01,\n",
       "            6.0852e-02,  1.7456e-01],\n",
       "          [ 5.2148e-01,  2.5757e-01,  3.5083e-01,  ..., -3.3276e-01,\n",
       "            4.9683e-01, -2.3340e-01]],\n",
       "\n",
       "         [[-2.7809e-03,  2.4414e-03,  1.6127e-03,  ..., -7.5817e-04,\n",
       "           -1.7414e-03, -7.8583e-03],\n",
       "          [ 2.2034e-01, -1.7773e-01, -3.4106e-01,  ..., -2.7539e-01,\n",
       "           -3.6926e-02,  2.8101e-01],\n",
       "          [ 9.7473e-02, -3.7915e-01,  1.1572e-01,  ...,  1.1639e-01,\n",
       "            1.8225e-01, -1.9028e-02],\n",
       "          ...,\n",
       "          [-9.1248e-02,  2.1094e-01,  4.4775e-01,  ...,  8.2458e-02,\n",
       "            2.3514e-02,  1.2421e-01],\n",
       "          [-6.6284e-02,  3.2666e-01, -3.2166e-02,  ..., -1.1407e-01,\n",
       "           -6.2164e-02,  1.8616e-02],\n",
       "          [-3.5620e-01, -2.9980e-01, -4.3701e-01,  ..., -1.9873e-01,\n",
       "            7.1289e-02, -1.4343e-03]]],\n",
       "\n",
       "\n",
       "        [[[-5.1346e-03,  3.6602e-03,  1.6680e-03,  ...,  2.8419e-03,\n",
       "            1.5478e-03, -2.0866e-03],\n",
       "          [-3.1348e-01, -4.4647e-02,  8.1360e-02,  ..., -1.2488e-01,\n",
       "            1.3782e-01,  7.8247e-02],\n",
       "          [-6.2683e-02,  2.3523e-01,  1.4900e-02,  ..., -1.3318e-01,\n",
       "            1.3260e-02, -4.1412e-02],\n",
       "          ...,\n",
       "          [ 3.1348e-01, -2.9663e-01,  2.2571e-01,  ...,  3.2617e-01,\n",
       "            9.4299e-02, -1.3252e-02],\n",
       "          [ 3.5376e-01, -2.6343e-01,  1.9946e-01,  ...,  3.4521e-01,\n",
       "            6.4087e-02,  3.2288e-02],\n",
       "          [ 3.7524e-01, -2.6367e-01,  1.7224e-01,  ...,  2.7881e-01,\n",
       "            9.7046e-02,  5.7129e-02]],\n",
       "\n",
       "         [[ 1.2016e-02, -3.8242e-03, -1.6693e-02,  ..., -3.3264e-03,\n",
       "           -2.6417e-03, -3.0441e-03],\n",
       "          [ 1.0809e-01,  1.3660e-01, -7.0763e-03,  ...,  9.8999e-02,\n",
       "           -2.9956e-01,  8.3313e-02],\n",
       "          [-2.3083e-01,  4.0100e-02, -9.6741e-02,  ...,  3.0960e-02,\n",
       "           -1.6785e-01, -5.3345e-02],\n",
       "          ...,\n",
       "          [ 1.8652e-01, -4.7211e-02, -1.6345e-01,  ...,  1.9312e-01,\n",
       "           -9.2163e-02,  6.8115e-02],\n",
       "          [ 1.3367e-01, -1.9730e-02, -1.7493e-01,  ...,  1.6528e-01,\n",
       "           -1.1743e-01,  8.3374e-02],\n",
       "          [ 9.7046e-02, -2.2354e-03, -1.9031e-01,  ...,  1.2610e-01,\n",
       "           -8.3069e-02,  8.1116e-02]],\n",
       "\n",
       "         [[-7.3967e-03,  6.5517e-04,  2.0924e-03,  ...,  7.7209e-03,\n",
       "           -7.4577e-04, -1.4811e-03],\n",
       "          [-3.9380e-01, -2.7115e-02, -9.2850e-03,  ..., -9.4421e-02,\n",
       "            1.0559e-01, -9.1324e-03],\n",
       "          [-1.5100e-01,  2.1576e-02, -2.6367e-01,  ..., -9.9426e-02,\n",
       "           -3.7903e-02, -2.3877e-01],\n",
       "          ...,\n",
       "          [-3.0079e-03, -2.8296e-01, -2.1436e-01,  ...,  1.4636e-01,\n",
       "            1.1755e-01,  5.0000e-01],\n",
       "          [ 1.9012e-02, -2.6221e-01, -2.1191e-01,  ...,  1.4807e-01,\n",
       "            1.5588e-01,  4.8877e-01],\n",
       "          [ 3.7079e-02, -2.5928e-01, -1.9751e-01,  ...,  1.3550e-01,\n",
       "            1.7346e-01,  4.5117e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.6627e-03, -1.9817e-03, -3.3073e-03,  ...,  1.5312e-02,\n",
       "            1.1223e-02, -4.8027e-03],\n",
       "          [-1.2769e-01,  1.4795e-01, -1.1505e-01,  ...,  6.4514e-02,\n",
       "            2.1912e-01, -2.6733e-01],\n",
       "          [ 3.2520e-01, -1.7224e-01, -1.7480e-01,  ...,  5.9845e-02,\n",
       "            6.4270e-02,  3.1592e-01],\n",
       "          ...,\n",
       "          [ 2.1289e-01, -1.1330e-03, -1.4478e-01,  ...,  2.6123e-01,\n",
       "           -3.5229e-01, -8.2703e-02],\n",
       "          [ 2.2388e-01,  1.9608e-02, -8.8989e-02,  ...,  3.1274e-01,\n",
       "           -3.7622e-01, -5.9448e-02],\n",
       "          [ 2.4097e-01,  1.4435e-02, -7.9651e-02,  ...,  3.1274e-01,\n",
       "           -3.2715e-01, -3.5065e-02]],\n",
       "\n",
       "         [[ 5.9929e-03, -1.4503e-02,  8.7280e-03,  ..., -8.5220e-03,\n",
       "            6.1722e-03,  8.2626e-03],\n",
       "          [ 1.6101e-01,  8.2092e-02, -3.6548e-01,  ...,  2.1960e-01,\n",
       "           -2.0447e-01, -2.4731e-01],\n",
       "          [-1.5063e-01, -1.6956e-01,  2.8052e-01,  ...,  1.0095e-01,\n",
       "            2.5171e-01, -1.1157e-01],\n",
       "          ...,\n",
       "          [-2.0172e-02, -4.0344e-02,  8.6609e-02,  ..., -8.1604e-02,\n",
       "            4.2188e-01, -4.5654e-01],\n",
       "          [-4.3701e-02,  4.5319e-03,  3.8635e-02,  ..., -6.6101e-02,\n",
       "            4.2969e-01, -4.6484e-01],\n",
       "          [-4.7485e-02, -5.8594e-03,  5.2734e-02,  ..., -8.8745e-02,\n",
       "            4.0820e-01, -4.6973e-01]],\n",
       "\n",
       "         [[-2.7809e-03,  2.4414e-03,  1.6127e-03,  ..., -7.5817e-04,\n",
       "           -1.7414e-03, -7.8583e-03],\n",
       "          [ 2.2034e-01, -1.7773e-01, -3.4106e-01,  ..., -2.7539e-01,\n",
       "           -3.6926e-02,  2.8101e-01],\n",
       "          [ 4.5288e-02, -2.7466e-01, -2.0776e-01,  ...,  2.8174e-01,\n",
       "            1.3023e-02, -1.8555e-02],\n",
       "          ...,\n",
       "          [ 1.0681e-01, -5.2637e-01,  4.2969e-02,  ..., -1.8387e-02,\n",
       "           -1.7908e-01, -3.6743e-02],\n",
       "          [ 5.6976e-02, -4.8413e-01,  6.4148e-02,  ..., -4.1870e-02,\n",
       "           -1.6821e-01, -2.7863e-02],\n",
       "          [ 7.6477e-02, -4.4116e-01,  1.0223e-01,  ..., -5.0568e-02,\n",
       "           -1.5027e-01, -3.8696e-02]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.7700e-03,  6.8665e-03,  3.0975e-03,  ..., -1.2988e-01,\n",
       "            1.3391e-01,  2.1692e-01],\n",
       "          [-1.2988e+00, -2.1057e-01,  9.9609e-01,  ..., -2.0142e-01,\n",
       "           -5.8203e-01,  1.5137e-02],\n",
       "          [-1.9111e+00,  3.2129e-01,  1.9932e+00,  ..., -2.1240e-01,\n",
       "           -5.7129e-02, -4.4312e-01],\n",
       "          ...,\n",
       "          [ 2.3203e+00,  2.9004e-01, -1.0244e+00,  ...,  2.1777e+00,\n",
       "            6.0596e-01,  2.3750e+00],\n",
       "          [ 1.9849e-01,  1.8604e-01,  3.2275e-01,  ...,  2.0234e+00,\n",
       "           -1.7354e+00,  1.8516e+00],\n",
       "          [ 5.8594e-02,  2.6123e-01,  8.4229e-01,  ..., -1.0850e+00,\n",
       "           -2.6035e+00, -1.2720e-01]],\n",
       "\n",
       "         [[-2.1362e-03, -1.5442e-02,  1.8921e-03,  ...,  8.0762e-01,\n",
       "            6.4697e-01,  5.8545e-01],\n",
       "          [ 7.6416e-01, -1.5801e+00, -1.3447e+00,  ...,  4.8706e-01,\n",
       "           -9.6924e-01,  2.4395e+00],\n",
       "          [-2.9199e+00,  2.4487e-01, -1.2500e+00,  ...,  1.1737e-01,\n",
       "            1.5400e+00,  1.0889e+00],\n",
       "          ...,\n",
       "          [-1.2793e-01, -5.8691e-01,  1.4766e+00,  ...,  6.5430e-01,\n",
       "            1.1934e+00,  1.5303e+00],\n",
       "          [-1.0486e-01, -1.4185e-01,  1.5391e+00,  ...,  4.1113e-01,\n",
       "            1.3213e+00,  1.3203e+00],\n",
       "          [ 1.7227e+00,  7.2754e-01,  1.6162e+00,  ...,  1.1758e+00,\n",
       "            9.2822e-01,  6.9287e-01]],\n",
       "\n",
       "         [[-9.3994e-03,  7.7133e-03,  8.2092e-03,  ..., -1.5222e-01,\n",
       "           -3.7891e-01, -3.2739e-01],\n",
       "          [-3.5625e+00, -1.0293e+00, -2.0352e+00,  ...,  8.4033e-01,\n",
       "           -1.2061e+00,  1.1211e+00],\n",
       "          [ 3.3047e+00, -1.2324e+00, -5.6289e+00,  ..., -1.9312e-01,\n",
       "           -1.0098e+00, -1.3086e+00],\n",
       "          ...,\n",
       "          [ 2.9707e+00, -1.6738e+00,  1.1914e+00,  ..., -4.8975e-01,\n",
       "            4.3262e-01,  1.7847e-01],\n",
       "          [ 2.1367e+00, -3.3145e+00,  1.6367e+00,  ..., -3.1421e-01,\n",
       "           -2.3169e-01,  1.9141e-01],\n",
       "          [-2.9023e+00, -3.6641e+00,  1.6641e+00,  ...,  7.2998e-01,\n",
       "           -7.2754e-02,  3.1616e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.2419e-03,  1.2787e-02,  1.0712e-02,  ..., -8.4668e-01,\n",
       "            7.1631e-01,  5.0293e-01],\n",
       "          [ 1.0635e+00, -1.0576e+00,  5.3613e-01,  ..., -1.7061e+00,\n",
       "           -4.1577e-01,  4.6924e-01],\n",
       "          [ 3.7695e+00, -1.5605e+00,  1.2656e+00,  ...,  1.5059e+00,\n",
       "            1.2158e+00,  1.5508e+00],\n",
       "          ...,\n",
       "          [-2.2637e+00, -1.6133e+00, -1.5127e+00,  ...,  2.3477e+00,\n",
       "           -1.0327e-01, -6.8506e-01],\n",
       "          [-5.6641e-01, -4.1992e-01, -1.0000e+00,  ...,  2.9434e+00,\n",
       "            6.4258e-01,  1.1484e+00],\n",
       "          [-7.2510e-01,  4.8340e-01, -7.9590e-01,  ...,  2.8555e+00,\n",
       "            1.4575e-01, -1.2396e-01]],\n",
       "\n",
       "         [[ 9.5520e-03,  1.7822e-02,  4.8981e-03,  ...,  2.7686e-01,\n",
       "            1.7510e+00, -5.0098e-01],\n",
       "          [-1.0034e-01, -8.9795e-01, -3.0762e-02,  ...,  1.5186e+00,\n",
       "           -3.9668e+00,  1.5840e+00],\n",
       "          [-1.3291e+00, -1.5713e+00,  1.0322e+00,  ...,  2.3477e+00,\n",
       "           -5.5781e+00,  1.4424e+00],\n",
       "          ...,\n",
       "          [ 9.9414e-01, -1.1699e+00, -9.8340e-01,  ..., -6.2549e-01,\n",
       "           -2.9102e+00,  6.9385e-01],\n",
       "          [ 1.5125e-01, -4.1748e-01, -3.7842e-02,  ..., -2.5781e-01,\n",
       "           -1.2314e+00, -5.7037e-02],\n",
       "          [ 4.2285e-01, -3.1641e-01, -6.5918e-01,  ..., -4.2236e-01,\n",
       "           -2.5684e+00,  1.2861e+00]],\n",
       "\n",
       "         [[ 9.1553e-04, -1.2634e-02, -4.2038e-03,  ...,  1.9800e-01,\n",
       "           -1.6968e-01,  5.4150e-01],\n",
       "          [-1.5312e+00,  8.4814e-01, -1.1641e+00,  ...,  5.0732e-01,\n",
       "            7.1826e-01,  5.9424e-01],\n",
       "          [ 1.9678e+00,  1.6240e+00,  8.0664e-01,  ...,  1.7432e+00,\n",
       "            1.4434e+00,  1.5649e-01],\n",
       "          ...,\n",
       "          [ 1.0215e+00,  3.7500e-01, -7.2754e-01,  ..., -5.1641e+00,\n",
       "            1.4248e+00,  2.4746e+00],\n",
       "          [ 1.5356e-01,  2.1277e-01,  2.1896e-02,  ..., -2.0410e+00,\n",
       "            7.0361e-01, -7.7332e-02],\n",
       "          [-1.7139e+00,  1.7139e-01, -5.9375e-01,  ..., -7.5146e-01,\n",
       "           -1.9678e-01,  6.8652e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.7700e-03,  6.8665e-03,  3.0975e-03,  ..., -1.2988e-01,\n",
       "            1.3391e-01,  2.1692e-01],\n",
       "          [-1.2988e+00, -2.1057e-01,  9.9609e-01,  ..., -2.0142e-01,\n",
       "           -5.8203e-01,  1.5137e-02],\n",
       "          [-1.8818e+00,  8.3936e-01,  1.8086e+00,  ...,  4.6558e-01,\n",
       "            8.7402e-01, -1.2109e+00],\n",
       "          ...,\n",
       "          [ 6.3818e-01,  1.5625e-01, -1.0010e+00,  ..., -1.8398e+00,\n",
       "           -1.3809e+00,  9.6741e-02],\n",
       "          [ 4.3652e-01, -4.4312e-02, -1.1797e+00,  ..., -1.9492e+00,\n",
       "           -1.4043e+00,  1.4697e-01],\n",
       "          [-1.3574e-01, -1.4966e-01, -9.0625e-01,  ..., -1.9404e+00,\n",
       "           -1.1982e+00,  1.7603e-01]],\n",
       "\n",
       "         [[-2.1362e-03, -1.5442e-02,  1.8921e-03,  ...,  8.0762e-01,\n",
       "            6.4697e-01,  5.8545e-01],\n",
       "          [ 7.6416e-01, -1.5801e+00, -1.3447e+00,  ...,  4.8706e-01,\n",
       "           -9.6924e-01,  2.4395e+00],\n",
       "          [-2.9492e+00,  1.7725e-01, -1.5332e+00,  ...,  7.4414e-01,\n",
       "            1.7598e+00,  7.2021e-01],\n",
       "          ...,\n",
       "          [-2.6270e+00,  2.2510e-01,  1.3594e+00,  ...,  2.9248e-01,\n",
       "            1.0181e-01,  5.4590e-01],\n",
       "          [ 5.7031e-01,  5.9619e-01,  2.0586e+00,  ...,  2.3267e-01,\n",
       "            8.5083e-02,  6.9580e-01],\n",
       "          [ 3.2441e+00,  6.5674e-01,  1.9844e+00,  ...,  3.1885e-01,\n",
       "            9.9731e-02,  7.6611e-01]],\n",
       "\n",
       "         [[-9.3994e-03,  7.7133e-03,  8.2092e-03,  ..., -1.5222e-01,\n",
       "           -3.7891e-01, -3.2739e-01],\n",
       "          [-3.5625e+00, -1.0293e+00, -2.0352e+00,  ...,  8.4033e-01,\n",
       "           -1.2061e+00,  1.1211e+00],\n",
       "          [ 3.0742e+00, -1.3320e+00, -5.5781e+00,  ...,  5.0659e-02,\n",
       "           -1.1797e+00, -8.8574e-01],\n",
       "          ...,\n",
       "          [ 4.4453e+00, -4.7852e-01,  1.9355e+00,  ..., -1.0586e+00,\n",
       "           -3.3691e-01,  2.6123e-01],\n",
       "          [ 9.3750e-02, -2.5234e+00,  2.4316e+00,  ..., -9.6289e-01,\n",
       "           -2.0337e-01,  3.5693e-01],\n",
       "          [-4.3711e+00, -3.1738e+00,  1.9707e+00,  ..., -1.0059e+00,\n",
       "           -3.5547e-01,  2.3132e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.2419e-03,  1.2787e-02,  1.0712e-02,  ..., -8.4668e-01,\n",
       "            7.1631e-01,  5.0293e-01],\n",
       "          [ 1.0635e+00, -1.0576e+00,  5.3613e-01,  ..., -1.7061e+00,\n",
       "           -4.1577e-01,  4.6924e-01],\n",
       "          [ 3.9004e+00, -1.2305e+00,  1.3828e+00,  ...,  3.6963e-01,\n",
       "            1.3652e+00,  1.7090e+00],\n",
       "          ...,\n",
       "          [-9.9951e-01, -7.3633e-01, -8.5742e-01,  ...,  5.4883e-01,\n",
       "           -4.0649e-02,  9.0332e-01],\n",
       "          [-2.6133e+00,  7.0117e-01, -1.4082e+00,  ...,  5.9912e-01,\n",
       "           -2.5488e-01,  8.6426e-01],\n",
       "          [-1.7324e+00,  1.7754e+00, -1.4229e+00,  ...,  5.3906e-01,\n",
       "           -2.3047e-01,  9.5850e-01]],\n",
       "\n",
       "         [[ 9.5520e-03,  1.7822e-02,  4.8981e-03,  ...,  2.7686e-01,\n",
       "            1.7510e+00, -5.0098e-01],\n",
       "          [-1.0034e-01, -8.9795e-01, -3.0762e-02,  ...,  1.5186e+00,\n",
       "           -3.9668e+00,  1.5840e+00],\n",
       "          [-1.1611e+00, -1.1816e+00,  1.0938e+00,  ...,  2.1836e+00,\n",
       "           -5.7656e+00,  2.2637e+00],\n",
       "          ...,\n",
       "          [-7.5439e-02, -1.2512e-01, -1.5710e-01,  ..., -4.3555e-01,\n",
       "           -3.6758e+00,  1.0449e+00],\n",
       "          [ 1.4795e-01,  1.0231e-02, -4.9805e-01,  ..., -5.7520e-01,\n",
       "           -3.4180e+00,  7.7539e-01],\n",
       "          [ 2.5049e-01,  1.9946e-01, -5.6348e-01,  ..., -3.7646e-01,\n",
       "           -3.4395e+00,  1.1094e+00]],\n",
       "\n",
       "         [[ 9.1553e-04, -1.2634e-02, -4.2038e-03,  ...,  1.9800e-01,\n",
       "           -1.6968e-01,  5.4150e-01],\n",
       "          [-1.5312e+00,  8.4814e-01, -1.1641e+00,  ...,  5.0732e-01,\n",
       "            7.1826e-01,  5.9424e-01],\n",
       "          [ 1.5811e+00,  1.9082e+00,  5.1025e-01,  ...,  9.5801e-01,\n",
       "            3.2788e-01,  6.7139e-01],\n",
       "          ...,\n",
       "          [ 2.4492e+00,  9.7803e-01, -5.1270e-02,  ..., -3.4155e-01,\n",
       "           -3.1836e-01,  4.3506e-01],\n",
       "          [-3.1006e-02,  1.7393e+00, -1.2783e+00,  ..., -5.1416e-01,\n",
       "           -3.8428e-01,  3.0640e-01],\n",
       "          [-2.5938e+00,  1.4795e+00, -1.9844e+00,  ..., -3.0664e-01,\n",
       "           -3.4521e-01,  1.9482e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 5.1727e-03, -2.6989e-04,  1.2703e-03,  ..., -1.1969e-03,\n",
       "            8.6212e-04,  7.5836e-03],\n",
       "          [-1.7920e-01,  5.3662e-01, -1.0056e-02,  ..., -5.3284e-02,\n",
       "            2.5464e-01,  1.7712e-01],\n",
       "          [-3.9624e-01,  1.1597e-01,  4.2188e-01,  ...,  5.6885e-02,\n",
       "            4.7302e-02, -1.1218e-01],\n",
       "          ...,\n",
       "          [-2.8760e-01, -7.8491e-02,  1.8848e-01,  ...,  9.1736e-02,\n",
       "            2.2171e-02, -1.5686e-01],\n",
       "          [-1.8738e-02,  5.7178e-01,  5.3320e-01,  ...,  1.4856e-01,\n",
       "            3.9581e-02,  5.4047e-02],\n",
       "          [-1.4685e-01, -2.5708e-01, -2.2217e-01,  ..., -9.2224e-02,\n",
       "            2.6764e-02, -5.0684e-01]],\n",
       "\n",
       "         [[-7.4425e-03,  1.8829e-02, -2.0752e-03,  ...,  1.6699e-03,\n",
       "           -2.9144e-03,  5.7755e-03],\n",
       "          [ 3.9886e-02,  1.3513e-01, -9.2651e-02,  ..., -1.4244e-02,\n",
       "           -1.8713e-01, -7.5378e-02],\n",
       "          [ 2.8687e-02, -3.5522e-01,  1.3293e-01,  ..., -2.6428e-02,\n",
       "           -1.5613e-01, -1.5076e-01],\n",
       "          ...,\n",
       "          [ 3.8501e-01,  5.6824e-02,  1.5161e-01,  ...,  3.5767e-01,\n",
       "           -6.1676e-02, -2.8418e-01],\n",
       "          [-5.2795e-02, -1.2323e-01, -8.2703e-02,  ...,  2.4231e-01,\n",
       "           -2.0605e-01, -6.8359e-03],\n",
       "          [ 3.5797e-02,  1.6980e-01, -2.0520e-01,  ...,  1.3538e-01,\n",
       "           -4.6326e-02,  2.5482e-03]],\n",
       "\n",
       "         [[-6.3477e-03, -2.6894e-03, -1.2512e-02,  ..., -5.3596e-03,\n",
       "           -2.2125e-04, -4.3945e-03],\n",
       "          [ 9.7198e-03, -1.6724e-01, -1.0742e-02,  ...,  1.0834e-01,\n",
       "            3.1177e-01, -1.1823e-01],\n",
       "          [-2.7832e-01,  5.7520e-01,  5.9619e-01,  ...,  5.2063e-02,\n",
       "            2.0203e-01, -3.2806e-02],\n",
       "          ...,\n",
       "          [ 7.7057e-03,  4.1699e-01,  8.3435e-02,  ...,  1.6528e-01,\n",
       "            2.7130e-02, -2.1805e-02],\n",
       "          [ 1.0828e-01, -1.5405e-01,  2.4353e-01,  ..., -7.1594e-02,\n",
       "            1.7554e-01,  1.4087e-01],\n",
       "          [ 1.2964e-01, -2.4707e-01,  8.6487e-02,  ...,  1.1395e-01,\n",
       "            7.4951e-02,  2.8613e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.5008e-03,  9.5215e-03,  1.3405e-02,  ...,  7.4081e-03,\n",
       "            3.9444e-03,  7.4005e-04],\n",
       "          [-6.5723e-01,  3.4448e-01,  5.8319e-02,  ..., -5.1807e-01,\n",
       "            4.6826e-01,  4.1602e-01],\n",
       "          [-1.0736e-01, -2.1301e-01,  7.8674e-02,  ..., -2.2241e-01,\n",
       "            7.3608e-02, -1.8567e-01],\n",
       "          ...,\n",
       "          [-1.7249e-01,  2.3590e-02, -1.9202e-01,  ..., -9.2651e-02,\n",
       "           -2.4390e-01,  1.4725e-02],\n",
       "          [-7.8278e-03,  2.0129e-01,  7.1350e-02,  ...,  1.3708e-01,\n",
       "            1.7834e-01, -2.2668e-01],\n",
       "          [-2.3767e-01,  4.0942e-01,  2.7405e-02,  ...,  1.7615e-01,\n",
       "            7.6782e-02, -1.1505e-01]],\n",
       "\n",
       "         [[-4.8828e-04,  3.5515e-03,  4.8584e-02,  ..., -1.9562e-02,\n",
       "            5.2223e-03,  6.4545e-03],\n",
       "          [-4.5868e-02,  1.0931e-01,  5.5969e-02,  ...,  4.8071e-01,\n",
       "           -1.4575e-01,  6.3110e-02],\n",
       "          [-2.2302e-01,  3.2788e-01,  3.0786e-01,  ..., -8.2642e-02,\n",
       "           -1.1978e-02,  1.4275e-02],\n",
       "          ...,\n",
       "          [-5.1025e-02, -1.7059e-02,  3.0838e-02,  ..., -4.7119e-01,\n",
       "            2.6831e-01, -9.7412e-02],\n",
       "          [-4.0405e-01, -4.1504e-02, -3.1494e-01,  ...,  1.6272e-01,\n",
       "           -4.4098e-03,  5.2979e-01],\n",
       "          [-3.7646e-01, -2.2815e-01,  1.4725e-02,  ..., -1.0508e+00,\n",
       "            4.6606e-01, -1.0718e-01]],\n",
       "\n",
       "         [[-3.3630e-02,  2.2449e-03, -1.8494e-02,  ...,  5.5218e-04,\n",
       "           -4.1676e-04, -1.3828e-03],\n",
       "          [-7.5989e-02,  3.5425e-01, -9.1797e-02,  ...,  1.7236e-01,\n",
       "            3.8544e-02, -3.3203e-01],\n",
       "          [ 4.5190e-01,  1.0443e-01, -2.3071e-02,  ...,  7.8003e-02,\n",
       "            1.1517e-01, -2.1436e-01],\n",
       "          ...,\n",
       "          [-5.1953e-01, -5.2051e-01, -8.9417e-02,  ..., -6.0156e-01,\n",
       "            2.2900e-01,  1.1218e-01],\n",
       "          [-3.5706e-02, -1.7297e-01, -9.6558e-02,  ...,  2.3376e-02,\n",
       "           -1.1591e-01,  1.0095e-01],\n",
       "          [-7.7667e-03,  3.7866e-01, -3.1763e-01,  ...,  4.8676e-02,\n",
       "            1.4458e-02, -1.2646e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.1727e-03, -2.6989e-04,  1.2703e-03,  ..., -1.1969e-03,\n",
       "            8.6212e-04,  7.5836e-03],\n",
       "          [-1.7920e-01,  5.3662e-01, -1.0056e-02,  ..., -5.3284e-02,\n",
       "            2.5464e-01,  1.7712e-01],\n",
       "          [-7.0374e-02,  9.7595e-02,  1.6138e-01,  ..., -1.9397e-01,\n",
       "            1.8030e-01, -1.7883e-02],\n",
       "          ...,\n",
       "          [ 2.9404e-02, -3.6133e-01,  1.6028e-01,  ..., -6.2469e-02,\n",
       "           -4.1431e-01, -3.6548e-01],\n",
       "          [ 5.6580e-02, -3.2422e-01,  1.9653e-01,  ..., -6.7932e-02,\n",
       "           -3.6011e-01, -3.4424e-01],\n",
       "          [ 6.1417e-03, -2.9883e-01,  2.7466e-01,  ..., -1.3599e-01,\n",
       "           -2.9712e-01, -3.2617e-01]],\n",
       "\n",
       "         [[-7.4425e-03,  1.8829e-02, -2.0752e-03,  ...,  1.6699e-03,\n",
       "           -2.9144e-03,  5.7755e-03],\n",
       "          [ 3.9886e-02,  1.3513e-01, -9.2651e-02,  ..., -1.4244e-02,\n",
       "           -1.8713e-01, -7.5378e-02],\n",
       "          [ 1.3367e-01, -1.9153e-01, -6.5125e-02,  ..., -5.9033e-01,\n",
       "            2.0093e-01,  2.1436e-01],\n",
       "          ...,\n",
       "          [ 2.5537e-01, -3.2666e-01, -5.6885e-02,  ..., -2.1521e-01,\n",
       "            2.4988e-01, -2.6685e-01],\n",
       "          [ 3.3032e-01, -2.8296e-01,  2.5345e-02,  ..., -1.5393e-01,\n",
       "            1.8372e-01, -2.8198e-01],\n",
       "          [ 2.7100e-01, -2.6611e-01,  3.2257e-02,  ..., -1.1945e-01,\n",
       "            2.3865e-01, -2.6367e-01]],\n",
       "\n",
       "         [[-6.3477e-03, -2.6894e-03, -1.2512e-02,  ..., -5.3596e-03,\n",
       "           -2.2125e-04, -4.3945e-03],\n",
       "          [ 9.7198e-03, -1.6724e-01, -1.0742e-02,  ...,  1.0834e-01,\n",
       "            3.1177e-01, -1.1823e-01],\n",
       "          [-5.1361e-02,  2.4829e-01,  4.5288e-01,  ...,  3.1128e-01,\n",
       "           -2.6514e-01, -3.8483e-02],\n",
       "          ...,\n",
       "          [ 3.7061e-01,  1.6064e-01,  3.7549e-01,  ...,  1.8481e-01,\n",
       "           -2.8491e-01,  7.3486e-02],\n",
       "          [ 3.3301e-01,  1.3647e-01,  4.0112e-01,  ...,  1.7871e-01,\n",
       "           -2.3132e-01,  7.1289e-02],\n",
       "          [ 3.5864e-01,  1.7688e-01,  4.8218e-01,  ...,  1.3843e-01,\n",
       "           -1.8860e-01,  9.3384e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.5008e-03,  9.5215e-03,  1.3405e-02,  ...,  7.4081e-03,\n",
       "            3.9444e-03,  7.4005e-04],\n",
       "          [-6.5723e-01,  3.4448e-01,  5.8319e-02,  ..., -5.1807e-01,\n",
       "            4.6826e-01,  4.1602e-01],\n",
       "          [-3.3960e-01,  2.6025e-01,  1.5869e-01,  ..., -3.9136e-01,\n",
       "           -2.0239e-01,  1.0674e-02],\n",
       "          ...,\n",
       "          [ 2.7441e-01,  2.1741e-01,  3.7811e-02,  ..., -7.8552e-02,\n",
       "           -5.8174e-05, -1.3831e-01],\n",
       "          [ 2.6855e-01,  2.3474e-01,  6.6101e-02,  ..., -8.4229e-02,\n",
       "           -5.7487e-03, -1.5723e-01],\n",
       "          [ 2.7979e-01,  2.4109e-01, -1.2611e-02,  ..., -5.9906e-02,\n",
       "            1.9943e-02, -1.7822e-01]],\n",
       "\n",
       "         [[-4.8828e-04,  3.5515e-03,  4.8584e-02,  ..., -1.9562e-02,\n",
       "            5.2223e-03,  6.4545e-03],\n",
       "          [-4.5868e-02,  1.0931e-01,  5.5969e-02,  ...,  4.8071e-01,\n",
       "           -1.4575e-01,  6.3110e-02],\n",
       "          [ 3.6621e-03,  2.9980e-01,  4.8828e-01,  ..., -9.9609e-02,\n",
       "            1.3000e-01, -9.0698e-02],\n",
       "          ...,\n",
       "          [-1.8082e-02, -1.4478e-01, -9.5520e-02,  ..., -7.1484e-01,\n",
       "            2.7148e-01, -3.0420e-01],\n",
       "          [-5.9113e-02, -9.7473e-02, -1.3232e-01,  ..., -6.8457e-01,\n",
       "            2.8589e-01, -2.9980e-01],\n",
       "          [-1.2802e-02, -6.5857e-02, -1.1176e-01,  ..., -7.6318e-01,\n",
       "            3.3936e-01, -3.4375e-01]],\n",
       "\n",
       "         [[-3.3630e-02,  2.2449e-03, -1.8494e-02,  ...,  5.5218e-04,\n",
       "           -4.1676e-04, -1.3828e-03],\n",
       "          [-7.5989e-02,  3.5425e-01, -9.1797e-02,  ...,  1.7236e-01,\n",
       "            3.8544e-02, -3.3203e-01],\n",
       "          [ 8.2092e-02,  3.8281e-01, -5.3345e-02,  ..., -2.3621e-02,\n",
       "            2.3547e-01, -2.4744e-01],\n",
       "          ...,\n",
       "          [-3.8135e-01, -9.5398e-02, -1.8384e-01,  ...,  1.0101e-02,\n",
       "           -1.8896e-01, -3.7231e-01],\n",
       "          [-4.0967e-01, -1.0590e-01, -2.2339e-01,  ...,  4.2206e-02,\n",
       "           -1.9751e-01, -3.7402e-01],\n",
       "          [-4.0234e-01, -1.1621e-01, -1.6394e-01,  ..., -7.2403e-03,\n",
       "           -1.6565e-01, -3.5425e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.7944e-02, -2.3773e-02, -3.3569e-03,  ...,  1.5442e-02,\n",
       "            3.9697e-01, -2.4438e-01],\n",
       "          [ 3.1445e+00,  1.3955e+00,  5.8203e-01,  ...,  8.9062e-01,\n",
       "            1.0332e+00,  2.2681e-01],\n",
       "          [-6.6309e-01, -1.6133e+00, -4.2407e-01,  ...,  2.1216e-01,\n",
       "            1.7891e+00, -6.3184e-01],\n",
       "          ...,\n",
       "          [-4.3477e+00,  2.5879e+00, -3.4277e+00,  ...,  2.6172e-01,\n",
       "           -1.7354e+00,  6.9629e-01],\n",
       "          [-8.9355e-01, -4.1748e-01, -7.0557e-01,  ...,  2.2852e+00,\n",
       "           -9.2969e-01,  5.5450e-02],\n",
       "          [ 4.1719e+00, -2.3457e+00,  1.9570e+00,  ...,  1.3457e+00,\n",
       "           -1.1689e+00, -8.6523e-01]],\n",
       "\n",
       "         [[ 2.6764e-02,  5.3711e-03, -1.4534e-02,  ...,  9.7961e-02,\n",
       "            7.0251e-02, -4.5074e-02],\n",
       "          [-5.3613e-01, -7.2070e-01,  4.5459e-01,  ..., -3.3862e-01,\n",
       "           -1.9238e-01, -1.4551e+00],\n",
       "          [-1.0664e+00,  7.4463e-01, -1.2439e-01,  ...,  5.0000e-01,\n",
       "           -8.4961e-01, -1.6055e+00],\n",
       "          ...,\n",
       "          [ 1.5215e+00, -7.5439e-01,  2.3096e-01,  ...,  5.3027e-01,\n",
       "           -1.6172e+00, -2.1016e+00],\n",
       "          [-2.2925e-01, -1.1398e-02,  7.2571e-02,  ..., -2.3694e-01,\n",
       "           -1.1914e+00, -1.2803e+00],\n",
       "          [-3.4424e-02,  7.1973e-01,  4.0381e-01,  ...,  5.5176e-01,\n",
       "            4.8047e-01, -3.9014e-01]],\n",
       "\n",
       "         [[-4.0436e-03, -1.3588e-02, -1.3130e-02,  ...,  8.9233e-02,\n",
       "           -7.9102e-02, -1.7402e+00],\n",
       "          [ 2.0156e+00,  9.6631e-01,  1.1035e+00,  ..., -2.1543e+00,\n",
       "            9.6973e-01,  3.0781e+00],\n",
       "          [ 1.0039e+00,  3.7891e+00, -1.5273e+00,  ...,  4.8193e-01,\n",
       "           -4.2139e-01,  3.8125e+00],\n",
       "          ...,\n",
       "          [-2.0781e+00,  8.7695e-01, -1.2100e+00,  ..., -1.9297e+00,\n",
       "           -1.5557e+00,  8.0859e+00],\n",
       "          [-3.2666e-01,  4.0015e-01, -4.9561e-02,  ..., -6.6895e-01,\n",
       "           -6.1707e-02,  6.3672e+00],\n",
       "          [ 3.4863e-01,  1.0439e+00,  1.2412e+00,  ..., -2.1035e+00,\n",
       "            4.3304e-02,  3.0195e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.1830e-02,  1.6937e-03, -1.6022e-03,  ...,  4.1357e-01,\n",
       "            1.8945e-01, -3.1909e-01],\n",
       "          [ 8.8379e-01,  1.3721e+00,  4.2944e-01,  ...,  6.9092e-01,\n",
       "            5.9717e-01,  1.2510e+00],\n",
       "          [-6.0938e-01,  1.1982e+00,  1.3145e+00,  ...,  4.0234e-01,\n",
       "            6.9275e-02,  2.6328e+00],\n",
       "          ...,\n",
       "          [-4.9170e-01,  9.4092e-01, -1.6895e+00,  ..., -4.9951e-01,\n",
       "           -4.2236e-01, -1.4478e-01],\n",
       "          [-5.4199e-01,  2.7197e-01, -4.4385e-01,  ...,  2.7979e-01,\n",
       "           -4.3311e-01,  1.8418e+00],\n",
       "          [ 7.6709e-01, -5.4688e-02, -2.8394e-01,  ...,  9.6777e-01,\n",
       "           -2.8760e-01,  2.1328e+00]],\n",
       "\n",
       "         [[-2.8992e-03,  1.0986e-03,  4.0588e-03,  ..., -1.9458e-01,\n",
       "            2.6367e-01, -9.7778e-02],\n",
       "          [ 7.7539e-01, -1.8066e+00,  6.1768e-01,  ..., -2.5605e+00,\n",
       "            3.5078e+00, -1.8477e+00],\n",
       "          [ 1.6758e+00, -1.4932e+00,  2.2227e+00,  ..., -2.2461e+00,\n",
       "            1.0215e+00,  2.5586e-01],\n",
       "          ...,\n",
       "          [-1.5215e+00, -2.1074e+00, -1.5498e+00,  ..., -1.1953e+00,\n",
       "           -1.2256e+00, -2.0527e+00],\n",
       "          [-4.9219e-01, -3.7866e-01, -7.2754e-01,  ...,  1.0156e+00,\n",
       "           -1.8662e+00, -1.8579e-01],\n",
       "          [ 3.2715e-02,  4.6924e-01, -6.7090e-01,  ...,  5.2832e-01,\n",
       "           -2.0938e+00, -4.5801e-01]],\n",
       "\n",
       "         [[ 1.1963e-02,  1.9226e-02, -3.1921e-02,  ..., -1.1877e-01,\n",
       "            1.7700e-01,  3.5303e-01],\n",
       "          [-1.1582e+00, -1.8281e+00,  1.0391e+00,  ...,  2.7715e+00,\n",
       "            1.2708e-01, -6.0400e-01],\n",
       "          [ 2.5039e+00, -1.1895e+00, -5.2100e-01,  ...,  1.5801e+00,\n",
       "           -1.8789e+00,  1.0156e+00],\n",
       "          ...,\n",
       "          [-4.7852e-01, -1.1406e+00,  3.6426e-01,  ...,  1.7517e-01,\n",
       "           -4.5312e-01,  1.0583e-01],\n",
       "          [-4.3921e-01, -1.0801e+00,  1.1494e+00,  ...,  9.4824e-01,\n",
       "            1.0449e+00,  3.2642e-01],\n",
       "          [-8.2715e-01,  2.0605e-01,  1.0010e+00,  ...,  8.8574e-01,\n",
       "           -1.0234e+00, -1.2812e+00]]],\n",
       "\n",
       "\n",
       "        [[[-1.7944e-02, -2.3773e-02, -3.3569e-03,  ...,  1.5442e-02,\n",
       "            3.9697e-01, -2.4438e-01],\n",
       "          [ 3.1445e+00,  1.3955e+00,  5.8203e-01,  ...,  8.9062e-01,\n",
       "            1.0332e+00,  2.2681e-01],\n",
       "          [-1.3730e+00, -1.9102e+00, -2.1033e-01,  ...,  2.3254e-01,\n",
       "            2.2422e+00, -1.0918e+00],\n",
       "          ...,\n",
       "          [-3.7812e+00,  2.1289e+00, -1.4316e+00,  ...,  1.9609e+00,\n",
       "           -2.5513e-01,  2.3938e-01],\n",
       "          [-7.5488e-01,  3.5400e-03, -3.5571e-01,  ...,  1.8154e+00,\n",
       "           -2.2974e-01,  1.7896e-01],\n",
       "          [ 3.0547e+00, -2.0977e+00,  9.3359e-01,  ...,  1.7900e+00,\n",
       "           -2.4915e-01,  2.6050e-01]],\n",
       "\n",
       "         [[ 2.6764e-02,  5.3711e-03, -1.4534e-02,  ...,  9.7961e-02,\n",
       "            7.0251e-02, -4.5074e-02],\n",
       "          [-5.3613e-01, -7.2070e-01,  4.5459e-01,  ..., -3.3862e-01,\n",
       "           -1.9238e-01, -1.4551e+00],\n",
       "          [-1.6719e+00,  1.1182e+00, -4.3628e-01,  ...,  8.6670e-03,\n",
       "           -4.9902e-01, -7.3340e-01],\n",
       "          ...,\n",
       "          [ 5.1855e-01, -7.5879e-01, -9.2773e-02,  ...,  1.4736e+00,\n",
       "           -8.9941e-01, -6.4160e-01],\n",
       "          [ 1.4355e+00,  2.9443e-01,  3.9819e-01,  ...,  1.3359e+00,\n",
       "           -7.3926e-01, -5.4834e-01],\n",
       "          [ 1.0469e+00,  1.2402e+00,  6.6992e-01,  ...,  1.3584e+00,\n",
       "           -9.2432e-01, -8.2861e-01]],\n",
       "\n",
       "         [[-4.0436e-03, -1.3588e-02, -1.3130e-02,  ...,  8.9233e-02,\n",
       "           -7.9102e-02, -1.7402e+00],\n",
       "          [ 2.0156e+00,  9.6631e-01,  1.1035e+00,  ..., -2.1543e+00,\n",
       "            9.6973e-01,  3.0781e+00],\n",
       "          [ 1.3525e+00,  4.3672e+00, -2.0254e+00,  ..., -4.6899e-01,\n",
       "           -9.2529e-01,  3.4004e+00],\n",
       "          ...,\n",
       "          [-2.1191e+00,  2.5439e-01, -6.6260e-01,  ..., -8.0664e-01,\n",
       "            2.5073e-01,  1.7324e+00],\n",
       "          [-1.9502e+00,  1.5078e+00,  2.2229e-01,  ..., -8.3594e-01,\n",
       "            9.2285e-02,  1.7676e+00],\n",
       "          [ 1.8359e-01,  1.8965e+00,  9.9609e-01,  ..., -9.1357e-01,\n",
       "            2.0599e-03,  1.7910e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.1830e-02,  1.6937e-03, -1.6022e-03,  ...,  4.1357e-01,\n",
       "            1.8945e-01, -3.1909e-01],\n",
       "          [ 8.8379e-01,  1.3721e+00,  4.2944e-01,  ...,  6.9092e-01,\n",
       "            5.9717e-01,  1.2510e+00],\n",
       "          [-1.0293e+00,  1.2305e+00,  1.3789e+00,  ...,  8.5059e-01,\n",
       "            1.1133e-01,  2.1445e+00],\n",
       "          ...,\n",
       "          [-3.4644e-01,  5.6055e-01, -7.3438e-01,  ...,  6.5918e-01,\n",
       "            1.2500e-01,  2.1621e+00],\n",
       "          [ 3.1030e-01,  2.4915e-01, -3.1250e-01,  ...,  7.3584e-01,\n",
       "            7.2266e-02,  2.3867e+00],\n",
       "          [ 6.8799e-01, -1.9128e-01,  2.0435e-01,  ...,  7.5488e-01,\n",
       "            2.1509e-01,  2.3984e+00]],\n",
       "\n",
       "         [[-2.8992e-03,  1.0986e-03,  4.0588e-03,  ..., -1.9458e-01,\n",
       "            2.6367e-01, -9.7778e-02],\n",
       "          [ 7.7539e-01, -1.8066e+00,  6.1768e-01,  ..., -2.5605e+00,\n",
       "            3.5078e+00, -1.8477e+00],\n",
       "          [ 1.9492e+00, -1.8076e+00,  2.1836e+00,  ..., -2.9551e+00,\n",
       "            9.9609e-01, -3.2056e-01],\n",
       "          ...,\n",
       "          [-3.3228e-01, -1.4404e+00, -6.1523e-01,  ...,  1.9141e-01,\n",
       "           -1.6172e+00, -2.5146e-01],\n",
       "          [-1.2588e+00, -1.1357e+00, -1.0000e+00,  ...,  1.3184e-01,\n",
       "           -1.3672e+00, -1.8323e-01],\n",
       "          [-1.1562e+00, -1.3525e-01, -1.0225e+00,  ...,  9.3262e-02,\n",
       "           -1.2861e+00, -1.0571e-01]],\n",
       "\n",
       "         [[ 1.1963e-02,  1.9226e-02, -3.1921e-02,  ..., -1.1877e-01,\n",
       "            1.7700e-01,  3.5303e-01],\n",
       "          [-1.1582e+00, -1.8281e+00,  1.0391e+00,  ...,  2.7715e+00,\n",
       "            1.2708e-01, -6.0400e-01],\n",
       "          [ 3.1484e+00, -1.2148e+00, -5.9570e-01,  ...,  1.2822e+00,\n",
       "           -1.4854e+00,  1.5723e+00],\n",
       "          ...,\n",
       "          [ 9.9268e-01, -1.4414e+00, -1.8860e-01,  ..., -8.6377e-01,\n",
       "           -7.3877e-01, -1.5491e-01],\n",
       "          [-7.3193e-01, -1.2705e+00,  2.5659e-01,  ..., -8.2520e-01,\n",
       "           -9.3604e-01, -3.2471e-01],\n",
       "          [-1.8486e+00, -2.4023e-01,  5.3906e-01,  ..., -9.7705e-01,\n",
       "           -9.7705e-01, -3.0176e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-8.0414e-03, -8.6288e-03, -4.7035e-03,  ...,  5.1041e-03,\n",
       "            2.6093e-03, -9.9716e-03],\n",
       "          [ 9.9854e-02, -3.4790e-02,  5.7281e-02,  ...,  2.2449e-01,\n",
       "           -2.4231e-01,  7.9712e-02],\n",
       "          [ 8.3191e-02, -2.4988e-01,  5.6982e-01,  ..., -1.6455e-01,\n",
       "            1.8420e-01, -8.2947e-02],\n",
       "          ...,\n",
       "          [-3.0347e-01,  8.3191e-02,  1.2311e-01,  ...,  2.3926e-01,\n",
       "            2.5537e-01, -1.8051e-02],\n",
       "          [ 2.6929e-01,  6.5869e-01,  2.0703e-01,  ...,  4.1089e-01,\n",
       "            1.5710e-01,  3.8574e-02],\n",
       "          [ 1.6809e-01,  7.0215e-01,  7.6141e-03,  ...,  2.1460e-01,\n",
       "            1.2146e-01, -4.1113e-01]],\n",
       "\n",
       "         [[ 3.5782e-03, -2.0485e-03,  1.0300e-04,  ..., -1.9302e-03,\n",
       "           -9.4986e-03,  6.2485e-03],\n",
       "          [-1.0780e-02,  3.9886e-02,  1.4453e-01,  ..., -5.4932e-02,\n",
       "            4.2206e-02, -1.0675e-01],\n",
       "          [ 8.9050e-02, -1.7114e-01,  1.9897e-01,  ..., -4.5435e-01,\n",
       "            2.3828e-01,  1.3916e-01],\n",
       "          ...,\n",
       "          [ 2.1948e-01, -3.7384e-04, -9.2316e-03,  ...,  3.4277e-01,\n",
       "            5.5664e-01,  6.5381e-01],\n",
       "          [ 2.9346e-01, -2.1692e-01, -3.0664e-01,  ..., -5.5811e-01,\n",
       "            1.7175e-01,  5.0537e-01],\n",
       "          [-2.7930e-01,  5.1727e-02, -2.1228e-01,  ..., -3.8599e-01,\n",
       "            1.4453e-01, -5.6915e-03]],\n",
       "\n",
       "         [[-3.6240e-04, -1.1063e-03,  3.7899e-03,  ..., -1.3762e-03,\n",
       "           -2.3899e-03,  2.6207e-03],\n",
       "          [-2.4673e-02, -3.6987e-02,  3.2593e-02,  ...,  7.8278e-03,\n",
       "            1.2524e-01,  3.4204e-01],\n",
       "          [ 2.5635e-02, -3.2166e-02,  1.5161e-01,  ...,  5.8838e-02,\n",
       "           -7.3914e-02, -6.5735e-02],\n",
       "          ...,\n",
       "          [ 1.6211e-01,  1.6663e-01,  1.4624e-01,  ..., -1.0689e-02,\n",
       "           -2.7344e-01, -2.3315e-01],\n",
       "          [ 2.4500e-01,  1.2280e-01,  6.3904e-02,  ..., -1.3062e-01,\n",
       "            5.3940e-03,  2.6831e-01],\n",
       "          [ 1.5723e-01, -3.4326e-01, -7.7515e-02,  ...,  3.2562e-02,\n",
       "            3.4302e-02,  1.2476e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.9291e-03,  5.3253e-03,  1.2970e-03,  ...,  2.6360e-03,\n",
       "            3.8910e-03, -1.0971e-02],\n",
       "          [-2.5195e-01,  3.1885e-01, -1.7273e-02,  ...,  5.4626e-02,\n",
       "           -2.7881e-01, -1.6431e-01],\n",
       "          [-3.6450e-01,  4.4037e-02, -2.3047e-01,  ..., -1.2550e-02,\n",
       "            1.5979e-01, -1.7529e-01],\n",
       "          ...,\n",
       "          [ 1.7529e-01, -4.0698e-01,  2.1460e-01,  ..., -2.3972e-02,\n",
       "           -1.4868e-01, -4.3677e-01],\n",
       "          [-9.6008e-02, -1.1536e-01,  4.9097e-01,  ..., -1.6211e-01,\n",
       "            2.2693e-01,  1.2158e-01],\n",
       "          [-1.6919e-01,  1.1908e-01,  1.3879e-01,  ..., -2.2693e-01,\n",
       "           -1.3757e-01, -2.6245e-01]],\n",
       "\n",
       "         [[ 3.2520e-03, -2.0046e-03,  1.1681e-02,  ..., -2.8107e-02,\n",
       "            6.0501e-03,  1.5907e-03],\n",
       "          [-1.3684e-01, -6.7578e-01, -9.7656e-04,  ...,  2.2974e-01,\n",
       "            1.4429e-01,  1.5833e-01],\n",
       "          [ 4.1260e-02, -2.7539e-01,  1.4490e-01,  ...,  3.0420e-01,\n",
       "           -4.0356e-01,  1.5613e-01],\n",
       "          ...,\n",
       "          [-7.9163e-02, -2.3645e-01, -2.3438e-02,  ..., -1.2622e-01,\n",
       "           -5.3406e-02, -5.2277e-02],\n",
       "          [ 2.7881e-01, -5.2051e-01, -2.1008e-01,  ..., -2.3145e-01,\n",
       "            1.7615e-01,  2.5659e-01],\n",
       "          [-4.2389e-02, -9.5020e-01, -2.1851e-01,  ...,  2.8625e-02,\n",
       "           -3.0615e-01, -7.7454e-02]],\n",
       "\n",
       "         [[ 7.0343e-03, -9.4748e-04,  1.0956e-02,  ..., -6.4163e-03,\n",
       "            9.6436e-03,  1.1322e-02],\n",
       "          [-2.5482e-02,  3.3984e-01, -3.1787e-01,  ...,  3.7231e-02,\n",
       "            4.2139e-01, -1.4917e-01],\n",
       "          [ 2.3389e-01, -2.6465e-01,  8.9233e-02,  ...,  1.7493e-01,\n",
       "           -1.7651e-01, -1.6342e-02],\n",
       "          ...,\n",
       "          [-2.9053e-01,  2.4084e-01,  8.4277e-01,  ...,  3.8501e-01,\n",
       "           -2.3279e-01, -1.4502e-01],\n",
       "          [ 9.6802e-02, -5.2643e-02,  1.8591e-01,  ..., -1.0828e-01,\n",
       "            3.2867e-02,  1.3452e-01],\n",
       "          [ 1.3904e-01,  1.3318e-01, -7.6294e-02,  ...,  2.1863e-01,\n",
       "            5.7861e-02,  4.7485e-02]]],\n",
       "\n",
       "\n",
       "        [[[-8.0414e-03, -8.6288e-03, -4.7035e-03,  ...,  5.1041e-03,\n",
       "            2.6093e-03, -9.9716e-03],\n",
       "          [ 9.9854e-02, -3.4790e-02,  5.7281e-02,  ...,  2.2449e-01,\n",
       "           -2.4231e-01,  7.9712e-02],\n",
       "          [ 2.5928e-01, -6.7871e-02,  1.8359e-01,  ..., -3.0591e-01,\n",
       "            4.2554e-01, -4.0771e-02],\n",
       "          ...,\n",
       "          [-1.7688e-01,  4.6484e-01,  2.7637e-01,  ...,  3.2129e-01,\n",
       "            2.5220e-01, -1.2256e-01],\n",
       "          [-1.6833e-01,  4.7876e-01,  2.3303e-01,  ...,  2.9858e-01,\n",
       "            2.6245e-01, -1.4514e-01],\n",
       "          [-1.2817e-01,  4.7949e-01,  2.1167e-01,  ...,  2.8906e-01,\n",
       "            2.2925e-01, -1.5100e-01]],\n",
       "\n",
       "         [[ 3.5782e-03, -2.0485e-03,  1.0300e-04,  ..., -1.9302e-03,\n",
       "           -9.4986e-03,  6.2485e-03],\n",
       "          [-1.0780e-02,  3.9886e-02,  1.4453e-01,  ..., -5.4932e-02,\n",
       "            4.2206e-02, -1.0675e-01],\n",
       "          [-1.0101e-02, -9.2041e-02,  1.2671e-01,  ..., -6.4600e-01,\n",
       "           -1.7444e-01, -1.7139e-01],\n",
       "          ...,\n",
       "          [-2.8711e-01, -1.5198e-01, -2.3462e-01,  ..., -2.2571e-01,\n",
       "            1.8469e-01, -2.4646e-01],\n",
       "          [-2.5708e-01, -1.2952e-01, -2.5342e-01,  ..., -1.6614e-01,\n",
       "            2.5049e-01, -2.4390e-01],\n",
       "          [-2.5635e-01, -1.8433e-01, -2.3364e-01,  ..., -1.2445e-01,\n",
       "            2.4121e-01, -2.4951e-01]],\n",
       "\n",
       "         [[-3.6240e-04, -1.1063e-03,  3.7899e-03,  ..., -1.3762e-03,\n",
       "           -2.3899e-03,  2.6207e-03],\n",
       "          [-2.4673e-02, -3.6987e-02,  3.2593e-02,  ...,  7.8278e-03,\n",
       "            1.2524e-01,  3.4204e-01],\n",
       "          [ 2.1606e-01, -1.2634e-01,  2.8857e-01,  ...,  1.9995e-01,\n",
       "           -1.2073e-01, -3.2166e-02],\n",
       "          ...,\n",
       "          [ 7.5073e-02, -5.0049e-03, -2.8174e-01,  ..., -4.4458e-01,\n",
       "            9.8694e-02,  1.6650e-01],\n",
       "          [ 8.4106e-02,  2.0485e-03, -2.9199e-01,  ..., -4.5996e-01,\n",
       "            3.0334e-02,  1.4575e-01],\n",
       "          [ 8.5144e-02,  4.4342e-02, -2.8662e-01,  ..., -4.5361e-01,\n",
       "            9.7351e-03,  1.9092e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.9291e-03,  5.3253e-03,  1.2970e-03,  ...,  2.6360e-03,\n",
       "            3.8910e-03, -1.0971e-02],\n",
       "          [-2.5195e-01,  3.1885e-01, -1.7273e-02,  ...,  5.4626e-02,\n",
       "           -2.7881e-01, -1.6431e-01],\n",
       "          [-4.4336e-01, -1.1133e-01, -2.9541e-01,  ...,  4.1870e-01,\n",
       "            1.9800e-01, -2.6685e-01],\n",
       "          ...,\n",
       "          [-1.7725e-01,  2.5513e-01,  1.4087e-01,  ..., -1.8494e-02,\n",
       "            2.0599e-02, -4.4946e-01],\n",
       "          [-1.8750e-01,  2.1191e-01,  1.5930e-01,  ..., -6.9275e-02,\n",
       "            6.0120e-03, -4.7998e-01],\n",
       "          [-1.6663e-01,  1.8091e-01,  1.8335e-01,  ..., -6.0059e-02,\n",
       "            4.1962e-05, -5.2734e-01]],\n",
       "\n",
       "         [[ 3.2520e-03, -2.0046e-03,  1.1681e-02,  ..., -2.8107e-02,\n",
       "            6.0501e-03,  1.5907e-03],\n",
       "          [-1.3684e-01, -6.7578e-01, -9.7656e-04,  ...,  2.2974e-01,\n",
       "            1.4429e-01,  1.5833e-01],\n",
       "          [-6.2286e-02, -4.9243e-01,  2.1582e-01,  ...,  5.8154e-01,\n",
       "           -3.3350e-01,  1.1279e-01],\n",
       "          ...,\n",
       "          [-3.5461e-02, -5.4150e-01, -2.9907e-01,  ...,  3.1006e-01,\n",
       "            3.6035e-01, -7.5836e-03],\n",
       "          [-4.2999e-02, -5.4297e-01, -2.3425e-01,  ...,  2.9419e-01,\n",
       "            3.6060e-01, -4.6478e-02],\n",
       "          [-3.0365e-03, -5.4395e-01, -2.4451e-01,  ...,  3.0444e-01,\n",
       "            3.1812e-01, -3.7170e-02]],\n",
       "\n",
       "         [[ 7.0343e-03, -9.4748e-04,  1.0956e-02,  ..., -6.4163e-03,\n",
       "            9.6436e-03,  1.1322e-02],\n",
       "          [-2.5482e-02,  3.3984e-01, -3.1787e-01,  ...,  3.7231e-02,\n",
       "            4.2139e-01, -1.4917e-01],\n",
       "          [ 2.1851e-01, -1.6516e-01, -3.6011e-02,  ...,  3.0380e-02,\n",
       "           -3.3667e-01, -3.1708e-02],\n",
       "          ...,\n",
       "          [-2.0129e-01,  8.5022e-02, -1.0089e-01,  ...,  2.7734e-01,\n",
       "           -5.2246e-01, -1.8115e-01],\n",
       "          [-2.4023e-01,  6.5735e-02, -7.4219e-02,  ...,  2.8955e-01,\n",
       "           -5.0000e-01, -1.2079e-01],\n",
       "          [-2.1692e-01,  9.8083e-02, -1.1127e-01,  ...,  2.7954e-01,\n",
       "           -4.7534e-01, -1.7236e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-2.7771e-02, -1.3046e-02, -1.4320e-02,  ..., -2.0508e-02,\n",
       "            2.8491e-01,  1.2213e-01],\n",
       "          [-3.4131e-01,  1.0889e+00,  1.6387e+00,  ...,  2.2534e-01,\n",
       "           -3.4277e-01, -2.4609e+00],\n",
       "          [ 5.0391e-01,  1.5167e-02,  4.0698e-01,  ..., -7.8271e-01,\n",
       "           -2.0156e+00, -4.8511e-01],\n",
       "          ...,\n",
       "          [-4.3433e-01,  1.3535e+00, -8.3984e-01,  ...,  2.0215e+00,\n",
       "            4.5435e-01, -3.9883e+00],\n",
       "          [ 9.2010e-03,  1.7871e-01, -3.7939e-01,  ...,  1.2109e+00,\n",
       "           -2.5970e-02, -2.9238e+00],\n",
       "          [ 1.2036e-01, -2.6367e-01,  1.5576e-01,  ...,  6.1865e-01,\n",
       "           -1.4648e-01, -1.4336e+00]],\n",
       "\n",
       "         [[ 5.3406e-04, -2.2705e-02,  2.6733e-02,  ..., -1.1310e-01,\n",
       "            4.6216e-01,  1.7273e-02],\n",
       "          [ 1.5566e+00,  1.5527e+00, -1.5928e+00,  ...,  3.9746e-01,\n",
       "            5.4834e-01, -1.0049e+00],\n",
       "          [ 2.3945e+00,  2.0156e+00, -3.1079e-01,  ..., -2.6172e+00,\n",
       "            6.9580e-03,  6.6309e-01],\n",
       "          ...,\n",
       "          [-3.3203e+00,  4.5078e+00,  1.8770e+00,  ...,  1.2878e-01,\n",
       "           -2.4377e-01,  6.9775e-01],\n",
       "          [-6.6992e-01,  7.9541e-01,  3.0908e-01,  ...,  2.7002e-01,\n",
       "           -4.0137e-01,  4.8950e-01],\n",
       "          [-1.4004e+00,  2.4414e-02, -1.2109e-01,  ..., -1.9707e+00,\n",
       "            4.0552e-01, -1.5244e+00]],\n",
       "\n",
       "         [[ 8.3466e-03, -1.8341e-02,  1.9989e-02,  ..., -1.3516e+00,\n",
       "            1.0394e-01, -7.8613e-01],\n",
       "          [ 1.7969e-01,  1.0498e-02, -3.3496e-01,  ...,  2.0273e+00,\n",
       "            9.7314e-01,  1.1367e+00],\n",
       "          [-2.8735e-01,  9.4849e-02,  2.0557e-01,  ..., -4.0332e-01,\n",
       "            1.1143e+00,  2.1406e+00],\n",
       "          ...,\n",
       "          [ 4.4287e-01,  2.4707e-01,  1.7773e-01,  ..., -7.4707e-01,\n",
       "           -4.0259e-01,  4.0156e+00],\n",
       "          [ 3.5767e-01, -7.6447e-03, -1.9104e-01,  ..., -1.1152e+00,\n",
       "            4.9805e-01,  2.9453e+00],\n",
       "          [-5.4150e-01,  6.4453e-02,  4.4263e-01,  ..., -1.9580e+00,\n",
       "           -9.3018e-01,  1.2754e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.1353e-02, -1.3763e-02,  1.4893e-02,  ..., -2.6807e-01,\n",
       "            3.8501e-01,  1.1224e-01],\n",
       "          [ 6.3623e-01,  1.0088e+00, -8.4180e-01,  ...,  4.5471e-02,\n",
       "           -6.6602e-01, -4.4678e-01],\n",
       "          [-6.5381e-01,  1.3828e+00, -5.0635e-01,  ...,  4.1431e-01,\n",
       "            5.2783e-01, -6.9727e-01],\n",
       "          ...,\n",
       "          [-2.0781e+00,  2.4980e+00,  3.2104e-01,  ...,  7.0898e-01,\n",
       "            1.1045e+00, -6.0645e-01],\n",
       "          [-7.0679e-02,  1.4082e+00, -1.7090e-01,  ...,  8.8257e-02,\n",
       "            8.8623e-01, -2.6221e-01],\n",
       "          [ 1.0000e+00,  1.4189e+00, -5.3223e-01,  ..., -1.1719e+00,\n",
       "            9.0576e-01,  2.0386e-01]],\n",
       "\n",
       "         [[-1.3611e-02, -1.6052e-02, -8.9264e-03,  ..., -4.3970e-01,\n",
       "           -6.8555e-01,  4.1333e-01],\n",
       "          [-1.7646e+00,  5.3076e-01,  5.6738e-01,  ...,  7.5195e-01,\n",
       "            5.3174e-01,  7.2559e-01],\n",
       "          [-1.1797e+00, -1.1523e+00,  2.3906e+00,  ...,  1.1172e+00,\n",
       "           -2.9785e-02,  8.1104e-01],\n",
       "          ...,\n",
       "          [ 1.7363e+00, -5.8594e-03, -8.4863e-01,  ...,  6.5332e-01,\n",
       "            4.6484e+00,  1.7959e+00],\n",
       "          [-3.8672e-01, -2.6855e-01, -5.9375e-01,  ...,  4.4849e-01,\n",
       "            2.4082e+00,  1.7754e+00],\n",
       "          [-5.6445e-01, -6.8262e-01, -1.3940e-01,  ...,  2.5664e+00,\n",
       "            7.5537e-01, -2.8882e-01]],\n",
       "\n",
       "         [[-3.1738e-03, -8.5144e-03, -2.0721e-02,  ...,  1.8662e+00,\n",
       "           -5.0098e-01, -1.7324e+00],\n",
       "          [ 9.3555e-01,  7.4512e-01,  6.5381e-01,  ...,  4.0356e-01,\n",
       "            3.7451e-01,  1.8281e+00],\n",
       "          [ 1.2490e+00, -7.8271e-01,  1.6318e+00,  ..., -2.8984e+00,\n",
       "           -6.6504e-01,  2.3105e+00],\n",
       "          ...,\n",
       "          [-1.5859e+00,  1.3008e+00, -1.0381e+00,  ..., -4.0234e+00,\n",
       "           -1.6699e+00,  3.1797e+00],\n",
       "          [-2.3572e-01, -5.5664e-01, -2.2180e-01,  ..., -3.2422e+00,\n",
       "           -1.0332e+00,  4.8516e+00],\n",
       "          [ 3.2227e-01, -3.5498e-01,  4.1895e-01,  ..., -9.5020e-01,\n",
       "            5.0781e-01,  2.3594e+00]]],\n",
       "\n",
       "\n",
       "        [[[-2.7771e-02, -1.3046e-02, -1.4320e-02,  ..., -2.0508e-02,\n",
       "            2.8491e-01,  1.2213e-01],\n",
       "          [-3.4131e-01,  1.0889e+00,  1.6387e+00,  ...,  2.2534e-01,\n",
       "           -3.4277e-01, -2.4609e+00],\n",
       "          [ 6.9189e-01,  4.8389e-01,  5.9473e-01,  ..., -2.2119e-01,\n",
       "           -1.7363e+00, -8.6426e-01],\n",
       "          ...,\n",
       "          [ 5.2832e-01,  5.4639e-01, -4.4043e-01,  ...,  4.0009e-02,\n",
       "           -6.4111e-01,  1.9177e-01],\n",
       "          [-1.4954e-01,  6.6016e-01, -6.2158e-01,  ...,  8.6792e-02,\n",
       "           -7.2168e-01,  1.4664e-02],\n",
       "          [-8.5205e-01,  4.5581e-01, -6.3770e-01,  ...,  2.2340e-04,\n",
       "           -6.6699e-01, -1.8478e-02]],\n",
       "\n",
       "         [[ 5.3406e-04, -2.2705e-02,  2.6733e-02,  ..., -1.1310e-01,\n",
       "            4.6216e-01,  1.7273e-02],\n",
       "          [ 1.5566e+00,  1.5527e+00, -1.5928e+00,  ...,  3.9746e-01,\n",
       "            5.4834e-01, -1.0049e+00],\n",
       "          [ 3.3301e+00,  1.9131e+00, -1.0938e+00,  ..., -2.3652e+00,\n",
       "           -4.8828e-01,  4.7388e-01],\n",
       "          ...,\n",
       "          [-6.3477e-03,  1.2617e+00,  1.1309e+00,  ..., -2.5605e+00,\n",
       "            4.1724e-01,  6.4307e-01],\n",
       "          [-1.8467e+00,  1.0889e+00,  1.0771e+00,  ..., -2.5410e+00,\n",
       "            3.8525e-01,  5.7178e-01],\n",
       "          [-1.7793e+00,  1.1572e-01,  5.8594e-01,  ..., -2.5098e+00,\n",
       "            3.1079e-01,  5.6934e-01]],\n",
       "\n",
       "         [[ 8.3466e-03, -1.8341e-02,  1.9989e-02,  ..., -1.3516e+00,\n",
       "            1.0394e-01, -7.8613e-01],\n",
       "          [ 1.7969e-01,  1.0498e-02, -3.3496e-01,  ...,  2.0273e+00,\n",
       "            9.7314e-01,  1.1367e+00],\n",
       "          [ 3.0957e-01,  1.8997e-02, -1.5125e-01,  ...,  1.3994e+00,\n",
       "            9.9951e-01,  7.8613e-01],\n",
       "          ...,\n",
       "          [ 4.7632e-01,  3.3984e-01,  3.4180e-01,  ...,  7.7148e-02,\n",
       "            1.1172e+00,  1.7578e-01],\n",
       "          [ 5.6885e-02,  2.4097e-01,  6.1279e-01,  ..., -2.7002e-01,\n",
       "            8.7061e-01,  4.9487e-01],\n",
       "          [-3.7671e-01,  4.8828e-04,  6.0693e-01,  ..., -3.2520e-01,\n",
       "            1.1504e+00,  4.6362e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.1353e-02, -1.3763e-02,  1.4893e-02,  ..., -2.6807e-01,\n",
       "            3.8501e-01,  1.1224e-01],\n",
       "          [ 6.3623e-01,  1.0088e+00, -8.4180e-01,  ...,  4.5471e-02,\n",
       "           -6.6602e-01, -4.4678e-01],\n",
       "          [-9.3945e-01,  1.6533e+00, -3.7354e-02,  ...,  6.7871e-01,\n",
       "            5.2588e-01, -1.9446e-01],\n",
       "          ...,\n",
       "          [-7.5293e-01,  6.5771e-01,  6.4648e-01,  ..., -8.7646e-01,\n",
       "           -1.5063e-01, -2.4878e-01],\n",
       "          [-1.4856e-01,  1.0625e+00,  2.0056e-01,  ..., -9.8975e-01,\n",
       "           -2.2681e-01, -2.6294e-01],\n",
       "          [ 5.2051e-01,  7.5391e-01, -2.5195e-01,  ..., -9.1699e-01,\n",
       "           -1.4136e-01, -2.7905e-01]],\n",
       "\n",
       "         [[-1.3611e-02, -1.6052e-02, -8.9264e-03,  ..., -4.3970e-01,\n",
       "           -6.8555e-01,  4.1333e-01],\n",
       "          [-1.7646e+00,  5.3076e-01,  5.6738e-01,  ...,  7.5195e-01,\n",
       "            5.3174e-01,  7.2559e-01],\n",
       "          [-2.0547e+00, -1.6758e+00,  2.8125e+00,  ...,  1.6035e+00,\n",
       "            4.2896e-01,  2.0239e-01],\n",
       "          ...,\n",
       "          [ 1.0674e+00,  1.0815e-01, -7.4609e-01,  ...,  5.7373e-01,\n",
       "            5.7373e-02, -8.9160e-01],\n",
       "          [ 6.3672e-01, -7.1240e-01, -9.9121e-01,  ...,  7.9102e-01,\n",
       "           -6.3232e-02, -9.6729e-01],\n",
       "          [-3.9404e-01, -1.0859e+00, -8.8916e-01,  ...,  5.3027e-01,\n",
       "            1.0474e-01, -8.0078e-01]],\n",
       "\n",
       "         [[-3.1738e-03, -8.5144e-03, -2.0721e-02,  ...,  1.8662e+00,\n",
       "           -5.0098e-01, -1.7324e+00],\n",
       "          [ 9.3555e-01,  7.4512e-01,  6.5381e-01,  ...,  4.0356e-01,\n",
       "            3.7451e-01,  1.8281e+00],\n",
       "          [ 1.9189e+00, -1.1025e+00,  2.0449e+00,  ..., -2.6836e+00,\n",
       "           -2.5806e-01,  2.5547e+00],\n",
       "          ...,\n",
       "          [-6.0059e-01,  3.1299e-01, -1.0752e+00,  ..., -6.8604e-01,\n",
       "            1.3672e+00,  3.8301e+00],\n",
       "          [-1.0732e+00, -2.4219e-01, -1.1709e+00,  ..., -1.0039e+00,\n",
       "            1.1885e+00,  3.8691e+00],\n",
       "          [-5.7812e-01, -7.0215e-01, -7.7246e-01,  ..., -1.2754e+00,\n",
       "            1.4023e+00,  4.1641e+00]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 4.1771e-03, -1.7914e-02, -1.6403e-04,  ..., -3.8910e-04,\n",
       "            4.1656e-03,  9.3079e-03],\n",
       "          [-1.3574e-01,  2.9810e-01, -1.3428e-01,  ...,  7.1350e-02,\n",
       "           -5.0201e-03, -2.9709e-02],\n",
       "          [ 4.0924e-02,  1.3843e-01, -5.5756e-02,  ..., -5.2765e-02,\n",
       "           -4.6448e-02, -1.1377e-01],\n",
       "          ...,\n",
       "          [ 1.4722e-01, -1.0614e-01, -2.8976e-02,  ..., -3.6084e-01,\n",
       "            1.5393e-01, -1.1981e-01],\n",
       "          [ 4.1809e-02,  1.7053e-01, -4.6436e-01,  ...,  7.6561e-03,\n",
       "            7.2693e-02,  4.0942e-01],\n",
       "          [ 5.7324e-01,  3.2544e-01,  3.0884e-01,  ...,  2.6550e-02,\n",
       "           -9.4910e-02,  2.1631e-01]],\n",
       "\n",
       "         [[-9.3689e-03, -3.9215e-03, -1.1948e-02,  ...,  5.9547e-03,\n",
       "           -2.5421e-02,  3.5954e-03],\n",
       "          [ 1.4465e-02,  6.2012e-01,  6.6162e-02,  ..., -2.6758e-01,\n",
       "           -9.6802e-02,  1.9116e-01],\n",
       "          [-1.6833e-01, -1.9556e-01,  6.8848e-01,  ..., -4.3896e-01,\n",
       "            1.6858e-01, -3.1226e-01],\n",
       "          ...,\n",
       "          [-3.4424e-01, -1.2250e-01,  1.9092e-01,  ..., -5.0195e-01,\n",
       "            1.6431e-01, -4.2633e-02],\n",
       "          [-5.9814e-01,  3.0566e-01,  1.2152e-01,  ...,  4.5105e-02,\n",
       "            8.0200e-02,  2.4390e-01],\n",
       "          [-3.9673e-03,  1.6833e-01,  7.5317e-02,  ...,  3.6401e-01,\n",
       "            1.9897e-01,  1.1072e-01]],\n",
       "\n",
       "         [[-5.1804e-03,  1.7258e-02,  1.7471e-03,  ...,  7.1640e-03,\n",
       "           -8.0338e-03, -8.6823e-03],\n",
       "          [ 1.1426e-01, -7.3364e-02,  2.5439e-01,  ...,  2.6538e-01,\n",
       "            5.3680e-02,  3.0933e-01],\n",
       "          [-4.2969e-02, -2.0776e-01, -1.7358e-01,  ...,  5.0293e-01,\n",
       "            2.5171e-01,  1.6406e-01],\n",
       "          ...,\n",
       "          [-2.8882e-01, -2.7026e-01,  9.2407e-02,  ..., -6.3037e-01,\n",
       "           -3.8501e-01,  3.2593e-02],\n",
       "          [ 3.8037e-01,  8.2397e-02,  2.8247e-01,  ..., -1.5198e-01,\n",
       "           -1.4001e-01,  1.0455e-01],\n",
       "          [-3.0420e-01,  1.0437e-01, -4.8431e-02,  ..., -5.1074e-01,\n",
       "            1.4331e-01, -7.8674e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.9602e-03, -1.0162e-02, -5.6229e-03,  ..., -7.9269e-03,\n",
       "           -1.0004e-01, -6.0501e-03],\n",
       "          [ 2.1741e-01,  1.1963e-01,  2.3047e-01,  ..., -3.3521e-01,\n",
       "           -1.3525e+00, -5.5273e-01],\n",
       "          [-1.5857e-01, -3.3838e-01,  8.2947e-02,  ..., -2.5854e-01,\n",
       "           -6.1035e-01, -2.4390e-01],\n",
       "          ...,\n",
       "          [ 1.7639e-01, -6.2134e-02, -8.4381e-03,  ...,  2.0093e-01,\n",
       "            2.7368e-01, -1.5833e-01],\n",
       "          [-2.5586e-01, -7.6416e-02, -2.8662e-01,  ...,  1.4441e-01,\n",
       "            2.2119e-01,  2.3718e-01],\n",
       "          [-8.3862e-02, -3.7891e-01,  2.2876e-01,  ...,  3.1348e-01,\n",
       "           -1.2305e+00,  4.9463e-01]],\n",
       "\n",
       "         [[-3.3264e-03, -6.1417e-03,  2.4185e-03,  ..., -8.9188e-03,\n",
       "            1.2390e-02, -3.7422e-03],\n",
       "          [ 2.3376e-01, -1.1719e-01,  1.8713e-01,  ..., -2.3346e-02,\n",
       "            1.9189e-01, -2.1240e-01],\n",
       "          [ 2.1753e-01, -1.1841e-01, -6.0577e-03,  ..., -5.5969e-02,\n",
       "           -2.8735e-01,  2.0828e-02],\n",
       "          ...,\n",
       "          [-6.1584e-02, -2.9102e-01, -1.2207e-04,  ...,  9.0698e-02,\n",
       "           -2.9892e-02, -5.6061e-02],\n",
       "          [ 2.5903e-01,  1.4758e-01, -2.7515e-01,  ...,  1.4380e-01,\n",
       "           -7.2754e-02,  1.3879e-01],\n",
       "          [ 7.2571e-02, -2.3315e-01,  2.3914e-01,  ..., -3.5095e-02,\n",
       "           -7.3669e-02,  3.2043e-02]],\n",
       "\n",
       "         [[ 1.0468e-02, -1.2001e-02, -1.1688e-02,  ...,  8.4400e-04,\n",
       "            1.6144e-02, -2.0386e-02],\n",
       "          [ 8.3237e-03,  1.5419e-02,  1.3940e-01,  ...,  1.2317e-01,\n",
       "           -1.7432e-01, -3.3691e-02],\n",
       "          [ 1.6211e-01, -1.1365e-01, -1.6699e-01,  ...,  1.9196e-02,\n",
       "           -8.6182e-02,  2.8638e-01],\n",
       "          ...,\n",
       "          [ 1.5234e-01, -3.1396e-01,  5.7617e-02,  ...,  9.7290e-02,\n",
       "           -3.8745e-01, -3.2690e-01],\n",
       "          [ 3.2104e-01,  5.3802e-02, -7.0435e-02,  ...,  2.2485e-01,\n",
       "            9.5062e-03, -2.1118e-01],\n",
       "          [ 2.7069e-02, -1.2842e-01, -1.8689e-01,  ..., -3.9886e-02,\n",
       "            1.0754e-01, -5.0342e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.1771e-03, -1.7914e-02, -1.6403e-04,  ..., -3.8910e-04,\n",
       "            4.1656e-03,  9.3079e-03],\n",
       "          [-1.3574e-01,  2.9810e-01, -1.3428e-01,  ...,  7.1350e-02,\n",
       "           -5.0201e-03, -2.9709e-02],\n",
       "          [-3.3112e-02,  1.9775e-01, -1.0461e-01,  ...,  1.4038e-02,\n",
       "           -5.1514e-02, -7.4768e-02],\n",
       "          ...,\n",
       "          [ 3.2080e-01,  7.8857e-02, -2.1436e-01,  ..., -9.3384e-02,\n",
       "           -8.0322e-02,  2.3694e-01],\n",
       "          [ 3.2861e-01,  6.1890e-02, -1.9348e-01,  ..., -1.0571e-01,\n",
       "           -4.4159e-02,  2.2864e-01],\n",
       "          [ 3.1787e-01,  8.3130e-02, -1.6797e-01,  ..., -7.0862e-02,\n",
       "           -5.6458e-02,  2.4207e-01]],\n",
       "\n",
       "         [[-9.3689e-03, -3.9215e-03, -1.1948e-02,  ...,  5.9547e-03,\n",
       "           -2.5421e-02,  3.5954e-03],\n",
       "          [ 1.4465e-02,  6.2012e-01,  6.6162e-02,  ..., -2.6758e-01,\n",
       "           -9.6802e-02,  1.9116e-01],\n",
       "          [-3.4180e-03, -2.1191e-01,  2.5562e-01,  ..., -4.5044e-01,\n",
       "            4.1797e-01, -1.3940e-01],\n",
       "          ...,\n",
       "          [ 6.3770e-01,  1.4380e-01, -6.6260e-01,  ...,  2.9956e-01,\n",
       "            1.5393e-01,  7.5378e-02],\n",
       "          [ 6.0645e-01,  1.7554e-01, -5.6982e-01,  ...,  3.0933e-01,\n",
       "            9.1370e-02,  7.5012e-02],\n",
       "          [ 6.6846e-01,  1.4038e-01, -5.2588e-01,  ...,  3.1226e-01,\n",
       "            8.3435e-02,  3.5767e-02]],\n",
       "\n",
       "         [[-5.1804e-03,  1.7258e-02,  1.7471e-03,  ...,  7.1640e-03,\n",
       "           -8.0338e-03, -8.6823e-03],\n",
       "          [ 1.1426e-01, -7.3364e-02,  2.5439e-01,  ...,  2.6538e-01,\n",
       "            5.3680e-02,  3.0933e-01],\n",
       "          [-3.0884e-01, -5.6244e-02, -1.3635e-01,  ...,  3.9038e-01,\n",
       "            4.1260e-01,  1.1267e-01],\n",
       "          ...,\n",
       "          [-2.4512e-01,  2.1802e-01, -6.3330e-01,  ..., -1.6565e-01,\n",
       "           -9.1309e-02, -1.1438e-01],\n",
       "          [-2.6904e-01,  1.8884e-01, -6.1377e-01,  ..., -1.6333e-01,\n",
       "           -1.4783e-01, -7.2021e-02],\n",
       "          [-2.3560e-01,  2.4207e-01, -5.8496e-01,  ..., -8.4106e-02,\n",
       "           -1.4294e-01, -1.1230e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.9602e-03, -1.0162e-02, -5.6229e-03,  ..., -7.9269e-03,\n",
       "           -1.0004e-01, -6.0501e-03],\n",
       "          [ 2.1741e-01,  1.1963e-01,  2.3047e-01,  ..., -3.3521e-01,\n",
       "           -1.3525e+00, -5.5273e-01],\n",
       "          [ 3.5217e-02, -3.8403e-01,  5.8014e-02,  ..., -1.0254e-01,\n",
       "           -5.6152e-01, -2.8882e-01],\n",
       "          ...,\n",
       "          [-8.5632e-02,  1.3763e-02, -5.1605e-02,  ...,  1.0455e-01,\n",
       "           -1.0107e+00,  4.6814e-02],\n",
       "          [-5.3955e-02,  6.4941e-02, -8.5938e-02,  ...,  6.9763e-02,\n",
       "           -1.0088e+00,  8.9661e-02],\n",
       "          [-8.3801e-02, -5.0354e-03, -4.0710e-02,  ...,  5.6641e-02,\n",
       "           -1.0391e+00,  1.3623e-01]],\n",
       "\n",
       "         [[-3.3264e-03, -6.1417e-03,  2.4185e-03,  ..., -8.9188e-03,\n",
       "            1.2390e-02, -3.7422e-03],\n",
       "          [ 2.3376e-01, -1.1719e-01,  1.8713e-01,  ..., -2.3346e-02,\n",
       "            1.9189e-01, -2.1240e-01],\n",
       "          [ 5.1483e-02, -2.0300e-01,  9.0515e-02,  ..., -5.4596e-02,\n",
       "           -8.5571e-02,  3.4180e-02],\n",
       "          ...,\n",
       "          [ 8.7708e-02,  4.1565e-02, -2.7100e-01,  ...,  2.7808e-01,\n",
       "            1.5656e-02,  2.5818e-02],\n",
       "          [ 4.6631e-02,  2.3041e-03, -2.8101e-01,  ...,  2.6880e-01,\n",
       "           -4.0833e-02,  1.3382e-02],\n",
       "          [ 5.5664e-02,  1.2825e-02, -2.9224e-01,  ...,  2.1545e-01,\n",
       "           -4.8096e-02, -3.0899e-03]],\n",
       "\n",
       "         [[ 1.0468e-02, -1.2001e-02, -1.1688e-02,  ...,  8.4400e-04,\n",
       "            1.6144e-02, -2.0386e-02],\n",
       "          [ 8.3237e-03,  1.5419e-02,  1.3940e-01,  ...,  1.2317e-01,\n",
       "           -1.7432e-01, -3.3691e-02],\n",
       "          [ 6.6589e-02, -2.0776e-01, -3.0835e-01,  ...,  1.2314e-02,\n",
       "           -1.1438e-01,  1.4417e-01],\n",
       "          ...,\n",
       "          [ 2.1265e-01, -2.1716e-01,  4.4800e-02,  ...,  6.8970e-03,\n",
       "            2.4744e-01,  8.8959e-03],\n",
       "          [ 2.5732e-01, -2.6807e-01,  8.0994e-02,  ...,  4.9820e-03,\n",
       "            2.6733e-01, -1.2268e-02],\n",
       "          [ 2.5659e-01, -2.7759e-01,  5.2185e-02,  ..., -3.7903e-02,\n",
       "            2.2290e-01, -9.3994e-03]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 2.4902e-02,  1.7441e-02, -4.6326e-02,  ...,  4.7656e-01,\n",
       "           -4.8096e-02, -3.6377e-01],\n",
       "          [-1.3501e-01, -6.1621e-01,  1.1914e-01,  ...,  3.9893e-01,\n",
       "            5.1904e-01, -4.3774e-01],\n",
       "          [ 2.8906e-01, -1.7266e+00,  6.7749e-02,  ...,  1.3418e+00,\n",
       "           -7.3096e-01, -6.7969e-01],\n",
       "          ...,\n",
       "          [ 1.3125e+00, -1.0107e-01, -1.4043e+00,  ...,  1.5156e+00,\n",
       "            1.9473e+00, -3.0918e+00],\n",
       "          [ 2.0825e-01, -6.5088e-01, -7.9932e-01,  ..., -3.7964e-02,\n",
       "           -5.5078e-01, -1.8457e+00],\n",
       "          [-3.3960e-01, -9.2529e-02, -7.6367e-01,  ...,  1.0537e+00,\n",
       "           -2.5566e+00,  1.9824e+00]],\n",
       "\n",
       "         [[ 2.4246e-02,  1.8280e-02, -2.6108e-02,  ..., -4.9390e-01,\n",
       "            6.3232e-01,  4.7632e-01],\n",
       "          [-8.4277e-01, -1.3193e+00,  1.0840e+00,  ..., -5.1758e-01,\n",
       "           -8.9990e-01,  1.9470e-01],\n",
       "          [ 7.5146e-01, -1.6174e-01,  1.1221e+00,  ..., -7.8271e-01,\n",
       "           -1.9033e+00,  3.5571e-01],\n",
       "          ...,\n",
       "          [ 2.9785e-01, -2.0742e+00, -6.2354e-01,  ..., -1.1353e-01,\n",
       "           -6.9092e-01, -6.6699e-01],\n",
       "          [-2.7783e-01, -8.6133e-01, -1.1084e+00,  ..., -4.5581e-01,\n",
       "           -7.9541e-01,  1.5264e+00],\n",
       "          [-1.4238e+00, -2.7441e-01, -1.6436e+00,  ...,  2.5635e-01,\n",
       "           -5.2930e-01,  6.2402e-01]],\n",
       "\n",
       "         [[ 3.5767e-02,  1.3000e-02, -7.3853e-03,  ...,  2.5293e-01,\n",
       "           -1.4783e-01, -6.5430e-02],\n",
       "          [-2.6426e+00, -2.3594e+00,  1.7266e+00,  ..., -1.2128e-01,\n",
       "           -1.0020e+00,  3.3960e-01],\n",
       "          [ 5.7764e-01, -1.8574e+00,  2.1074e+00,  ...,  1.1963e+00,\n",
       "            6.5918e-01,  5.5371e-01],\n",
       "          ...,\n",
       "          [ 1.7314e+00, -1.1660e+00, -3.1094e+00,  ...,  5.5450e-02,\n",
       "           -9.8486e-01,  3.9490e-02],\n",
       "          [ 3.5742e-01, -2.2051e+00, -8.4863e-01,  ...,  2.3181e-01,\n",
       "            8.6731e-02, -7.1716e-02],\n",
       "          [-1.8301e+00, -1.0273e+00,  1.4478e-01,  ...,  6.3672e-01,\n",
       "            2.5586e-01, -1.2568e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 8.4381e-03,  1.2360e-02, -5.7297e-03,  ..., -3.8818e-01,\n",
       "            3.1211e+00, -1.7297e-01],\n",
       "          [ 4.3652e-01, -2.3291e-01, -2.7295e-01,  ...,  2.2480e+00,\n",
       "           -4.8242e+00,  1.6675e-01],\n",
       "          [ 6.8311e-01,  6.2549e-01, -6.6016e-01,  ..., -2.6367e-02,\n",
       "           -3.9531e+00, -1.5967e-01],\n",
       "          ...,\n",
       "          [-1.4990e+00,  3.9014e-01,  1.1709e+00,  ...,  1.9121e+00,\n",
       "           -5.4883e+00,  5.4297e-01],\n",
       "          [-1.8005e-01, -2.2314e-01,  3.1421e-01,  ...,  1.1045e+00,\n",
       "           -3.9043e+00, -4.8486e-01],\n",
       "          [ 9.1980e-02, -2.5635e-01,  3.6499e-01,  ..., -1.5894e-01,\n",
       "           -3.1016e+00, -5.0195e-01]],\n",
       "\n",
       "         [[-1.6968e-02,  5.6763e-03,  3.8635e-02,  ...,  1.3916e-01,\n",
       "           -5.4932e-03,  1.1292e-02],\n",
       "          [-1.0605e+00, -1.5156e+00, -1.4277e+00,  ..., -4.8486e-01,\n",
       "            1.3750e+00,  2.0801e-01],\n",
       "          [ 3.5767e-02, -1.1885e+00, -6.0352e-01,  ...,  3.0371e-01,\n",
       "            1.3824e-02,  4.7119e-02],\n",
       "          ...,\n",
       "          [-6.1719e-01, -6.0645e-01,  9.3994e-01,  ..., -6.3281e-01,\n",
       "           -1.9824e+00,  1.6289e+00],\n",
       "          [-6.2744e-01, -5.3613e-01, -1.0967e+00,  ...,  2.6025e-01,\n",
       "           -2.0781e+00, -1.6211e+00],\n",
       "          [-6.7383e-01,  3.2056e-01, -1.0410e+00,  ..., -1.5491e-01,\n",
       "           -8.3154e-01,  9.7559e-01]],\n",
       "\n",
       "         [[ 2.1637e-02, -8.7433e-03, -1.4801e-03,  ..., -3.9795e-01,\n",
       "            6.0156e-01,  2.9961e+00],\n",
       "          [-1.0205e+00,  9.4580e-01,  3.3057e-01,  ..., -8.5596e-01,\n",
       "            2.0527e+00, -3.7383e+00],\n",
       "          [-1.3047e+00,  1.8420e-01,  8.9551e-01,  ...,  6.3086e-01,\n",
       "            1.6875e+00, -4.6484e+00],\n",
       "          ...,\n",
       "          [ 5.9326e-01, -2.9639e-01, -3.8477e-01,  ...,  6.8848e-01,\n",
       "            1.3623e+00, -6.7969e+00],\n",
       "          [ 7.7820e-02, -4.1821e-01, -2.0096e-02,  ...,  1.0039e+00,\n",
       "            5.5615e-01, -5.5078e+00],\n",
       "          [ 5.7080e-01, -8.8965e-01, -2.5488e-01,  ...,  5.9277e-01,\n",
       "           -7.8027e-01, -4.2539e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 2.4902e-02,  1.7441e-02, -4.6326e-02,  ...,  4.7656e-01,\n",
       "           -4.8096e-02, -3.6377e-01],\n",
       "          [-1.3501e-01, -6.1621e-01,  1.1914e-01,  ...,  3.9893e-01,\n",
       "            5.1904e-01, -4.3774e-01],\n",
       "          [ 8.5632e-02, -1.8242e+00,  3.6850e-03,  ..., -2.6270e-01,\n",
       "           -6.7139e-01,  4.4580e-01],\n",
       "          ...,\n",
       "          [ 5.2637e-01, -5.4688e-01, -2.1277e-01,  ..., -1.7969e-01,\n",
       "           -8.0811e-02, -4.6704e-01],\n",
       "          [ 3.9575e-01, -8.2373e-01, -2.4304e-01,  ...,  1.0901e-01,\n",
       "            7.1655e-02, -5.0391e-01],\n",
       "          [-5.5298e-02, -6.4453e-01, -1.6455e-01,  ...,  8.7891e-02,\n",
       "           -2.0496e-01, -5.2344e-01]],\n",
       "\n",
       "         [[ 2.4246e-02,  1.8280e-02, -2.6108e-02,  ..., -4.9390e-01,\n",
       "            6.3232e-01,  4.7632e-01],\n",
       "          [-8.4277e-01, -1.3193e+00,  1.0840e+00,  ..., -5.1758e-01,\n",
       "           -8.9990e-01,  1.9470e-01],\n",
       "          [ 7.5684e-01, -2.6782e-01,  1.4023e+00,  ..., -1.2051e+00,\n",
       "           -1.6260e+00, -2.1167e-01],\n",
       "          ...,\n",
       "          [ 1.0479e+00, -9.5996e-01, -1.2627e+00,  ..., -4.0576e-01,\n",
       "            2.4854e-01,  5.2734e-01],\n",
       "          [-3.0273e-01, -1.1973e+00, -1.8320e+00,  ..., -4.5752e-01,\n",
       "            2.3486e-01,  5.5664e-01],\n",
       "          [-1.3770e+00, -6.4404e-01, -1.6172e+00,  ..., -4.3384e-01,\n",
       "            2.6978e-01,  5.2051e-01]],\n",
       "\n",
       "         [[ 3.5767e-02,  1.3000e-02, -7.3853e-03,  ...,  2.5293e-01,\n",
       "           -1.4783e-01, -6.5430e-02],\n",
       "          [-2.6426e+00, -2.3594e+00,  1.7266e+00,  ..., -1.2128e-01,\n",
       "           -1.0020e+00,  3.3960e-01],\n",
       "          [ 1.1934e+00, -1.7773e+00,  2.4551e+00,  ...,  5.5371e-01,\n",
       "            5.1709e-01,  9.5020e-01],\n",
       "          ...,\n",
       "          [ 2.3984e+00, -1.9004e+00, -1.7139e+00,  ...,  7.6221e-01,\n",
       "           -1.3416e-01, -2.5330e-02],\n",
       "          [ 6.8726e-02, -2.5117e+00, -1.4336e+00,  ...,  8.6084e-01,\n",
       "           -9.0637e-02, -1.4563e-01],\n",
       "          [-2.4102e+00, -1.5645e+00, -6.6357e-01,  ...,  8.7939e-01,\n",
       "           -8.1177e-02, -1.2671e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 8.4381e-03,  1.2360e-02, -5.7297e-03,  ..., -3.8818e-01,\n",
       "            3.1211e+00, -1.7297e-01],\n",
       "          [ 4.3652e-01, -2.3291e-01, -2.7295e-01,  ...,  2.2480e+00,\n",
       "           -4.8242e+00,  1.6675e-01],\n",
       "          [ 1.1943e+00,  9.2383e-01, -9.5557e-01,  ...,  6.1719e-01,\n",
       "           -3.9434e+00, -3.3008e-01],\n",
       "          ...,\n",
       "          [-4.9951e-01,  2.1484e-02,  6.5186e-01,  ...,  8.0127e-01,\n",
       "           -2.7773e+00, -2.1445e+00],\n",
       "          [-1.1621e+00,  6.2354e-01,  8.1299e-01,  ...,  7.3926e-01,\n",
       "           -2.7031e+00, -1.9219e+00],\n",
       "          [-8.2520e-01,  9.6191e-01,  6.2744e-01,  ...,  1.0264e+00,\n",
       "           -2.7422e+00, -1.9971e+00]],\n",
       "\n",
       "         [[-1.6968e-02,  5.6763e-03,  3.8635e-02,  ...,  1.3916e-01,\n",
       "           -5.4932e-03,  1.1292e-02],\n",
       "          [-1.0605e+00, -1.5156e+00, -1.4277e+00,  ..., -4.8486e-01,\n",
       "            1.3750e+00,  2.0801e-01],\n",
       "          [ 2.2205e-01, -1.4033e+00, -1.0254e+00,  ...,  4.7388e-01,\n",
       "           -5.5420e-01,  1.9043e-02],\n",
       "          ...,\n",
       "          [ 1.0742e-02, -1.0830e+00,  8.4375e-01,  ...,  6.2012e-01,\n",
       "           -1.0781e+00,  5.2832e-01],\n",
       "          [-1.5195e+00, -1.2285e+00, -4.8291e-01,  ...,  6.6650e-01,\n",
       "           -1.1133e+00,  6.4404e-01],\n",
       "          [-1.6504e+00, -5.8496e-01, -1.5039e+00,  ...,  7.1631e-01,\n",
       "           -1.1875e+00,  6.6895e-01]],\n",
       "\n",
       "         [[ 2.1637e-02, -8.7433e-03, -1.4801e-03,  ..., -3.9795e-01,\n",
       "            6.0156e-01,  2.9961e+00],\n",
       "          [-1.0205e+00,  9.4580e-01,  3.3057e-01,  ..., -8.5596e-01,\n",
       "            2.0527e+00, -3.7383e+00],\n",
       "          [-1.6562e+00,  6.8555e-01,  9.3799e-01,  ...,  7.3730e-01,\n",
       "            2.2793e+00, -4.2031e+00],\n",
       "          ...,\n",
       "          [ 8.2715e-01,  3.3984e-01,  6.2866e-02,  ..., -1.1592e+00,\n",
       "            1.6338e+00, -3.4902e+00],\n",
       "          [ 1.0830e+00, -4.6069e-01, -1.1127e-01,  ..., -1.3477e+00,\n",
       "            1.5176e+00, -3.4766e+00],\n",
       "          [ 3.2422e-01, -8.0029e-01, -2.9077e-01,  ..., -1.4453e+00,\n",
       "            1.4766e+00, -3.4609e+00]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 4.7455e-03,  5.0888e-03, -7.0679e-02,  ...,  2.0981e-03,\n",
       "           -1.1658e-02,  4.8828e-03],\n",
       "          [-1.1475e-01,  2.8442e-02,  2.2632e-01,  ...,  1.1467e-02,\n",
       "            4.7974e-02, -3.2031e-01],\n",
       "          [ 4.2236e-02, -4.2511e-02, -6.0242e-02,  ..., -7.0190e-02,\n",
       "           -1.1731e-01, -2.8247e-01],\n",
       "          ...,\n",
       "          [-4.3091e-02,  1.4734e-01,  1.6663e-01,  ..., -7.1106e-02,\n",
       "            2.7075e-01,  1.8066e-02],\n",
       "          [-2.7148e-01, -1.5051e-01, -8.0811e-02,  ...,  7.9956e-03,\n",
       "            1.9226e-01,  2.5574e-02],\n",
       "          [ 1.2561e-01, -2.3621e-01, -3.8745e-01,  ..., -2.9282e-02,\n",
       "            3.5400e-01, -1.8579e-01]],\n",
       "\n",
       "         [[ 5.1003e-03,  1.9760e-03, -1.0078e-02,  ..., -7.0267e-03,\n",
       "           -1.0994e-02, -2.2308e-02],\n",
       "          [ 6.8701e-01, -2.0447e-01,  3.9600e-01,  ...,  7.5293e-01,\n",
       "           -3.7158e-01, -2.2803e-01],\n",
       "          [-5.5908e-01, -6.9873e-01,  6.0498e-01,  ...,  1.0801e+00,\n",
       "           -4.0259e-01, -3.8135e-01],\n",
       "          ...,\n",
       "          [ 9.8999e-02,  2.6001e-01,  1.8787e-01,  ...,  9.9365e-02,\n",
       "            1.6650e-01, -2.5146e-01],\n",
       "          [ 3.9551e-01,  1.7944e-01,  2.9297e-03,  ..., -4.2627e-01,\n",
       "           -3.3813e-01, -1.1816e+00],\n",
       "          [ 4.1772e-01, -1.5747e-01, -7.6611e-01,  ...,  2.6099e-01,\n",
       "           -9.6338e-01, -2.7417e-01]],\n",
       "\n",
       "         [[ 2.6337e-02,  4.1901e-02,  2.8763e-03,  ..., -1.3680e-02,\n",
       "           -1.4015e-02, -1.6006e-02],\n",
       "          [-2.3291e-01,  3.8818e-01,  3.6890e-01,  ...,  8.4381e-03,\n",
       "            1.5649e-01, -3.8257e-01],\n",
       "          [-6.8359e-03, -1.3281e-01,  2.2675e-02,  ..., -2.2461e-01,\n",
       "            2.0889e-02, -1.2695e-01],\n",
       "          ...,\n",
       "          [ 4.8608e-01,  3.5706e-02,  1.6589e-01,  ..., -1.7615e-01,\n",
       "            1.8274e-01, -5.0720e-02],\n",
       "          [ 1.7017e-01, -5.3711e-01, -3.6316e-02,  ..., -1.0480e-01,\n",
       "           -6.0638e-02, -2.9224e-01],\n",
       "          [-1.0815e-01, -9.5068e-01, -3.0176e-01,  ..., -3.5889e-02,\n",
       "            1.6528e-01, -7.7362e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.1962e-04,  6.4468e-03,  7.0076e-03,  ...,  1.9684e-02,\n",
       "           -6.3324e-04,  9.6588e-03],\n",
       "          [ 2.0538e-02, -7.1045e-01,  4.8901e-01,  ..., -4.7089e-02,\n",
       "           -1.7432e-01,  1.5656e-02],\n",
       "          [-4.2664e-02,  3.1250e-01,  1.9312e-01,  ..., -3.0298e-01,\n",
       "           -1.3660e-01,  5.7861e-02],\n",
       "          ...,\n",
       "          [-1.4496e-02, -5.9021e-02, -2.6514e-01,  ...,  3.3667e-01,\n",
       "            1.0681e-01, -5.7471e-01],\n",
       "          [ 1.6931e-01, -5.5029e-01,  3.2471e-01,  ...,  4.4409e-01,\n",
       "           -9.3994e-02,  1.2366e-01],\n",
       "          [ 8.1738e-01,  4.9957e-02,  2.3926e-01,  ...,  1.4697e-01,\n",
       "           -1.6980e-01,  6.6504e-01]],\n",
       "\n",
       "         [[-5.2643e-03,  9.8343e-03, -6.0730e-03,  ...,  4.8752e-03,\n",
       "            1.2543e-02,  2.3880e-03],\n",
       "          [-1.5027e-01, -1.1633e-01, -5.2490e-03,  ...,  4.8877e-01,\n",
       "            3.5156e-01,  1.5820e-01],\n",
       "          [-2.0605e-01, -5.4883e-01,  2.4268e-01,  ..., -1.4783e-01,\n",
       "            4.5776e-01, -3.6157e-01],\n",
       "          ...,\n",
       "          [ 2.3926e-01,  2.2119e-01, -4.1333e-01,  ..., -2.9126e-01,\n",
       "            3.8757e-02,  5.6885e-02],\n",
       "          [ 1.2433e-01, -3.0493e-01, -1.0620e-01,  ...,  3.3350e-01,\n",
       "            4.4873e-01, -4.5264e-01],\n",
       "          [ 2.1423e-01, -5.6854e-02,  3.4149e-02,  ...,  1.2360e-02,\n",
       "           -4.8999e-01, -1.7468e-01]],\n",
       "\n",
       "         [[ 1.8829e-02,  8.9722e-03, -5.6000e-03,  ..., -1.0559e-02,\n",
       "            7.0251e-02,  1.4511e-02],\n",
       "          [ 4.0234e-01, -2.5269e-01,  4.4849e-01,  ...,  4.7949e-01,\n",
       "            2.0093e-01,  1.0431e-01],\n",
       "          [-2.7374e-02,  5.9906e-02,  2.7808e-01,  ...,  1.7664e-01,\n",
       "            1.0760e-01, -4.9072e-02],\n",
       "          ...,\n",
       "          [ 3.3813e-01, -2.5781e-01, -8.6865e-01,  ...,  1.9073e-02,\n",
       "           -1.0352e+00,  1.2524e-01],\n",
       "          [ 4.1577e-01, -4.1602e-01, -6.8115e-01,  ...,  1.7920e-01,\n",
       "           -7.1094e-01,  3.9966e-01],\n",
       "          [ 1.8359e-01, -7.7454e-02, -1.1553e+00,  ...,  2.0703e-01,\n",
       "           -1.9189e-01,  1.8945e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.7455e-03,  5.0888e-03, -7.0679e-02,  ...,  2.0981e-03,\n",
       "           -1.1658e-02,  4.8828e-03],\n",
       "          [-1.1475e-01,  2.8442e-02,  2.2632e-01,  ...,  1.1467e-02,\n",
       "            4.7974e-02, -3.2031e-01],\n",
       "          [-4.2877e-02,  2.2998e-01,  9.6252e-02,  ..., -6.7200e-02,\n",
       "            2.5681e-02, -1.8848e-01],\n",
       "          ...,\n",
       "          [-8.7097e-02,  3.1891e-02,  2.3706e-01,  ..., -1.3342e-01,\n",
       "            1.9150e-03, -3.7378e-01],\n",
       "          [-6.3721e-02,  4.8401e-02,  1.5344e-01,  ..., -1.2115e-01,\n",
       "            2.7802e-02, -3.7866e-01],\n",
       "          [-2.4155e-02,  4.1412e-02,  1.7175e-01,  ..., -1.7725e-01,\n",
       "            5.2673e-02, -4.1992e-01]],\n",
       "\n",
       "         [[ 5.1003e-03,  1.9760e-03, -1.0078e-02,  ..., -7.0267e-03,\n",
       "           -1.0994e-02, -2.2308e-02],\n",
       "          [ 6.8701e-01, -2.0447e-01,  3.9600e-01,  ...,  7.5293e-01,\n",
       "           -3.7158e-01, -2.2803e-01],\n",
       "          [-1.5662e-01, -1.3672e-01,  2.4927e-01,  ...,  9.8340e-01,\n",
       "           -4.9805e-01, -5.3516e-01],\n",
       "          ...,\n",
       "          [ 4.7974e-02,  1.7542e-01, -3.4277e-01,  ...,  3.2690e-01,\n",
       "            3.4863e-01, -1.3843e-01],\n",
       "          [ 9.9670e-02,  2.2302e-01, -3.4521e-01,  ...,  3.0078e-01,\n",
       "            4.0942e-01, -2.2449e-01],\n",
       "          [ 1.5979e-01,  1.9373e-01, -3.0029e-01,  ...,  2.6270e-01,\n",
       "            2.3230e-01, -2.3376e-02]],\n",
       "\n",
       "         [[ 2.6337e-02,  4.1901e-02,  2.8763e-03,  ..., -1.3680e-02,\n",
       "           -1.4015e-02, -1.6006e-02],\n",
       "          [-2.3291e-01,  3.8818e-01,  3.6890e-01,  ...,  8.4381e-03,\n",
       "            1.5649e-01, -3.8257e-01],\n",
       "          [ 8.5510e-02, -3.4351e-01,  2.7148e-01,  ..., -2.7295e-01,\n",
       "           -3.4607e-02,  1.0529e-02],\n",
       "          ...,\n",
       "          [-2.0374e-01, -3.5278e-02,  3.9478e-01,  ..., -8.6572e-01,\n",
       "            4.7241e-02, -2.5085e-02],\n",
       "          [-2.0386e-01, -6.0913e-02,  3.3569e-01,  ..., -8.4473e-01,\n",
       "            1.4046e-02, -8.0795e-03],\n",
       "          [-2.1716e-01, -2.8534e-02,  3.5303e-01,  ..., -8.8232e-01,\n",
       "            2.9205e-02,  4.1351e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.1962e-04,  6.4468e-03,  7.0076e-03,  ...,  1.9684e-02,\n",
       "           -6.3324e-04,  9.6588e-03],\n",
       "          [ 2.0538e-02, -7.1045e-01,  4.8901e-01,  ..., -4.7089e-02,\n",
       "           -1.7432e-01,  1.5656e-02],\n",
       "          [-2.5464e-01,  2.5439e-01, -9.2407e-02,  ..., -5.8350e-01,\n",
       "           -2.2534e-01, -1.9470e-02],\n",
       "          ...,\n",
       "          [ 2.0203e-01, -9.5886e-02, -3.1013e-03,  ..., -2.1606e-01,\n",
       "           -1.3135e-01,  3.0957e-01],\n",
       "          [ 2.3523e-01, -1.9507e-01, -2.1973e-02,  ..., -1.5723e-01,\n",
       "           -1.5759e-01,  3.0200e-01],\n",
       "          [ 2.6782e-01, -1.6833e-01,  3.4241e-02,  ..., -2.2339e-01,\n",
       "           -1.9324e-01,  3.0322e-01]],\n",
       "\n",
       "         [[-5.2643e-03,  9.8343e-03, -6.0730e-03,  ...,  4.8752e-03,\n",
       "            1.2543e-02,  2.3880e-03],\n",
       "          [-1.5027e-01, -1.1633e-01, -5.2490e-03,  ...,  4.8877e-01,\n",
       "            3.5156e-01,  1.5820e-01],\n",
       "          [-1.4343e-01, -4.2456e-01,  1.4343e-03,  ..., -3.7378e-01,\n",
       "            3.6572e-01, -3.2056e-01],\n",
       "          ...,\n",
       "          [ 1.1053e-01,  1.9531e-01,  2.3254e-02,  ..., -4.2993e-01,\n",
       "            8.3862e-02,  2.1985e-01],\n",
       "          [ 1.4502e-01,  1.6687e-01,  4.0375e-02,  ..., -3.9648e-01,\n",
       "            5.8105e-02,  1.6650e-01],\n",
       "          [ 8.4045e-02,  1.5735e-01,  8.5022e-02,  ..., -4.1602e-01,\n",
       "            5.8777e-02,  1.7322e-01]],\n",
       "\n",
       "         [[ 1.8829e-02,  8.9722e-03, -5.6000e-03,  ..., -1.0559e-02,\n",
       "            7.0251e-02,  1.4511e-02],\n",
       "          [ 4.0234e-01, -2.5269e-01,  4.4849e-01,  ...,  4.7949e-01,\n",
       "            2.0093e-01,  1.0431e-01],\n",
       "          [ 2.6978e-01,  8.5815e-02,  3.1250e-01,  ...,  3.3984e-01,\n",
       "           -1.5283e-01, -3.7079e-02],\n",
       "          ...,\n",
       "          [ 2.5732e-01,  4.0918e-01,  9.6191e-02,  ...,  1.1169e-01,\n",
       "           -1.3733e-03,  2.2186e-02],\n",
       "          [ 2.8833e-01,  3.8989e-01,  9.5886e-02,  ...,  9.7351e-02,\n",
       "           -1.8921e-02,  8.8867e-02],\n",
       "          [ 2.9858e-01,  3.5474e-01,  1.4087e-01,  ...,  1.0779e-01,\n",
       "            1.4984e-02,  1.2634e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-2.9114e-02,  4.6692e-03,  1.6144e-02,  ..., -4.2261e-01,\n",
       "           -8.0566e-02,  3.7018e-02],\n",
       "          [ 1.8994e+00,  7.3193e-01, -1.3320e+00,  ...,  2.7563e-01,\n",
       "           -4.0942e-01,  9.0479e-01],\n",
       "          [ 2.7393e-01,  1.2100e+00, -9.1699e-01,  ...,  4.5117e-01,\n",
       "           -9.6387e-01,  1.2073e-01],\n",
       "          ...,\n",
       "          [-5.9033e-01,  1.8164e+00,  5.4980e-01,  ...,  1.4709e-01,\n",
       "           -2.2383e+00, -3.3740e-01],\n",
       "          [-5.5420e-01,  1.2197e+00,  3.5571e-01,  ..., -1.9189e+00,\n",
       "           -1.7559e+00, -1.0977e+00],\n",
       "          [ 7.6709e-01,  5.3955e-02, -5.4102e-01,  ..., -2.5332e+00,\n",
       "           -4.1699e-01, -1.2070e+00]],\n",
       "\n",
       "         [[-4.2725e-04,  2.4902e-02, -1.7059e-02,  ..., -4.4092e-01,\n",
       "           -1.2207e+00, -1.5894e-01],\n",
       "          [-6.7285e-01, -6.9336e-01,  4.1138e-01,  ...,  2.0742e+00,\n",
       "            3.0410e+00,  2.5039e+00],\n",
       "          [-4.7729e-01, -7.3340e-01,  1.4336e+00,  ...,  3.7744e-01,\n",
       "            5.5820e+00,  9.7852e-01],\n",
       "          ...,\n",
       "          [ 1.1191e+00, -8.3350e-01, -4.9072e-01,  ...,  1.1592e+00,\n",
       "           -4.4219e+00,  1.8418e+00],\n",
       "          [ 2.4927e-01, -1.1768e-01,  4.3488e-02,  ...,  2.8418e+00,\n",
       "            2.1777e+00, -9.2383e-01],\n",
       "          [ 3.5449e-01,  1.2988e-01,  1.9653e-01,  ...,  1.0977e+00,\n",
       "            5.9883e+00,  9.3079e-03]],\n",
       "\n",
       "         [[-2.0584e-02, -2.6764e-02, -2.9984e-02,  ...,  1.8372e-02,\n",
       "           -2.6596e-02, -7.6843e-02],\n",
       "          [ 1.5889e+00,  8.0615e-01,  1.4297e+00,  ...,  7.7148e-01,\n",
       "           -7.3242e-01, -4.2212e-01],\n",
       "          [ 9.3701e-01, -9.0283e-01,  8.4668e-01,  ...,  1.3604e+00,\n",
       "            1.0742e-01,  2.2119e-01],\n",
       "          ...,\n",
       "          [-2.1719e+00,  5.0781e-01, -2.0349e-01,  ..., -3.2324e+00,\n",
       "           -3.7256e-01,  3.0713e-01],\n",
       "          [-3.8770e-01, -6.9727e-01, -4.7217e-01,  ..., -1.1562e+00,\n",
       "           -7.1387e-01,  5.6250e-01],\n",
       "          [ 2.5146e-02, -1.1035e+00, -6.8359e-03,  ...,  1.5508e+00,\n",
       "           -1.4102e+00,  4.7168e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.5400e-03, -2.6825e-02, -2.3804e-02,  ...,  5.5481e-02,\n",
       "            3.3711e+00,  2.7246e-01],\n",
       "          [-7.3828e-01,  5.5078e-01,  3.3521e-01,  ..., -5.9766e-01,\n",
       "           -4.6328e+00, -1.1533e+00],\n",
       "          [ 3.4473e-01,  4.8340e-01,  5.2539e-01,  ...,  1.0029e+00,\n",
       "           -4.5156e+00, -6.0645e-01],\n",
       "          ...,\n",
       "          [-1.2891e+00,  1.8213e+00,  1.0254e+00,  ...,  1.5713e+00,\n",
       "           -8.2656e+00, -3.3838e-01],\n",
       "          [-3.8965e-01,  9.5361e-01,  4.0253e-02,  ...,  3.4688e+00,\n",
       "           -5.4414e+00, -8.8770e-01],\n",
       "          [-7.5928e-01,  5.3271e-01, -1.4490e-01,  ...,  3.7207e+00,\n",
       "           -5.1250e+00, -3.4399e-01]],\n",
       "\n",
       "         [[ 2.6031e-02, -4.2419e-03, -2.7161e-02,  ..., -1.4514e-01,\n",
       "           -2.0776e-01, -2.0449e+00],\n",
       "          [-1.5166e+00, -3.2227e-01,  1.1250e+00,  ...,  7.9980e-01,\n",
       "            5.4736e-01, -9.3066e-01],\n",
       "          [-5.2686e-01, -6.5820e-01, -7.4463e-03,  ...,  1.5771e-01,\n",
       "            3.9868e-01,  1.2705e+00],\n",
       "          ...,\n",
       "          [ 6.6211e-01, -1.8535e+00,  1.4648e-01,  ..., -4.9951e-01,\n",
       "           -1.6394e-01,  5.1250e+00],\n",
       "          [-1.6138e-01, -9.7046e-02, -2.6709e-01,  ...,  4.7949e-01,\n",
       "           -4.9951e-01,  4.1289e+00],\n",
       "          [-4.5239e-01, -5.5615e-01,  2.2827e-01,  ...,  1.6211e-01,\n",
       "           -1.4941e+00,  2.9609e+00]],\n",
       "\n",
       "         [[ 4.4250e-02, -1.6724e-02, -1.7090e-02,  ...,  2.6489e-01,\n",
       "            4.2456e-01,  2.5830e-01],\n",
       "          [-9.1846e-01,  3.7891e-01,  8.0322e-01,  ..., -4.9097e-01,\n",
       "            1.3623e+00,  1.1826e+00],\n",
       "          [-1.0413e-01, -3.8135e-01, -3.6401e-01,  ..., -5.1172e-01,\n",
       "            5.3613e-01,  1.9600e+00],\n",
       "          ...,\n",
       "          [-3.9111e-01, -5.7178e-01,  8.7402e-02,  ...,  1.1826e+00,\n",
       "           -2.3730e+00, -2.8008e+00],\n",
       "          [ 5.0507e-02, -1.7041e-01,  2.9297e-01,  ...,  7.0703e-01,\n",
       "            2.8320e-01, -4.7046e-01],\n",
       "          [-1.7432e-01,  1.7944e-01,  8.8867e-02,  ...,  1.0664e+00,\n",
       "            1.0293e+00, -3.2764e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.9114e-02,  4.6692e-03,  1.6144e-02,  ..., -4.2261e-01,\n",
       "           -8.0566e-02,  3.7018e-02],\n",
       "          [ 1.8994e+00,  7.3193e-01, -1.3320e+00,  ...,  2.7563e-01,\n",
       "           -4.0942e-01,  9.0479e-01],\n",
       "          [ 1.9238e-01,  1.2627e+00, -1.0117e+00,  ...,  8.3398e-01,\n",
       "           -2.2290e-01,  5.9521e-01],\n",
       "          ...,\n",
       "          [-1.1660e+00,  5.5029e-01,  4.8022e-01,  ..., -7.5732e-01,\n",
       "            8.4863e-01,  3.1787e-01],\n",
       "          [-8.2080e-01,  6.4502e-01,  3.7231e-01,  ..., -8.2568e-01,\n",
       "            6.8604e-01, -1.1847e-01],\n",
       "          [ 1.7334e-01,  3.3813e-01,  1.0608e-01,  ..., -6.8555e-01,\n",
       "            6.0742e-01, -1.0841e-02]],\n",
       "\n",
       "         [[-4.2725e-04,  2.4902e-02, -1.7059e-02,  ..., -4.4092e-01,\n",
       "           -1.2207e+00, -1.5894e-01],\n",
       "          [-6.7285e-01, -6.9336e-01,  4.1138e-01,  ...,  2.0742e+00,\n",
       "            3.0410e+00,  2.5039e+00],\n",
       "          [-6.5869e-01, -1.0186e+00,  1.5127e+00,  ...,  1.5273e+00,\n",
       "            4.8047e+00,  9.0869e-01],\n",
       "          ...,\n",
       "          [ 1.0303e-01, -4.3799e-01, -1.0703e+00,  ...,  7.0361e-01,\n",
       "            4.7188e+00,  7.1533e-01],\n",
       "          [ 6.8408e-01, -6.2988e-01, -1.2051e+00,  ...,  7.2949e-01,\n",
       "            4.5469e+00,  8.0469e-01],\n",
       "          [ 6.5234e-01, -3.9648e-01, -8.7500e-01,  ...,  7.0410e-01,\n",
       "            4.5508e+00,  9.5752e-01]],\n",
       "\n",
       "         [[-2.0584e-02, -2.6764e-02, -2.9984e-02,  ...,  1.8372e-02,\n",
       "           -2.6596e-02, -7.6843e-02],\n",
       "          [ 1.5889e+00,  8.0615e-01,  1.4297e+00,  ...,  7.7148e-01,\n",
       "           -7.3242e-01, -4.2212e-01],\n",
       "          [ 1.1074e+00, -1.0889e+00,  1.1328e+00,  ...,  4.9878e-01,\n",
       "            4.5874e-01, -2.4646e-01],\n",
       "          ...,\n",
       "          [-1.4004e+00,  5.0244e-01, -1.0205e+00,  ...,  2.2695e+00,\n",
       "           -1.4668e+00,  7.0496e-02],\n",
       "          [-1.2070e+00, -6.9824e-01, -6.7480e-01,  ...,  2.2559e+00,\n",
       "           -1.7031e+00,  2.6465e-01],\n",
       "          [-5.6152e-02, -1.4121e+00, -1.7993e-01,  ...,  2.2754e+00,\n",
       "           -1.9541e+00,  5.7739e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.5400e-03, -2.6825e-02, -2.3804e-02,  ...,  5.5481e-02,\n",
       "            3.3711e+00,  2.7246e-01],\n",
       "          [-7.3828e-01,  5.5078e-01,  3.3521e-01,  ..., -5.9766e-01,\n",
       "           -4.6328e+00, -1.1533e+00],\n",
       "          [ 6.2598e-01,  7.4316e-01,  5.5518e-01,  ...,  3.9062e-03,\n",
       "           -4.2188e+00, -8.2129e-01],\n",
       "          ...,\n",
       "          [ 1.1572e+00,  8.2666e-01, -1.2201e-01,  ...,  1.2793e-01,\n",
       "           -4.2578e+00, -1.2178e+00],\n",
       "          [-1.5515e-01,  1.0244e+00, -1.8713e-01,  ...,  4.8071e-01,\n",
       "           -4.1758e+00, -1.1855e+00],\n",
       "          [-1.3906e+00,  5.8936e-01, -9.3262e-02,  ...,  4.6216e-01,\n",
       "           -4.2109e+00, -1.4072e+00]],\n",
       "\n",
       "         [[ 2.6031e-02, -4.2419e-03, -2.7161e-02,  ..., -1.4514e-01,\n",
       "           -2.0776e-01, -2.0449e+00],\n",
       "          [-1.5166e+00, -3.2227e-01,  1.1250e+00,  ...,  7.9980e-01,\n",
       "            5.4736e-01, -9.3066e-01],\n",
       "          [-6.8164e-01, -7.3828e-01, -2.0947e-01,  ...,  3.0396e-01,\n",
       "            1.5698e-01,  1.8750e+00],\n",
       "          ...,\n",
       "          [ 7.5000e-01, -5.6836e-01,  9.8877e-02,  ...,  7.1338e-01,\n",
       "           -1.0869e+00,  2.9180e+00],\n",
       "          [ 3.2983e-01, -8.1836e-01,  4.7021e-01,  ...,  8.9600e-01,\n",
       "           -1.1494e+00,  2.8574e+00],\n",
       "          [-3.6133e-01, -5.6836e-01,  7.7197e-01,  ...,  6.9873e-01,\n",
       "           -1.5225e+00,  3.0039e+00]],\n",
       "\n",
       "         [[ 4.4250e-02, -1.6724e-02, -1.7090e-02,  ...,  2.6489e-01,\n",
       "            4.2456e-01,  2.5830e-01],\n",
       "          [-9.1846e-01,  3.7891e-01,  8.0322e-01,  ..., -4.9097e-01,\n",
       "            1.3623e+00,  1.1826e+00],\n",
       "          [-2.4194e-01, -3.8745e-01, -6.9678e-01,  ..., -5.3564e-01,\n",
       "            6.8359e-01,  1.8145e+00],\n",
       "          ...,\n",
       "          [ 9.2621e-03, -4.0649e-02,  2.4194e-01,  ...,  1.7510e+00,\n",
       "            8.3801e-02,  2.5293e+00],\n",
       "          [-8.5449e-02, -2.3438e-01,  3.2031e-01,  ...,  1.6729e+00,\n",
       "           -3.2379e-02,  2.7012e+00],\n",
       "          [-7.2205e-02, -2.9199e-01,  3.1396e-01,  ...,  1.8477e+00,\n",
       "           -1.3501e-01,  2.6328e+00]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 3.1372e-02, -3.4790e-02,  3.5797e-02,  ...,  3.1891e-02,\n",
       "            8.6212e-04, -1.8997e-03],\n",
       "          [-9.3140e-02,  9.5886e-02,  4.3213e-01,  ...,  4.8828e-02,\n",
       "           -4.1064e-01, -8.5205e-02],\n",
       "          [-1.3818e-01, -2.5330e-03,  2.1606e-01,  ...,  7.1594e-02,\n",
       "           -1.3489e-01, -3.8361e-02],\n",
       "          ...,\n",
       "          [-1.0229e-01, -1.7761e-01, -8.4473e-02,  ..., -2.2913e-01,\n",
       "            4.0619e-02, -1.1414e-01],\n",
       "          [-4.3182e-03, -1.0229e-01, -5.2490e-02,  ..., -4.9634e-01,\n",
       "           -4.1040e-01, -2.2339e-01],\n",
       "          [-4.2725e-01, -1.3599e-01,  1.1365e-01,  ..., -1.2329e-01,\n",
       "           -3.5693e-01, -3.5278e-01]],\n",
       "\n",
       "         [[ 1.2085e-02,  2.3331e-02, -4.7302e-04,  ..., -1.0452e-02,\n",
       "            2.6672e-02,  6.8283e-03],\n",
       "          [-2.3950e-01,  5.3125e-01, -3.4729e-02,  ...,  4.3091e-02,\n",
       "           -1.4587e-01, -8.0948e-03],\n",
       "          [ 9.4360e-02,  1.6101e-01, -1.4075e-01,  ..., -1.3138e-02,\n",
       "            5.0232e-02, -5.1416e-01],\n",
       "          ...,\n",
       "          [ 2.5146e-01, -1.1182e-01,  3.9087e-01,  ..., -1.2256e-01,\n",
       "           -4.5166e-02, -3.6499e-01],\n",
       "          [-8.6609e-02, -6.8604e-02,  1.7297e-01,  ...,  7.9651e-02,\n",
       "            5.9631e-02,  2.8198e-01],\n",
       "          [ 3.4851e-02, -1.9556e-01,  6.4697e-01,  ..., -1.4917e-01,\n",
       "           -2.7979e-01,  2.6929e-01]],\n",
       "\n",
       "         [[ 1.5915e-02, -3.4771e-03,  1.2039e-02,  ..., -9.3231e-03,\n",
       "           -7.3242e-03,  1.2375e-02],\n",
       "          [ 8.7280e-02, -2.2437e-01,  1.8701e-01,  ..., -2.4524e-01,\n",
       "           -1.0908e+00, -6.0107e-01],\n",
       "          [-1.6699e-01,  1.9958e-01, -3.9453e-01,  ..., -1.4062e-01,\n",
       "           -6.7188e-01, -4.2725e-01],\n",
       "          ...,\n",
       "          [ 9.8022e-02,  3.6987e-01, -1.8616e-01,  ...,  2.8259e-02,\n",
       "            4.5532e-01, -5.9619e-01],\n",
       "          [ 3.2593e-02,  1.0956e-01,  7.3608e-02,  ..., -2.6465e-01,\n",
       "            2.0996e-01, -3.1348e-01],\n",
       "          [ 2.2583e-01, -1.4990e-01,  8.7402e-02,  ..., -1.6882e-01,\n",
       "            2.0239e-01, -1.8823e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.7863e-02,  3.1738e-02,  4.6616e-03,  ..., -1.5984e-03,\n",
       "           -2.4357e-03, -1.3382e-02],\n",
       "          [ 1.0480e-01, -4.8804e-01,  4.2358e-02,  ..., -1.4233e-01,\n",
       "            1.7981e-01, -4.3396e-02],\n",
       "          [-4.2633e-02, -6.4990e-01,  9.1736e-02,  ..., -4.7302e-04,\n",
       "           -2.1118e-01, -1.1365e-01],\n",
       "          ...,\n",
       "          [-5.4321e-02,  3.2715e-01,  2.7563e-01,  ..., -1.0962e-01,\n",
       "           -7.0862e-02, -2.2949e-01],\n",
       "          [ 7.7588e-01,  4.3848e-01,  1.4453e-01,  ..., -4.4189e-01,\n",
       "           -2.2412e-01,  3.8110e-01],\n",
       "          [-4.7485e-02,  5.3418e-01, -1.1163e-01,  ...,  3.4790e-01,\n",
       "            3.8672e-01,  2.8223e-01]],\n",
       "\n",
       "         [[-8.5373e-03, -2.4719e-02,  2.0187e-02,  ..., -3.2654e-03,\n",
       "           -1.6174e-02,  1.1330e-02],\n",
       "          [-5.8643e-01, -1.5686e-01,  1.1194e-01,  ..., -4.3628e-01,\n",
       "           -6.7688e-02,  1.2848e-02],\n",
       "          [ 1.6711e-01, -7.8857e-02,  1.3672e-01,  ..., -1.9678e-01,\n",
       "           -2.0630e-01, -7.2632e-02],\n",
       "          ...,\n",
       "          [ 1.0413e-01, -2.8174e-01, -4.0796e-01,  ...,  6.7627e-02,\n",
       "            4.8242e-01, -4.9023e-01],\n",
       "          [ 2.1622e-02, -2.4268e-01, -2.8320e-01,  ..., -2.5177e-04,\n",
       "           -8.9722e-03, -3.0200e-01],\n",
       "          [-2.9077e-01,  3.3960e-01, -5.0735e-03,  ..., -1.4877e-02,\n",
       "            6.8848e-02, -9.8755e-02]],\n",
       "\n",
       "         [[ 2.6550e-03, -3.1853e-04,  2.7252e-02,  ..., -2.9221e-03,\n",
       "           -2.3899e-03, -2.9984e-03],\n",
       "          [-4.8071e-01,  2.4658e-01,  4.5117e-01,  ...,  1.8945e-01,\n",
       "           -5.2734e-02, -1.8112e-02],\n",
       "          [-5.9668e-01, -1.6479e-01,  3.1616e-02,  ...,  1.4565e-02,\n",
       "            3.0884e-01, -2.0020e-01],\n",
       "          ...,\n",
       "          [-4.7119e-02, -1.4795e-01, -5.5420e-01,  ...,  2.7734e-01,\n",
       "           -2.5513e-02,  2.5806e-01],\n",
       "          [ 1.9824e-01, -8.8379e-02, -4.1968e-01,  ..., -6.3293e-02,\n",
       "            1.6382e-01, -3.5522e-01],\n",
       "          [ 2.9883e-01, -1.3110e-01, -2.3572e-01,  ...,  3.5645e-02,\n",
       "            5.6946e-02,  1.8555e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.1372e-02, -3.4790e-02,  3.5797e-02,  ...,  3.1891e-02,\n",
       "            8.6212e-04, -1.8997e-03],\n",
       "          [-9.3140e-02,  9.5886e-02,  4.3213e-01,  ...,  4.8828e-02,\n",
       "           -4.1064e-01, -8.5205e-02],\n",
       "          [-9.1309e-02,  1.8958e-01,  1.0883e-01,  ...,  2.5269e-01,\n",
       "           -1.8677e-01,  3.0396e-02],\n",
       "          ...,\n",
       "          [-2.9565e-01,  8.1299e-02, -2.2131e-01,  ...,  1.9446e-01,\n",
       "           -3.5693e-01, -4.5190e-01],\n",
       "          [-2.6025e-01,  4.7058e-02, -2.1655e-01,  ...,  1.7468e-01,\n",
       "           -2.4561e-01, -4.5581e-01],\n",
       "          [-2.7441e-01,  3.3203e-02, -1.9116e-01,  ...,  1.4636e-01,\n",
       "           -2.9199e-01, -4.4629e-01]],\n",
       "\n",
       "         [[ 1.2085e-02,  2.3331e-02, -4.7302e-04,  ..., -1.0452e-02,\n",
       "            2.6672e-02,  6.8283e-03],\n",
       "          [-2.3950e-01,  5.3125e-01, -3.4729e-02,  ...,  4.3091e-02,\n",
       "           -1.4587e-01, -8.0948e-03],\n",
       "          [ 1.5955e-01,  1.9678e-01, -1.5442e-01,  ...,  2.1008e-01,\n",
       "            3.2959e-03, -3.1006e-01],\n",
       "          ...,\n",
       "          [-2.8003e-01,  2.5317e-01,  1.1273e-01,  ...,  1.8396e-01,\n",
       "           -3.1641e-01,  2.4109e-01],\n",
       "          [-2.9443e-01,  2.5830e-01,  1.1719e-01,  ...,  1.7114e-01,\n",
       "           -3.7378e-01,  2.5513e-01],\n",
       "          [-3.3081e-01,  1.6992e-01,  1.8848e-01,  ...,  1.3977e-01,\n",
       "           -3.6255e-01,  2.1252e-01]],\n",
       "\n",
       "         [[ 1.5915e-02, -3.4771e-03,  1.2039e-02,  ..., -9.3231e-03,\n",
       "           -7.3242e-03,  1.2375e-02],\n",
       "          [ 8.7280e-02, -2.2437e-01,  1.8701e-01,  ..., -2.4524e-01,\n",
       "           -1.0908e+00, -6.0107e-01],\n",
       "          [-1.1072e-01,  2.5928e-01, -1.8823e-01,  ..., -6.9214e-02,\n",
       "           -5.5908e-01, -3.7598e-01],\n",
       "          ...,\n",
       "          [-6.4758e-02, -2.1088e-02, -6.6162e-02,  ..., -2.7539e-01,\n",
       "           -2.5146e-01, -9.1431e-02],\n",
       "          [-5.3650e-02,  3.4119e-02, -3.7018e-02,  ..., -2.7515e-01,\n",
       "           -2.2241e-01, -1.3843e-01],\n",
       "          [-4.2175e-02,  5.3925e-02, -9.8999e-02,  ..., -2.6807e-01,\n",
       "           -2.4841e-01, -6.3660e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.7863e-02,  3.1738e-02,  4.6616e-03,  ..., -1.5984e-03,\n",
       "           -2.4357e-03, -1.3382e-02],\n",
       "          [ 1.0480e-01, -4.8804e-01,  4.2358e-02,  ..., -1.4233e-01,\n",
       "            1.7981e-01, -4.3396e-02],\n",
       "          [-9.8206e-02, -7.3535e-01,  2.9144e-03,  ..., -4.4800e-02,\n",
       "            5.2124e-02,  2.3962e-01],\n",
       "          ...,\n",
       "          [-2.2314e-01, -2.2858e-02,  7.7148e-02,  ...,  2.9785e-01,\n",
       "           -2.3743e-01,  1.5503e-01],\n",
       "          [-1.9885e-01,  6.6833e-03,  9.9487e-02,  ...,  3.0713e-01,\n",
       "           -1.7957e-01,  1.7236e-01],\n",
       "          [-3.0542e-01,  6.5613e-03,  1.0883e-01,  ...,  2.7344e-01,\n",
       "           -1.1847e-01,  2.1472e-01]],\n",
       "\n",
       "         [[-8.5373e-03, -2.4719e-02,  2.0187e-02,  ..., -3.2654e-03,\n",
       "           -1.6174e-02,  1.1330e-02],\n",
       "          [-5.8643e-01, -1.5686e-01,  1.1194e-01,  ..., -4.3628e-01,\n",
       "           -6.7688e-02,  1.2848e-02],\n",
       "          [ 2.0459e-01,  1.1133e-01,  8.0750e-02,  ..., -3.1494e-01,\n",
       "           -1.0297e-01, -1.9653e-01],\n",
       "          ...,\n",
       "          [ 8.2581e-02,  1.2036e-01, -3.0640e-01,  ..., -5.5786e-02,\n",
       "            2.0966e-02, -2.9492e-01],\n",
       "          [ 9.1003e-02,  1.0095e-01, -3.1104e-01,  ..., -8.5632e-02,\n",
       "            5.6580e-02, -3.0786e-01],\n",
       "          [ 1.1163e-01,  1.3037e-01, -2.6636e-01,  ..., -1.1151e-01,\n",
       "            4.3762e-02, -2.9663e-01]],\n",
       "\n",
       "         [[ 2.6550e-03, -3.1853e-04,  2.7252e-02,  ..., -2.9221e-03,\n",
       "           -2.3899e-03, -2.9984e-03],\n",
       "          [-4.8071e-01,  2.4658e-01,  4.5117e-01,  ...,  1.8945e-01,\n",
       "           -5.2734e-02, -1.8112e-02],\n",
       "          [-4.8169e-01, -2.9126e-01,  1.3794e-01,  ..., -1.3672e-01,\n",
       "            3.7964e-01, -1.3635e-01],\n",
       "          ...,\n",
       "          [-1.6931e-01,  2.3303e-01, -8.3160e-03,  ...,  5.8105e-01,\n",
       "           -1.4966e-01, -1.5332e-01],\n",
       "          [-2.0862e-01,  2.1265e-01, -3.5095e-02,  ...,  6.5332e-01,\n",
       "           -1.2476e-01, -1.2817e-01],\n",
       "          [-2.0300e-01,  2.1838e-01,  4.4617e-02,  ...,  6.5771e-01,\n",
       "           -1.8005e-01, -1.3135e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-3.3722e-02,  1.4282e-02, -1.7639e-02,  ..., -5.1270e-01,\n",
       "           -1.4124e-01,  2.3027e+00],\n",
       "          [ 1.3760e+00, -1.1875e+00, -1.8835e-01,  ...,  8.4961e-01,\n",
       "           -4.7852e-01, -3.9004e+00],\n",
       "          [ 1.4316e+00,  8.1787e-01, -5.3955e-01,  ..., -2.7051e-01,\n",
       "           -5.0195e-01, -3.9082e+00],\n",
       "          ...,\n",
       "          [-2.1504e+00, -4.4873e-01,  2.7695e+00,  ...,  2.7441e-01,\n",
       "           -5.8936e-01, -6.2695e+00],\n",
       "          [-6.6113e-01, -4.2627e-01,  9.7314e-01,  ...,  5.8105e-01,\n",
       "           -1.3994e+00, -5.0859e+00],\n",
       "          [-2.5391e-01,  9.8193e-01,  8.1836e-01,  ..., -8.8281e-01,\n",
       "           -5.6982e-01, -3.6172e+00]],\n",
       "\n",
       "         [[ 1.8158e-02, -2.0081e-02,  3.6011e-03,  ..., -2.5952e-01,\n",
       "           -5.0244e-01, -5.6580e-02],\n",
       "          [ 7.5781e-01, -7.0996e-01,  1.2412e+00,  ..., -1.3613e+00,\n",
       "            2.0654e-01, -1.0869e+00],\n",
       "          [-1.9609e+00,  5.0195e-01,  1.8311e+00,  ..., -1.4043e+00,\n",
       "            7.1875e-01, -5.5762e-01],\n",
       "          ...,\n",
       "          [ 1.7051e+00,  1.5498e+00, -6.0303e-01,  ...,  8.3313e-03,\n",
       "           -1.2334e+00, -1.5469e+00],\n",
       "          [ 6.5479e-01,  9.9756e-01, -5.3516e-01,  ..., -2.0288e-01,\n",
       "           -1.4941e-01,  3.5693e-01],\n",
       "          [ 3.7207e-01,  7.6904e-03, -6.0059e-02,  ...,  4.4830e-02,\n",
       "           -3.2837e-01,  1.8086e+00]],\n",
       "\n",
       "         [[-9.0332e-03, -4.2648e-03, -1.4282e-02,  ..., -3.4888e-01,\n",
       "            1.5320e-01, -2.6587e-01],\n",
       "          [-9.2188e-01,  1.1504e+00,  9.6191e-01,  ..., -3.1860e-02,\n",
       "           -1.3545e+00, -1.3311e+00],\n",
       "          [-1.5752e+00,  4.5337e-01, -8.0566e-03,  ..., -3.9673e-01,\n",
       "           -2.6416e-01, -6.1572e-01],\n",
       "          ...,\n",
       "          [ 1.4629e+00,  1.0186e+00, -5.6934e-01,  ..., -1.3936e+00,\n",
       "            1.5137e-02, -1.4463e+00],\n",
       "          [ 5.4199e-01,  4.1504e-01,  5.9180e-01,  ..., -1.0518e+00,\n",
       "            5.2930e-01, -1.8909e-01],\n",
       "          [ 1.0437e-01, -1.7090e-02,  2.2534e-01,  ..., -2.7783e-01,\n",
       "            2.7051e-01, -9.2896e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.1530e-02, -2.2049e-02,  1.3641e-02,  ...,  5.5811e-01,\n",
       "           -7.6758e-01,  3.9990e-01],\n",
       "          [ 1.0098e+00,  9.2188e-01, -8.7988e-01,  ...,  1.5684e+00,\n",
       "            1.6201e+00,  6.3574e-01],\n",
       "          [-1.3794e-01,  1.7070e+00, -2.9150e-01,  ...,  5.5566e-01,\n",
       "            2.6484e+00,  4.4678e-01],\n",
       "          ...,\n",
       "          [-7.1094e-01,  1.1230e-02,  1.3467e+00,  ...,  5.3857e-01,\n",
       "            8.7061e-01,  2.3389e-01],\n",
       "          [ 4.6753e-01,  7.4316e-01, -8.3130e-02,  ..., -5.2100e-01,\n",
       "            2.0996e+00, -5.1003e-03],\n",
       "          [ 1.4736e+00,  1.0996e+00, -5.5225e-01,  ..., -2.9419e-01,\n",
       "            2.6719e+00, -5.0830e-01]],\n",
       "\n",
       "         [[ 2.7985e-02,  2.9388e-02, -5.1270e-03,  ..., -4.3506e-01,\n",
       "            1.3440e-01,  3.2568e-01],\n",
       "          [ 3.9990e-01, -1.5049e+00, -1.3086e+00,  ..., -6.0400e-01,\n",
       "            3.0762e-01, -1.1157e-01],\n",
       "          [-2.4453e+00,  2.5996e+00, -1.8994e+00,  ...,  3.1885e-01,\n",
       "           -2.7295e-01,  4.8584e-01],\n",
       "          ...,\n",
       "          [ 2.6035e+00, -4.4922e-01,  2.3828e+00,  ..., -4.1875e+00,\n",
       "           -6.4111e-01, -3.5938e-01],\n",
       "          [ 1.2852e+00,  3.5742e-01,  7.6514e-01,  ..., -2.2598e+00,\n",
       "           -5.8838e-01,  1.2734e+00],\n",
       "          [-6.2988e-02,  1.5576e-01, -1.8066e-02,  ..., -7.5195e-01,\n",
       "           -1.8662e+00,  2.6016e+00]],\n",
       "\n",
       "         [[ 2.3651e-03,  1.5450e-02,  4.9438e-02,  ..., -3.0347e-01,\n",
       "           -7.4951e-02, -1.2402e-01],\n",
       "          [-9.7656e-01, -8.7585e-02, -1.7324e+00,  ..., -1.8672e+00,\n",
       "           -5.1074e-01, -7.6025e-01],\n",
       "          [-2.4746e+00,  7.2559e-01,  4.7119e-01,  ..., -2.0938e+00,\n",
       "            4.9316e-01, -1.7734e+00],\n",
       "          ...,\n",
       "          [ 1.7480e+00, -3.5391e+00, -7.0117e-01,  ...,  1.2073e-01,\n",
       "           -1.2656e+00, -4.9854e-01],\n",
       "          [ 1.6914e+00, -2.9590e-01, -4.8560e-01,  ..., -4.8340e-01,\n",
       "            4.3506e-01, -1.5771e+00],\n",
       "          [ 9.3945e-01,  9.1309e-01, -1.4551e+00,  ..., -1.4277e+00,\n",
       "            5.6445e-01, -2.4238e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.3722e-02,  1.4282e-02, -1.7639e-02,  ..., -5.1270e-01,\n",
       "           -1.4124e-01,  2.3027e+00],\n",
       "          [ 1.3760e+00, -1.1875e+00, -1.8835e-01,  ...,  8.4961e-01,\n",
       "           -4.7852e-01, -3.9004e+00],\n",
       "          [ 1.9180e+00,  1.0859e+00, -4.6436e-01,  ..., -3.8208e-01,\n",
       "           -4.5850e-01, -3.9824e+00],\n",
       "          ...,\n",
       "          [-6.5332e-01, -1.5137e+00,  3.3301e-01,  ..., -1.6602e+00,\n",
       "           -1.3779e+00, -2.8359e+00],\n",
       "          [-1.3311e+00, -2.5098e-01,  6.8896e-01,  ..., -1.6436e+00,\n",
       "           -1.3340e+00, -2.8926e+00],\n",
       "          [-7.5781e-01,  1.1836e+00,  7.4170e-01,  ..., -1.6768e+00,\n",
       "           -1.3740e+00, -2.8594e+00]],\n",
       "\n",
       "         [[ 1.8158e-02, -2.0081e-02,  3.6011e-03,  ..., -2.5952e-01,\n",
       "           -5.0244e-01, -5.6580e-02],\n",
       "          [ 7.5781e-01, -7.0996e-01,  1.2412e+00,  ..., -1.3613e+00,\n",
       "            2.0654e-01, -1.0869e+00],\n",
       "          [-2.4180e+00,  6.4160e-01,  1.9678e+00,  ..., -2.0547e+00,\n",
       "            1.1855e+00, -1.1426e-01],\n",
       "          ...,\n",
       "          [ 3.3423e-01,  8.8330e-01, -1.1445e+00,  ..., -1.4893e+00,\n",
       "           -4.6240e-01, -3.8574e-01],\n",
       "          [ 1.3691e+00,  1.1895e+00, -5.7080e-01,  ..., -1.4805e+00,\n",
       "           -4.2407e-01, -3.6084e-01],\n",
       "          [ 1.1699e+00,  7.2949e-01,  1.4355e-01,  ..., -1.4453e+00,\n",
       "           -4.0967e-01, -1.3110e-01]],\n",
       "\n",
       "         [[-9.0332e-03, -4.2648e-03, -1.4282e-02,  ..., -3.4888e-01,\n",
       "            1.5320e-01, -2.6587e-01],\n",
       "          [-9.2188e-01,  1.1504e+00,  9.6191e-01,  ..., -3.1860e-02,\n",
       "           -1.3545e+00, -1.3311e+00],\n",
       "          [-1.9854e+00,  7.7979e-01, -2.3267e-01,  ..., -2.4695e-01,\n",
       "           -5.1416e-01, -3.9209e-01],\n",
       "          ...,\n",
       "          [ 8.4521e-01,  5.4590e-01, -4.3945e-01,  ..., -2.5903e-01,\n",
       "            8.3789e-01, -4.0967e-01],\n",
       "          [ 1.1074e+00,  4.5435e-01,  3.1030e-01,  ..., -2.0557e-01,\n",
       "            7.1582e-01, -4.9536e-01],\n",
       "          [ 4.2651e-01,  1.4685e-01,  9.6973e-01,  ..., -1.8555e-01,\n",
       "            6.6748e-01, -6.0645e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.1530e-02, -2.2049e-02,  1.3641e-02,  ...,  5.5811e-01,\n",
       "           -7.6758e-01,  3.9990e-01],\n",
       "          [ 1.0098e+00,  9.2188e-01, -8.7988e-01,  ...,  1.5684e+00,\n",
       "            1.6201e+00,  6.3574e-01],\n",
       "          [-4.0356e-01,  2.1367e+00, -3.5010e-01,  ...,  1.0996e+00,\n",
       "            2.6348e+00,  4.9902e-01],\n",
       "          ...,\n",
       "          [-6.4844e-01, -1.8848e-01,  1.9641e-01,  ..., -5.4053e-01,\n",
       "            1.9297e+00, -1.1426e+00],\n",
       "          [ 4.5459e-01,  6.5918e-01, -3.2196e-02,  ..., -5.5127e-01,\n",
       "            1.9219e+00, -1.1602e+00],\n",
       "          [ 1.1914e+00,  1.0361e+00, -2.5537e-01,  ..., -6.2500e-01,\n",
       "            1.8926e+00, -1.3506e+00]],\n",
       "\n",
       "         [[ 2.7985e-02,  2.9388e-02, -5.1270e-03,  ..., -4.3506e-01,\n",
       "            1.3440e-01,  3.2568e-01],\n",
       "          [ 3.9990e-01, -1.5049e+00, -1.3086e+00,  ..., -6.0400e-01,\n",
       "            3.0762e-01, -1.1157e-01],\n",
       "          [-2.9766e+00,  2.8496e+00, -2.3711e+00,  ...,  5.3613e-01,\n",
       "            2.1729e-01,  5.7764e-01],\n",
       "          ...,\n",
       "          [ 6.5088e-01, -8.5840e-01,  1.2588e+00,  ...,  3.0811e-01,\n",
       "           -6.5381e-01,  2.3770e+00],\n",
       "          [ 1.7920e+00,  5.9814e-01,  8.7256e-01,  ...,  3.8257e-01,\n",
       "           -7.2070e-01,  2.2832e+00],\n",
       "          [ 1.2344e+00,  1.7510e+00,  1.2305e-01,  ...,  1.7444e-01,\n",
       "           -7.0166e-01,  2.5352e+00]],\n",
       "\n",
       "         [[ 2.3651e-03,  1.5450e-02,  4.9438e-02,  ..., -3.0347e-01,\n",
       "           -7.4951e-02, -1.2402e-01],\n",
       "          [-9.7656e-01, -8.7585e-02, -1.7324e+00,  ..., -1.8672e+00,\n",
       "           -5.1074e-01, -7.6025e-01],\n",
       "          [-3.0449e+00,  9.8193e-01,  4.3262e-01,  ..., -1.7559e+00,\n",
       "            7.8906e-01, -1.8359e+00],\n",
       "          ...,\n",
       "          [ 9.2090e-01, -1.2178e+00,  6.4062e-01,  ...,  1.6370e-01,\n",
       "            1.5898e+00, -2.0293e+00],\n",
       "          [ 2.1270e+00, -6.8066e-01, -3.9575e-01,  ...,  4.7974e-02,\n",
       "            1.6006e+00, -2.1074e+00],\n",
       "          [ 1.3896e+00,  3.6133e-01, -1.2100e+00,  ...,  4.5135e-02,\n",
       "            1.5293e+00, -2.2109e+00]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 7.8064e-02, -2.3758e-02, -7.5439e-02,  ..., -3.5477e-04,\n",
       "            9.6817e-03, -2.0599e-04],\n",
       "          [-2.8882e-01,  3.4814e-01, -1.1445e+00,  ...,  8.1250e-01,\n",
       "            1.2781e-01, -4.4482e-01],\n",
       "          [ 3.1641e-01,  1.5649e-01, -5.6543e-01,  ...,  2.3413e-01,\n",
       "           -4.9072e-02, -2.0142e-01],\n",
       "          ...,\n",
       "          [-1.9531e-02,  5.0879e-01, -2.0398e-01,  ..., -5.5713e-01,\n",
       "            8.0383e-02,  1.3977e-01],\n",
       "          [-1.0730e-01,  3.5278e-01, -2.8467e-01,  ...,  3.7183e-01,\n",
       "            1.5881e-01,  8.5693e-02],\n",
       "          [ 6.0400e-01,  1.2383e+00, -4.0527e-01,  ...,  5.2930e-01,\n",
       "            1.4404e-01,  1.0925e-01]],\n",
       "\n",
       "         [[-7.4120e-03, -3.3150e-03, -3.1357e-03,  ...,  6.8855e-03,\n",
       "            1.3397e-02, -4.7417e-03],\n",
       "          [-1.4075e-01,  2.8296e-01, -2.6392e-01,  ..., -1.7029e-01,\n",
       "            1.2067e-01, -9.1675e-02],\n",
       "          [ 8.6731e-02,  1.7419e-01,  1.3501e-01,  ..., -2.3743e-01,\n",
       "           -8.0994e-02, -5.4810e-02],\n",
       "          ...,\n",
       "          [ 3.3862e-01,  1.7615e-01,  1.8896e-01,  ..., -7.1338e-01,\n",
       "           -5.5176e-01,  4.3518e-02],\n",
       "          [ 1.1475e-01,  3.4229e-01,  2.8003e-01,  ..., -3.0762e-01,\n",
       "           -2.2791e-01,  1.5283e-01],\n",
       "          [ 1.5820e-01,  1.2610e-01,  4.4092e-01,  ...,  8.5022e-02,\n",
       "           -5.2783e-01,  8.8379e-02]],\n",
       "\n",
       "         [[-1.8921e-02,  6.0196e-03,  3.0003e-03,  ..., -2.3453e-02,\n",
       "            6.2332e-03, -4.7874e-03],\n",
       "          [ 1.4307e-01,  2.9688e-01, -1.3770e-01,  ...,  4.0161e-01,\n",
       "           -1.2512e-02, -2.5806e-01],\n",
       "          [ 3.5303e-01, -1.1841e-01, -1.4539e-01,  ...,  1.1719e-01,\n",
       "            1.1926e-01,  3.4497e-01],\n",
       "          ...,\n",
       "          [ 2.0264e-01, -1.6602e-02, -2.1088e-02,  ...,  1.2988e-01,\n",
       "           -4.1382e-02,  3.1055e-01],\n",
       "          [ 1.5332e-01, -3.6133e-02,  6.1890e-02,  ..., -1.2543e-02,\n",
       "            9.2590e-02,  3.9258e-01],\n",
       "          [-1.2671e-01,  2.2620e-01, -3.9978e-03,  ...,  4.8438e-01,\n",
       "            2.6318e-01, -7.0251e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.0507e-03,  1.4236e-02,  1.0040e-02,  ...,  1.8936e-02,\n",
       "            1.2955e-02,  6.5674e-02],\n",
       "          [-2.5684e-01, -4.3994e-01, -3.5352e-01,  ..., -2.7222e-01,\n",
       "           -1.9836e-01,  3.1104e-01],\n",
       "          [-3.4009e-01, -5.2612e-02,  1.2671e-01,  ...,  3.1812e-01,\n",
       "           -4.2944e-01, -8.1558e-03],\n",
       "          ...,\n",
       "          [-2.4597e-02,  6.9031e-02,  5.1422e-02,  ..., -2.5024e-01,\n",
       "            1.0229e-01, -1.7588e+00],\n",
       "          [ 3.1348e-01, -7.5623e-02, -7.1533e-02,  ..., -5.3613e-01,\n",
       "           -1.8555e-02, -1.7070e+00],\n",
       "          [-1.2683e-01, -2.1936e-01, -6.3049e-02,  ..., -8.2959e-01,\n",
       "           -1.3702e-02, -1.4722e-01]],\n",
       "\n",
       "         [[ 1.9806e-02,  9.5444e-03,  2.6340e-03,  ..., -7.8583e-03,\n",
       "           -1.3611e-02, -1.4778e-02],\n",
       "          [ 8.8086e-01,  2.0984e-01,  2.7100e-02,  ...,  1.5063e-01,\n",
       "            1.6333e-01,  2.9102e-01],\n",
       "          [ 3.7061e-01, -8.6670e-02, -9.6130e-02,  ...,  1.0693e-01,\n",
       "            4.9805e-02,  1.3513e-01],\n",
       "          ...,\n",
       "          [ 8.5107e-01,  2.0032e-01,  3.3252e-01,  ...,  3.9160e-01,\n",
       "           -1.5125e-01, -3.4888e-01],\n",
       "          [ 2.2607e-01,  1.3855e-01, -5.9319e-03,  ..., -5.1239e-02,\n",
       "            1.2222e-02, -5.7831e-02],\n",
       "          [-4.1431e-01,  1.6687e-01, -1.7603e-01,  ..., -1.5942e-01,\n",
       "           -3.8159e-01,  3.8940e-01]],\n",
       "\n",
       "         [[-4.8065e-04, -9.6924e-02,  2.9602e-03,  ..., -2.5520e-03,\n",
       "            2.7084e-02,  7.2479e-03],\n",
       "          [ 1.8970e-01,  8.0176e-01,  2.4304e-01,  ..., -3.8965e-01,\n",
       "            3.0273e-01, -1.8909e-01],\n",
       "          [ 5.6274e-02, -5.2185e-02, -4.9042e-02,  ...,  1.6113e-02,\n",
       "            2.9834e-01,  6.6833e-02],\n",
       "          ...,\n",
       "          [ 8.4656e-02, -1.7566e-01,  3.0713e-01,  ..., -1.0364e-01,\n",
       "            6.8262e-01,  4.1016e-02],\n",
       "          [-9.2346e-02, -2.5952e-01,  2.8931e-01,  ..., -1.0095e-01,\n",
       "            1.0425e-01,  1.5051e-01],\n",
       "          [-2.1149e-02, -6.6772e-02,  3.1299e-01,  ..., -6.4893e-01,\n",
       "           -2.1851e-01,  1.4099e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 7.8064e-02, -2.3758e-02, -7.5439e-02,  ..., -3.5477e-04,\n",
       "            9.6817e-03, -2.0599e-04],\n",
       "          [-2.8882e-01,  3.4814e-01, -1.1445e+00,  ...,  8.1250e-01,\n",
       "            1.2781e-01, -4.4482e-01],\n",
       "          [ 2.4951e-01,  1.7838e-02, -5.2588e-01,  ...,  1.4270e-01,\n",
       "            1.6101e-01, -1.9727e-01],\n",
       "          ...,\n",
       "          [-2.4597e-01,  1.2476e-01, -3.8770e-01,  ...,  5.1416e-01,\n",
       "            9.5154e-02, -1.4856e-01],\n",
       "          [-2.5049e-01,  1.5076e-01, -3.8916e-01,  ...,  5.6494e-01,\n",
       "            1.4832e-01, -1.2280e-01],\n",
       "          [-3.1152e-01,  1.9141e-01, -3.6987e-01,  ...,  5.6738e-01,\n",
       "            1.4014e-01, -8.6975e-02]],\n",
       "\n",
       "         [[-7.4120e-03, -3.3150e-03, -3.1357e-03,  ...,  6.8855e-03,\n",
       "            1.3397e-02, -4.7417e-03],\n",
       "          [-1.4075e-01,  2.8296e-01, -2.6392e-01,  ..., -1.7029e-01,\n",
       "            1.2067e-01, -9.1675e-02],\n",
       "          [-1.0675e-01, -6.9336e-02,  2.2964e-02,  ..., -2.7930e-01,\n",
       "           -3.4033e-01, -9.1064e-02],\n",
       "          ...,\n",
       "          [ 2.3828e-01, -3.9581e-02, -2.5464e-01,  ...,  8.1970e-02,\n",
       "           -3.6963e-01, -1.4084e-02],\n",
       "          [ 2.5537e-01, -1.6541e-02, -2.6562e-01,  ...,  4.4128e-02,\n",
       "           -3.8916e-01,  9.5291e-03],\n",
       "          [ 2.5732e-01,  4.1534e-02, -2.0544e-01,  ...,  2.9816e-02,\n",
       "           -3.6572e-01, -9.6436e-03]],\n",
       "\n",
       "         [[-1.8921e-02,  6.0196e-03,  3.0003e-03,  ..., -2.3453e-02,\n",
       "            6.2332e-03, -4.7874e-03],\n",
       "          [ 1.4307e-01,  2.9688e-01, -1.3770e-01,  ...,  4.0161e-01,\n",
       "           -1.2512e-02, -2.5806e-01],\n",
       "          [ 2.8467e-01,  8.2764e-02, -1.0858e-01,  ...,  1.2463e-01,\n",
       "            2.0422e-01,  1.2244e-01],\n",
       "          ...,\n",
       "          [-2.8052e-01, -2.4805e-01, -3.5156e-01,  ..., -1.3049e-01,\n",
       "            1.2189e-01,  6.3916e-01],\n",
       "          [-2.7368e-01, -2.9028e-01, -3.9331e-01,  ..., -1.0681e-01,\n",
       "            1.2683e-01,  6.0889e-01],\n",
       "          [-2.9858e-01, -2.9175e-01, -3.6499e-01,  ..., -1.5942e-01,\n",
       "            1.2396e-01,  6.5381e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.0507e-03,  1.4236e-02,  1.0040e-02,  ...,  1.8936e-02,\n",
       "            1.2955e-02,  6.5674e-02],\n",
       "          [-2.5684e-01, -4.3994e-01, -3.5352e-01,  ..., -2.7222e-01,\n",
       "           -1.9836e-01,  3.1104e-01],\n",
       "          [-2.4646e-01, -1.5358e-02,  7.6660e-02,  ...,  1.9409e-01,\n",
       "           -1.6870e-01,  2.9492e-01],\n",
       "          ...,\n",
       "          [ 6.4795e-01,  1.4453e-01, -2.4451e-01,  ..., -2.5684e-01,\n",
       "            3.3765e-01,  6.9092e-02],\n",
       "          [ 6.9141e-01,  1.7322e-01, -3.1543e-01,  ..., -3.0640e-01,\n",
       "            4.0894e-01, -6.7787e-03],\n",
       "          [ 6.8555e-01,  1.4417e-01, -2.7856e-01,  ..., -2.2437e-01,\n",
       "            3.6230e-01,  7.6172e-02]],\n",
       "\n",
       "         [[ 1.9806e-02,  9.5444e-03,  2.6340e-03,  ..., -7.8583e-03,\n",
       "           -1.3611e-02, -1.4778e-02],\n",
       "          [ 8.8086e-01,  2.0984e-01,  2.7100e-02,  ...,  1.5063e-01,\n",
       "            1.6333e-01,  2.9102e-01],\n",
       "          [ 2.4609e-01, -1.5457e-02,  4.0283e-02,  ..., -3.9062e-02,\n",
       "            6.7688e-02,  1.3379e-01],\n",
       "          ...,\n",
       "          [-1.2878e-01, -2.1521e-01, -8.5205e-02,  ...,  1.8845e-02,\n",
       "           -2.6270e-01, -7.4524e-02],\n",
       "          [-8.8074e-02, -1.2524e-01, -1.0480e-01,  ..., -1.5251e-02,\n",
       "           -3.2300e-01, -9.4360e-02],\n",
       "          [-8.7952e-02, -1.8115e-01, -9.9060e-02,  ..., -6.7078e-02,\n",
       "           -3.3618e-01, -1.1627e-01]],\n",
       "\n",
       "         [[-4.8065e-04, -9.6924e-02,  2.9602e-03,  ..., -2.5520e-03,\n",
       "            2.7084e-02,  7.2479e-03],\n",
       "          [ 1.8970e-01,  8.0176e-01,  2.4304e-01,  ..., -3.8965e-01,\n",
       "            3.0273e-01, -1.8909e-01],\n",
       "          [ 1.1438e-01,  1.1407e-01, -2.3035e-01,  ..., -1.1133e-01,\n",
       "            3.1128e-01,  2.6465e-01],\n",
       "          ...,\n",
       "          [ 3.9209e-01, -4.6289e-01,  7.2144e-02,  ..., -2.4683e-01,\n",
       "            1.3147e-01,  1.8640e-01],\n",
       "          [ 4.0454e-01, -4.4873e-01,  9.0271e-02,  ..., -2.4182e-01,\n",
       "            9.8267e-02,  1.3342e-01],\n",
       "          [ 4.1089e-01, -3.7085e-01,  1.2335e-01,  ..., -2.6245e-01,\n",
       "            5.3833e-02,  9.6741e-02]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-3.3607e-03, -1.9348e-02,  8.3008e-03,  ..., -6.2939e-01,\n",
       "           -1.8936e+00,  8.7549e-01],\n",
       "          [-3.2520e-01,  4.6802e-01, -2.6221e-01,  ...,  3.6084e-01,\n",
       "           -5.8350e-01,  3.3945e+00],\n",
       "          [ 2.5635e-01, -3.4729e-02,  1.2439e-01,  ...,  1.7695e+00,\n",
       "            7.8711e-01,  2.6270e+00],\n",
       "          ...,\n",
       "          [ 6.5186e-02, -1.4038e-01,  3.1250e-01,  ..., -3.3320e+00,\n",
       "           -8.7549e-01, -4.4238e-01],\n",
       "          [-1.7815e-03,  5.0537e-02, -1.4856e-01,  ..., -1.9355e+00,\n",
       "            1.1926e-01,  1.1123e+00],\n",
       "          [ 1.4429e-01, -1.5527e-01,  1.2451e-02,  ...,  7.2168e-01,\n",
       "           -5.9277e-01,  2.2598e+00]],\n",
       "\n",
       "         [[-4.6692e-03, -8.1482e-03, -6.5918e-03,  ...,  2.2266e-01,\n",
       "           -4.0131e-02,  2.4258e+00],\n",
       "          [ 3.8086e-01,  8.5107e-01, -3.5181e-01,  ...,  9.2432e-01,\n",
       "            7.2998e-01, -4.1797e+00],\n",
       "          [-1.8274e-01,  7.9883e-01, -5.5908e-02,  ..., -2.3486e-01,\n",
       "            1.8091e-01, -3.9102e+00],\n",
       "          ...,\n",
       "          [-1.7988e+00,  2.9932e-01, -6.7529e-01,  ...,  1.9248e+00,\n",
       "            1.0876e-01, -7.5234e+00],\n",
       "          [ 2.4817e-01,  3.1641e-01, -2.7100e-01,  ...,  8.3984e-02,\n",
       "           -5.4138e-02, -4.8672e+00],\n",
       "          [ 1.0687e-01,  3.1470e-01, -1.4355e-01,  ..., -6.3428e-01,\n",
       "           -1.0107e+00, -3.9941e+00]],\n",
       "\n",
       "         [[-2.9434e-02,  2.3926e-02,  5.3711e-03,  ..., -1.0779e-01,\n",
       "            4.3579e-01, -6.2988e-01],\n",
       "          [ 9.6777e-01, -4.1431e-01, -1.4160e-02,  ..., -4.2139e-01,\n",
       "            3.2324e+00, -3.4453e+00],\n",
       "          [ 4.0161e-01,  2.7893e-02, -6.0547e-01,  ...,  4.0625e-01,\n",
       "            7.5879e-01, -8.8281e-01],\n",
       "          ...,\n",
       "          [-1.3049e-01,  3.2251e-01, -1.0071e-01,  ...,  1.5820e+00,\n",
       "            1.9609e+00, -2.0078e+00],\n",
       "          [-3.4106e-01,  1.3550e-01, -7.9163e-02,  ...,  2.1816e+00,\n",
       "            2.7148e-01, -1.6484e+00],\n",
       "          [-1.2909e-02,  6.1084e-01,  1.1389e-01,  ...,  1.4766e+00,\n",
       "            1.3369e+00, -6.4844e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.7181e-02, -7.3242e-04,  2.6550e-02,  ..., -7.6562e-01,\n",
       "            5.2917e-02,  4.9072e-02],\n",
       "          [-1.2217e+00, -9.9316e-01, -1.4727e+00,  ..., -4.7119e-02,\n",
       "            2.6523e+00, -1.0039e+00],\n",
       "          [-5.6738e-01, -3.7903e-02, -7.7637e-01,  ...,  6.2451e-01,\n",
       "            1.5322e+00,  2.2742e-01],\n",
       "          ...,\n",
       "          [ 1.3203e+00,  4.4775e-01,  1.4844e+00,  ..., -2.1924e-01,\n",
       "            3.8164e+00, -3.0449e+00],\n",
       "          [ 3.5376e-01, -4.7095e-01,  5.5762e-01,  ...,  1.2246e+00,\n",
       "            4.6406e+00, -2.7324e+00],\n",
       "          [-6.0059e-02,  7.4316e-01,  4.4678e-02,  ...,  1.6602e+00,\n",
       "            3.0215e+00,  2.6050e-01]],\n",
       "\n",
       "         [[ 1.0925e-02, -2.0142e-02,  3.7384e-03,  ..., -2.5659e-01,\n",
       "           -1.3293e-01,  1.2451e-01],\n",
       "          [ 1.8555e-01, -5.2832e-01, -4.2090e-01,  ...,  6.1328e-01,\n",
       "            6.3086e-01,  1.3379e-01],\n",
       "          [-9.4189e-01,  2.3291e-01,  6.7627e-01,  ...,  1.9141e-01,\n",
       "           -1.7783e+00,  1.0928e+00],\n",
       "          ...,\n",
       "          [ 5.5762e-01,  9.1113e-01, -2.6807e-01,  ..., -4.0137e-01,\n",
       "           -1.5205e+00, -4.4336e+00],\n",
       "          [ 9.4141e-01,  1.1553e+00, -9.5654e-01,  ..., -8.2812e-01,\n",
       "           -8.2715e-01, -7.8955e-01],\n",
       "          [ 4.8633e-01,  5.5811e-01, -6.6162e-01,  ..., -4.5068e-01,\n",
       "           -2.8203e+00, -2.1826e-01]],\n",
       "\n",
       "         [[-2.3041e-02, -1.6205e-02, -8.7585e-03,  ...,  6.4453e-01,\n",
       "           -5.2637e-01, -7.2510e-01],\n",
       "          [ 8.9160e-01,  8.9502e-01,  6.6602e-01,  ...,  9.0479e-01,\n",
       "           -9.5996e-01, -1.7617e+00],\n",
       "          [ 4.2871e-01,  1.9312e-01,  1.5371e+00,  ...,  4.8462e-01,\n",
       "            1.8604e-01, -1.1143e+00],\n",
       "          ...,\n",
       "          [-2.0117e+00,  3.4023e+00, -1.1475e+00,  ...,  6.9580e-01,\n",
       "           -5.5127e-01, -1.5391e+00],\n",
       "          [-9.8535e-01,  1.4775e+00, -1.9346e+00,  ..., -8.5107e-01,\n",
       "           -9.2969e-01, -6.2012e-01],\n",
       "          [ 2.0435e-01,  2.2559e-01, -1.2197e+00,  ..., -1.7549e+00,\n",
       "           -1.7651e-01, -1.1934e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.3607e-03, -1.9348e-02,  8.3008e-03,  ..., -6.2939e-01,\n",
       "           -1.8936e+00,  8.7549e-01],\n",
       "          [-3.2520e-01,  4.6802e-01, -2.6221e-01,  ...,  3.6084e-01,\n",
       "           -5.8350e-01,  3.3945e+00],\n",
       "          [-7.6172e-02, -6.8481e-02,  6.4636e-02,  ...,  1.2461e+00,\n",
       "            2.3027e+00,  2.9883e+00],\n",
       "          ...,\n",
       "          [ 5.6152e-01,  3.8184e-01,  7.3242e-01,  ...,  1.2559e+00,\n",
       "            2.4609e+00,  3.5352e+00],\n",
       "          [-3.4546e-02, -1.5552e-01,  5.4199e-01,  ...,  1.2178e+00,\n",
       "            2.5234e+00,  3.6836e+00],\n",
       "          [-6.3672e-01, -5.5469e-01,  1.2354e-01,  ...,  1.2012e+00,\n",
       "            2.7344e+00,  3.8281e+00]],\n",
       "\n",
       "         [[-4.6692e-03, -8.1482e-03, -6.5918e-03,  ...,  2.2266e-01,\n",
       "           -4.0131e-02,  2.4258e+00],\n",
       "          [ 3.8086e-01,  8.5107e-01, -3.5181e-01,  ...,  9.2432e-01,\n",
       "            7.2998e-01, -4.1797e+00],\n",
       "          [ 2.0251e-01,  8.8379e-01,  4.9878e-01,  ...,  6.9092e-02,\n",
       "            2.7295e-01, -3.4199e+00],\n",
       "          ...,\n",
       "          [-4.7168e-01,  8.2227e-01, -3.7280e-01,  ...,  1.9912e+00,\n",
       "            2.8198e-01, -2.2402e+00],\n",
       "          [ 1.5369e-01,  1.1260e+00, -5.9570e-01,  ...,  2.0723e+00,\n",
       "            2.0557e-01, -2.2871e+00],\n",
       "          [ 6.9385e-01,  7.6465e-01, -5.0000e-01,  ...,  2.3398e+00,\n",
       "            2.4878e-01, -2.2500e+00]],\n",
       "\n",
       "         [[-2.9434e-02,  2.3926e-02,  5.3711e-03,  ..., -1.0779e-01,\n",
       "            4.3579e-01, -6.2988e-01],\n",
       "          [ 9.6777e-01, -4.1431e-01, -1.4160e-02,  ..., -4.2139e-01,\n",
       "            3.2324e+00, -3.4453e+00],\n",
       "          [ 6.1914e-01, -6.2439e-02, -8.0859e-01,  ...,  9.2188e-01,\n",
       "           -5.2539e-01, -1.7715e+00],\n",
       "          ...,\n",
       "          [-5.1465e-01,  1.3647e-01,  1.7957e-01,  ..., -5.0684e-01,\n",
       "           -1.1660e+00, -4.3115e-01],\n",
       "          [-6.3379e-01,  4.7632e-01,  5.6738e-01,  ..., -2.6660e-01,\n",
       "           -6.7773e-01, -3.9307e-01],\n",
       "          [-2.0776e-01,  4.3213e-01,  6.0352e-01,  ..., -1.2164e-01,\n",
       "           -1.1270e+00, -7.4756e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.7181e-02, -7.3242e-04,  2.6550e-02,  ..., -7.6562e-01,\n",
       "            5.2917e-02,  4.9072e-02],\n",
       "          [-1.2217e+00, -9.9316e-01, -1.4727e+00,  ..., -4.7119e-02,\n",
       "            2.6523e+00, -1.0039e+00],\n",
       "          [-1.0137e+00,  5.8203e-01, -5.0146e-01,  ...,  3.8330e-01,\n",
       "            1.7998e+00, -2.6440e-01],\n",
       "          ...,\n",
       "          [ 7.9980e-01, -4.6533e-01,  9.2383e-01,  ...,  5.1465e-01,\n",
       "           -9.5117e-01,  1.9561e+00],\n",
       "          [ 1.0146e+00,  1.4526e-01,  6.5039e-01,  ...,  5.1855e-01,\n",
       "           -1.1504e+00,  1.9521e+00],\n",
       "          [ 3.7793e-01,  6.0840e-01,  1.4648e-01,  ...,  5.0879e-01,\n",
       "           -1.0742e+00,  2.0078e+00]],\n",
       "\n",
       "         [[ 1.0925e-02, -2.0142e-02,  3.7384e-03,  ..., -2.5659e-01,\n",
       "           -1.3293e-01,  1.2451e-01],\n",
       "          [ 1.8555e-01, -5.2832e-01, -4.2090e-01,  ...,  6.1328e-01,\n",
       "            6.3086e-01,  1.3379e-01],\n",
       "          [-9.6289e-01,  1.1639e-01,  7.9150e-01,  ..., -3.6011e-01,\n",
       "           -1.4609e+00,  1.2520e+00],\n",
       "          ...,\n",
       "          [ 5.5664e-01,  2.1802e-01,  1.1133e-01,  ...,  1.5396e-02,\n",
       "           -3.2422e+00, -4.8096e-01],\n",
       "          [ 1.7178e+00,  1.0342e+00, -6.8408e-01,  ...,  2.3987e-01,\n",
       "           -3.2207e+00, -6.0986e-01],\n",
       "          [ 1.1602e+00,  1.1074e+00, -1.0234e+00,  ...,  1.4282e-01,\n",
       "           -3.2949e+00, -8.9160e-01]],\n",
       "\n",
       "         [[-2.3041e-02, -1.6205e-02, -8.7585e-03,  ...,  6.4453e-01,\n",
       "           -5.2637e-01, -7.2510e-01],\n",
       "          [ 8.9160e-01,  8.9502e-01,  6.6602e-01,  ...,  9.0479e-01,\n",
       "           -9.5996e-01, -1.7617e+00],\n",
       "          [ 5.5811e-01,  2.4634e-01,  1.3223e+00,  ...,  1.1279e+00,\n",
       "            4.1699e-01, -1.2451e+00],\n",
       "          ...,\n",
       "          [-1.2363e+00,  1.4375e+00, -8.5059e-01,  ...,  3.6377e-01,\n",
       "            2.8809e-01, -4.1577e-01],\n",
       "          [-9.6191e-01,  1.1592e+00, -1.3516e+00,  ...,  3.7036e-01,\n",
       "            3.4692e-01, -5.8008e-01],\n",
       "          [ 1.9800e-01,  2.3877e-01, -1.3086e+00,  ...,  3.7158e-01,\n",
       "            3.7036e-01, -6.4111e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-5.9319e-03,  2.7466e-03, -9.6893e-04,  ...,  2.2934e-02,\n",
       "            3.7766e-04, -1.6693e-02],\n",
       "          [-5.7281e-02, -1.8677e-01,  1.2238e-01,  ...,  1.4868e-01,\n",
       "           -4.2603e-01, -5.8502e-02],\n",
       "          [-1.5808e-01, -6.7871e-02, -5.8228e-02,  ...,  1.7883e-01,\n",
       "           -2.9810e-01,  1.5295e-01],\n",
       "          ...,\n",
       "          [ 2.4988e-01, -9.7717e-02, -1.4233e-01,  ..., -1.4014e-01,\n",
       "            1.5491e-01, -1.0205e-01],\n",
       "          [ 8.8989e-02, -2.4292e-02,  1.6785e-01,  ...,  3.0029e-01,\n",
       "           -3.1714e-01, -1.3562e-01],\n",
       "          [ 2.3560e-01, -2.9761e-01,  2.5787e-02,  ...,  5.0977e-01,\n",
       "           -1.4014e-01, -1.9067e-01]],\n",
       "\n",
       "         [[ 8.8806e-03,  1.6510e-02, -3.7720e-02,  ...,  5.5237e-03,\n",
       "           -6.6757e-03, -8.0795e-03],\n",
       "          [-1.5234e-01, -1.7310e-01,  1.8652e-01,  ...,  2.2021e-01,\n",
       "           -4.3481e-01, -4.7607e-01],\n",
       "          [-3.0371e-01,  2.2961e-01, -3.2593e-01,  ...,  4.4434e-01,\n",
       "           -7.3059e-02, -1.9629e-01],\n",
       "          ...,\n",
       "          [-5.1025e-02,  6.2793e-01,  1.1902e-01,  ...,  2.6855e-01,\n",
       "           -1.1475e-01, -1.2512e-01],\n",
       "          [ 4.8120e-01,  3.0005e-01, -1.6345e-01,  ...,  2.7588e-01,\n",
       "           -4.6558e-01, -1.2866e-01],\n",
       "          [ 6.1621e-01, -9.7534e-02, -3.1738e-01,  ...,  3.6255e-01,\n",
       "            2.3254e-02, -6.8298e-02]],\n",
       "\n",
       "         [[-4.5929e-03, -4.2992e-03,  5.1270e-03,  ..., -1.9409e-02,\n",
       "            5.8632e-03,  3.0502e-02],\n",
       "          [-2.1072e-02, -4.8065e-02, -2.3987e-01,  ..., -2.0251e-01,\n",
       "           -4.5074e-02,  1.0571e-01],\n",
       "          [ 3.0322e-01,  9.7717e-02,  2.0584e-02,  ..., -9.2590e-02,\n",
       "           -2.4933e-02, -8.0994e-02],\n",
       "          ...,\n",
       "          [ 3.1396e-01, -5.1483e-02, -4.5929e-02,  ..., -2.0422e-01,\n",
       "            2.2241e-01, -8.6975e-02],\n",
       "          [ 2.5415e-01,  1.6028e-01,  3.2080e-01,  ..., -8.6060e-02,\n",
       "            2.2607e-01, -2.7197e-01],\n",
       "          [-3.6987e-02, -3.0441e-02,  5.3125e-01,  ...,  1.7053e-01,\n",
       "            5.8350e-01, -1.9202e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 9.6512e-03, -6.4011e-03,  1.0582e-02,  ...,  1.1101e-03,\n",
       "            7.5150e-03, -1.7273e-02],\n",
       "          [-4.1138e-02, -1.2360e-01, -2.8638e-01,  ..., -5.2588e-01,\n",
       "            3.7476e-02, -1.3257e-01],\n",
       "          [ 2.8534e-03, -2.3083e-01,  1.2183e-01,  ..., -3.6768e-01,\n",
       "            5.5359e-02,  8.2520e-02],\n",
       "          ...,\n",
       "          [ 7.1655e-02,  1.8091e-01,  1.2756e-01,  ...,  3.2935e-01,\n",
       "            5.8350e-02, -8.2275e-02],\n",
       "          [ 4.0088e-01,  2.7979e-01,  1.8188e-01,  ...,  2.0618e-01,\n",
       "           -3.8916e-01, -5.9753e-02],\n",
       "          [ 4.1699e-01,  2.4341e-01, -5.8197e-02,  ..., -1.0345e-01,\n",
       "           -1.3086e-01,  7.5439e-02]],\n",
       "\n",
       "         [[ 5.9631e-02, -5.8594e-03, -1.4572e-03,  ..., -2.9068e-03,\n",
       "           -2.3518e-03, -1.6994e-03],\n",
       "          [ 7.9541e-01, -5.5664e-01,  5.0928e-01,  ...,  1.4148e-01,\n",
       "            3.4692e-01,  9.2578e-01],\n",
       "          [-1.0486e-01, -1.8542e-01, -6.1310e-02,  ...,  9.1858e-03,\n",
       "           -4.3848e-01,  4.5190e-01],\n",
       "          ...,\n",
       "          [-3.9844e-01,  1.5857e-01,  4.5166e-01,  ...,  5.0830e-01,\n",
       "           -2.5269e-01,  3.4375e-01],\n",
       "          [-3.2520e-01,  3.5706e-02,  1.3867e-01,  ...,  3.3960e-01,\n",
       "           -2.3633e-01,  1.1243e-01],\n",
       "          [-3.3752e-02,  4.1797e-01,  2.0435e-01,  ...,  1.1602e+00,\n",
       "            2.9468e-01, -4.3060e-02]],\n",
       "\n",
       "         [[-1.3275e-03, -2.6199e-02, -1.6754e-02,  ...,  3.6957e-02,\n",
       "           -1.8478e-02, -5.7716e-03],\n",
       "          [ 4.9744e-02, -4.4556e-02,  1.9568e-01,  ..., -1.9849e-01,\n",
       "           -1.7554e-01, -4.1809e-02],\n",
       "          [-1.9666e-01, -2.0581e-01,  8.2153e-02,  ..., -1.1115e-01,\n",
       "           -1.2085e-01,  2.5659e-01],\n",
       "          ...,\n",
       "          [ 1.7603e-01, -6.7627e-02,  2.4915e-01,  ..., -1.4551e-01,\n",
       "           -1.9592e-02,  3.0615e-01],\n",
       "          [ 2.3950e-01, -2.2827e-02,  1.9092e-01,  ..., -3.4399e-01,\n",
       "            5.0781e-02,  3.9722e-01],\n",
       "          [-4.9316e-01,  2.1362e-02,  3.6987e-01,  ..., -4.4507e-01,\n",
       "           -1.5088e-01,  6.0791e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.9319e-03,  2.7466e-03, -9.6893e-04,  ...,  2.2934e-02,\n",
       "            3.7766e-04, -1.6693e-02],\n",
       "          [-5.7281e-02, -1.8677e-01,  1.2238e-01,  ...,  1.4868e-01,\n",
       "           -4.2603e-01, -5.8502e-02],\n",
       "          [-6.1676e-02,  1.3904e-01, -1.0620e-01,  ...,  8.9233e-02,\n",
       "           -1.8530e-01,  1.3684e-01],\n",
       "          ...,\n",
       "          [ 1.2372e-01, -2.8662e-01,  4.4250e-02,  ..., -5.0049e-02,\n",
       "           -5.4504e-02, -4.1089e-01],\n",
       "          [ 1.5894e-01, -3.3594e-01,  4.7272e-02,  ..., -2.8610e-03,\n",
       "           -5.1758e-02, -4.5728e-01],\n",
       "          [ 1.2927e-01, -3.1421e-01,  6.7078e-02,  ...,  1.7639e-02,\n",
       "           -9.0637e-02, -3.8965e-01]],\n",
       "\n",
       "         [[ 8.8806e-03,  1.6510e-02, -3.7720e-02,  ...,  5.5237e-03,\n",
       "           -6.6757e-03, -8.0795e-03],\n",
       "          [-1.5234e-01, -1.7310e-01,  1.8652e-01,  ...,  2.2021e-01,\n",
       "           -4.3481e-01, -4.7607e-01],\n",
       "          [-2.5513e-01,  2.5928e-01, -3.0664e-01,  ...,  5.3027e-01,\n",
       "           -2.2388e-01, -1.8750e-01],\n",
       "          ...,\n",
       "          [ 1.9104e-01,  5.4102e-01,  4.1699e-01,  ...,  1.5808e-02,\n",
       "           -3.0933e-01,  3.2910e-01],\n",
       "          [ 2.1179e-01,  5.7568e-01,  4.2041e-01,  ..., -5.7945e-03,\n",
       "           -3.5864e-01,  3.1396e-01],\n",
       "          [ 1.9312e-01,  7.1191e-01,  4.1528e-01,  ...,  2.7924e-02,\n",
       "           -3.2764e-01,  2.6660e-01]],\n",
       "\n",
       "         [[-4.5929e-03, -4.2992e-03,  5.1270e-03,  ..., -1.9409e-02,\n",
       "            5.8632e-03,  3.0502e-02],\n",
       "          [-2.1072e-02, -4.8065e-02, -2.3987e-01,  ..., -2.0251e-01,\n",
       "           -4.5074e-02,  1.0571e-01],\n",
       "          [ 3.9819e-01, -2.3694e-01, -3.2715e-02,  ...,  1.4458e-02,\n",
       "           -1.6632e-02, -4.8615e-02],\n",
       "          ...,\n",
       "          [-2.2461e-02,  1.3928e-01,  4.3262e-01,  ...,  1.2476e-01,\n",
       "            4.9341e-01, -1.5527e-01],\n",
       "          [-3.7567e-02,  1.3147e-01,  4.1211e-01,  ...,  1.3611e-01,\n",
       "            4.8364e-01, -1.7407e-01],\n",
       "          [-5.8563e-02,  1.6992e-01,  4.7095e-01,  ...,  1.1774e-01,\n",
       "            4.9658e-01, -1.0901e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 9.6512e-03, -6.4011e-03,  1.0582e-02,  ...,  1.1101e-03,\n",
       "            7.5150e-03, -1.7273e-02],\n",
       "          [-4.1138e-02, -1.2360e-01, -2.8638e-01,  ..., -5.2588e-01,\n",
       "            3.7476e-02, -1.3257e-01],\n",
       "          [ 3.2471e-01, -2.8345e-01, -8.0688e-02,  ..., -3.0420e-01,\n",
       "            3.1885e-01,  5.5298e-02],\n",
       "          ...,\n",
       "          [ 5.1318e-01, -4.2358e-02, -4.9194e-01,  ...,  3.0853e-02,\n",
       "            2.4670e-01,  1.6467e-01],\n",
       "          [ 5.4346e-01, -3.1403e-02, -4.7461e-01,  ..., -3.9642e-02,\n",
       "            2.6587e-01,  1.9604e-01],\n",
       "          [ 5.1855e-01, -2.9846e-02, -4.2139e-01,  ..., -8.3618e-02,\n",
       "            2.8125e-01,  2.3315e-01]],\n",
       "\n",
       "         [[ 5.9631e-02, -5.8594e-03, -1.4572e-03,  ..., -2.9068e-03,\n",
       "           -2.3518e-03, -1.6994e-03],\n",
       "          [ 7.9541e-01, -5.5664e-01,  5.0928e-01,  ...,  1.4148e-01,\n",
       "            3.4692e-01,  9.2578e-01],\n",
       "          [-1.0529e-01, -2.4463e-01,  1.5686e-02,  ..., -1.7517e-01,\n",
       "           -2.1692e-01,  3.2202e-01],\n",
       "          ...,\n",
       "          [-2.3657e-01, -7.8369e-02,  3.5327e-01,  ...,  4.2139e-01,\n",
       "            7.4561e-01,  9.4238e-01],\n",
       "          [-2.4097e-01, -9.1248e-02,  3.7036e-01,  ...,  4.8267e-01,\n",
       "            6.6992e-01,  9.7363e-01],\n",
       "          [-2.8174e-01, -4.9316e-02,  2.7881e-01,  ...,  4.1431e-01,\n",
       "            6.9678e-01,  9.5117e-01]],\n",
       "\n",
       "         [[-1.3275e-03, -2.6199e-02, -1.6754e-02,  ...,  3.6957e-02,\n",
       "           -1.8478e-02, -5.7716e-03],\n",
       "          [ 4.9744e-02, -4.4556e-02,  1.9568e-01,  ..., -1.9849e-01,\n",
       "           -1.7554e-01, -4.1809e-02],\n",
       "          [-9.8938e-02, -2.0728e-01,  1.8616e-01,  ..., -1.2463e-01,\n",
       "           -3.2013e-02,  2.0154e-01],\n",
       "          ...,\n",
       "          [-5.4053e-01,  5.5695e-02,  2.9639e-01,  ..., -7.1240e-01,\n",
       "           -4.1779e-02,  4.0503e-01],\n",
       "          [-5.4150e-01,  4.3121e-02,  2.5781e-01,  ..., -7.2217e-01,\n",
       "            2.1820e-02,  4.3506e-01],\n",
       "          [-4.8389e-01,  8.9417e-03,  2.5146e-01,  ..., -7.0215e-01,\n",
       "            7.9346e-04,  3.3569e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-5.9204e-03,  3.4637e-02,  3.2532e-02,  ...,  7.7515e-02,\n",
       "           -2.1387e-01,  2.9980e-01],\n",
       "          [ 5.6738e-01, -3.1934e-01,  6.6113e-01,  ..., -5.1709e-01,\n",
       "           -6.9824e-01,  9.8438e-01],\n",
       "          [-9.0820e-01, -4.2700e-01,  8.1934e-01,  ..., -1.9434e-01,\n",
       "            1.1582e+00, -6.3525e-01],\n",
       "          ...,\n",
       "          [-6.5918e-02, -1.0120e-01, -8.2568e-01,  ...,  3.6641e+00,\n",
       "           -1.5742e+00, -2.8965e+00],\n",
       "          [ 6.1084e-01, -5.7275e-01,  3.4399e-01,  ...,  1.8604e+00,\n",
       "            1.1758e+00, -3.4375e+00],\n",
       "          [ 1.0291e-01, -2.4243e-01,  1.2091e-01,  ...,  4.6606e-01,\n",
       "            2.7754e+00, -3.7324e+00]],\n",
       "\n",
       "         [[ 3.6499e-02, -2.7802e-02, -2.6306e-02,  ..., -1.7627e+00,\n",
       "            2.8320e-01,  4.3335e-02],\n",
       "          [-1.2656e+00,  5.4834e-01,  1.0957e+00,  ...,  1.1543e+00,\n",
       "           -3.2764e-01,  2.8242e+00],\n",
       "          [ 9.5605e-01, -5.9766e-01, -2.0715e-01,  ...,  2.5742e+00,\n",
       "           -5.6055e-01, -1.3418e+00],\n",
       "          ...,\n",
       "          [ 1.4600e-01, -9.6143e-01,  2.8320e-02,  ...,  4.4961e+00,\n",
       "           -5.9766e-01, -2.7754e+00],\n",
       "          [-6.3086e-01, -9.1650e-01,  2.8540e-01,  ...,  4.7617e+00,\n",
       "           -1.0645e+00, -3.5938e+00],\n",
       "          [-4.7852e-01, -4.7510e-01, -3.1641e-01,  ...,  3.0332e+00,\n",
       "           -6.7139e-01, -2.8828e+00]],\n",
       "\n",
       "         [[-2.9297e-02,  5.1575e-03,  2.9724e-02,  ...,  1.8127e-02,\n",
       "            9.1846e-01,  3.7769e-01],\n",
       "          [ 5.3438e+00,  2.5703e+00, -8.0029e-01,  ...,  4.6533e-01,\n",
       "            1.0010e+00,  9.8486e-01],\n",
       "          [-4.9414e-01,  3.8691e+00,  5.1270e-02,  ..., -2.1838e-01,\n",
       "            1.5879e+00,  1.2871e+00],\n",
       "          ...,\n",
       "          [-5.1133e+00,  1.4922e+00, -1.7041e+00,  ...,  5.7080e-01,\n",
       "            2.0840e+00,  2.0234e+00],\n",
       "          [-1.8203e+00,  3.7871e+00, -7.0264e-01,  ...,  1.2383e+00,\n",
       "            2.0879e+00,  5.3369e-01],\n",
       "          [ 3.8984e+00,  1.8857e+00, -7.0459e-01,  ...,  4.0088e-01,\n",
       "            1.5918e+00,  1.2803e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.0942e-03, -1.2444e-02, -2.2827e-02,  ...,  3.0054e-01,\n",
       "           -9.2285e-02, -6.3623e-01],\n",
       "          [ 4.0381e-01,  3.0737e-01, -1.2158e-01,  ..., -5.9961e-01,\n",
       "           -4.7314e-01, -3.6387e+00],\n",
       "          [-5.6982e-01,  6.4648e-01, -8.4814e-01,  ..., -1.8945e-01,\n",
       "            9.2480e-01, -2.9453e+00],\n",
       "          ...,\n",
       "          [-4.6362e-01,  8.1738e-01,  3.5889e-01,  ...,  6.7188e+00,\n",
       "            7.4023e-01, -1.4531e+00],\n",
       "          [ 9.6008e-02, -1.5759e-01,  2.4319e-03,  ...,  5.0781e+00,\n",
       "            3.4688e+00,  2.3608e-01],\n",
       "          [-1.0626e-01,  1.0620e-01, -3.9642e-02,  ...,  2.7285e+00,\n",
       "            3.2578e+00,  6.1670e-01]],\n",
       "\n",
       "         [[-1.1230e-02, -1.9958e-02, -8.5144e-03,  ..., -5.0098e-01,\n",
       "            1.6479e-01,  3.1738e-02],\n",
       "          [ 3.0640e-01,  6.7969e-01, -3.8330e-01,  ..., -1.9863e+00,\n",
       "           -4.4922e-01, -1.1353e-01],\n",
       "          [-2.1172e+00, -4.9622e-02,  1.8567e-01,  ...,  4.2139e-01,\n",
       "            1.9482e+00, -1.1064e+00],\n",
       "          ...,\n",
       "          [ 1.1631e+00, -6.2354e-01, -7.8320e-01,  ..., -1.9775e+00,\n",
       "            1.5781e+00,  2.5269e-01],\n",
       "          [ 7.9346e-01, -8.4326e-01, -1.3135e+00,  ..., -4.7510e-01,\n",
       "            5.6348e-01, -2.3413e-01],\n",
       "          [ 3.8281e-01, -2.8003e-01, -5.4199e-01,  ...,  9.8047e-01,\n",
       "            8.3447e-01,  2.5439e-01]],\n",
       "\n",
       "         [[-2.5848e-02,  8.6975e-04, -2.3972e-02,  ...,  5.5566e-01,\n",
       "            3.4180e-01, -1.9019e-01],\n",
       "          [-5.5762e-01,  7.5000e-01,  4.1138e-01,  ...,  2.6348e+00,\n",
       "           -8.6572e-01, -6.6406e-01],\n",
       "          [ 5.7031e-01,  5.3809e-01,  1.2903e-01,  ..., -1.0244e+00,\n",
       "           -1.3398e+00, -6.0254e-01],\n",
       "          ...,\n",
       "          [ 8.2336e-02, -1.1353e-01,  3.6230e-01,  ..., -2.3164e+00,\n",
       "           -6.2891e-01,  5.0156e+00],\n",
       "          [ 1.1987e-01,  4.0137e-01,  2.6685e-01,  ..., -4.3672e+00,\n",
       "           -1.4014e+00,  1.9453e+00],\n",
       "          [-9.9182e-04,  1.8646e-02, -1.9873e-01,  ..., -3.4414e+00,\n",
       "           -3.0591e-01,  9.8486e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.9204e-03,  3.4637e-02,  3.2532e-02,  ...,  7.7515e-02,\n",
       "           -2.1387e-01,  2.9980e-01],\n",
       "          [ 5.6738e-01, -3.1934e-01,  6.6113e-01,  ..., -5.1709e-01,\n",
       "           -6.9824e-01,  9.8438e-01],\n",
       "          [-9.3018e-01, -5.0293e-01,  9.1846e-01,  ..., -2.9639e-01,\n",
       "            1.2168e+00,  3.0908e-01],\n",
       "          ...,\n",
       "          [-1.5088e-01, -6.0449e-01, -1.9336e-01,  ..., -1.3750e+00,\n",
       "            2.5410e+00, -7.4707e-01],\n",
       "          [ 8.5449e-01, -5.7422e-01,  9.6741e-02,  ..., -1.4102e+00,\n",
       "            2.5918e+00, -9.1797e-01],\n",
       "          [ 1.1250e+00, -1.2793e-01,  2.3010e-01,  ..., -1.4072e+00,\n",
       "            2.5312e+00, -8.4033e-01]],\n",
       "\n",
       "         [[ 3.6499e-02, -2.7802e-02, -2.6306e-02,  ..., -1.7627e+00,\n",
       "            2.8320e-01,  4.3335e-02],\n",
       "          [-1.2656e+00,  5.4834e-01,  1.0957e+00,  ...,  1.1543e+00,\n",
       "           -3.2764e-01,  2.8242e+00],\n",
       "          [ 1.1172e+00, -2.9492e-01, -5.0635e-01,  ...,  2.5059e+00,\n",
       "           -2.5293e-01, -7.2266e-02],\n",
       "          ...,\n",
       "          [ 1.0049e+00, -7.4219e-01, -8.9258e-01,  ...,  2.3828e+00,\n",
       "           -1.4990e-01, -1.2402e+00],\n",
       "          [-3.6523e-01, -8.9404e-01, -3.7256e-01,  ...,  2.5977e+00,\n",
       "           -1.8579e-01, -1.4893e+00],\n",
       "          [-1.4111e+00, -4.6338e-01,  3.0420e-01,  ...,  2.6523e+00,\n",
       "           -2.2339e-01, -1.4443e+00]],\n",
       "\n",
       "         [[-2.9297e-02,  5.1575e-03,  2.9724e-02,  ...,  1.8127e-02,\n",
       "            9.1846e-01,  3.7769e-01],\n",
       "          [ 5.3438e+00,  2.5703e+00, -8.0029e-01,  ...,  4.6533e-01,\n",
       "            1.0010e+00,  9.8486e-01],\n",
       "          [-3.0859e-01,  3.7969e+00,  5.6689e-01,  ...,  1.7737e-01,\n",
       "            1.4805e+00,  1.0840e+00],\n",
       "          ...,\n",
       "          [-4.7109e+00,  1.9990e+00,  2.6123e-01,  ...,  1.0364e-01,\n",
       "            1.7754e+00,  8.5498e-01],\n",
       "          [-8.6133e-01,  2.9414e+00, -4.1309e-01,  ...,  1.9263e-01,\n",
       "            1.7598e+00,  8.1738e-01],\n",
       "          [ 3.7305e+00,  2.0781e+00, -9.4238e-01,  ...,  1.8738e-01,\n",
       "            1.8574e+00,  7.2900e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.0942e-03, -1.2444e-02, -2.2827e-02,  ...,  3.0054e-01,\n",
       "           -9.2285e-02, -6.3623e-01],\n",
       "          [ 4.0381e-01,  3.0737e-01, -1.2158e-01,  ..., -5.9961e-01,\n",
       "           -4.7314e-01, -3.6387e+00],\n",
       "          [-7.9590e-01,  7.0605e-01, -1.0439e+00,  ..., -3.3789e-01,\n",
       "            2.3169e-01, -3.1094e+00],\n",
       "          ...,\n",
       "          [-9.4824e-01,  3.8647e-01,  4.2773e-01,  ..., -6.3858e-03,\n",
       "            1.7207e+00, -1.8555e+00],\n",
       "          [ 7.7209e-02,  5.8691e-01,  6.1475e-01,  ..., -2.4853e-03,\n",
       "            1.7666e+00, -1.9043e+00],\n",
       "          [ 9.7852e-01,  4.3408e-01,  5.6592e-01,  ..., -2.5436e-02,\n",
       "            1.8594e+00, -2.1348e+00]],\n",
       "\n",
       "         [[-1.1230e-02, -1.9958e-02, -8.5144e-03,  ..., -5.0098e-01,\n",
       "            1.6479e-01,  3.1738e-02],\n",
       "          [ 3.0640e-01,  6.7969e-01, -3.8330e-01,  ..., -1.9863e+00,\n",
       "           -4.4922e-01, -1.1353e-01],\n",
       "          [-2.0762e+00, -7.3120e-02, -1.6724e-01,  ...,  6.6602e-01,\n",
       "            2.0645e+00, -8.1836e-01],\n",
       "          ...,\n",
       "          [-2.9004e-01, -2.6343e-01, -3.4241e-02,  ...,  3.0273e-01,\n",
       "           -1.1841e-01, -1.1143e+00],\n",
       "          [ 4.7388e-01, -1.3586e-01, -1.3953e-01,  ...,  1.6272e-01,\n",
       "            2.2266e-01, -8.4814e-01],\n",
       "          [ 7.5830e-01,  2.1606e-02, -3.9673e-02,  ...,  5.6763e-02,\n",
       "            3.0444e-01, -7.5000e-01]],\n",
       "\n",
       "         [[-2.5848e-02,  8.6975e-04, -2.3972e-02,  ...,  5.5566e-01,\n",
       "            3.4180e-01, -1.9019e-01],\n",
       "          [-5.5762e-01,  7.5000e-01,  4.1138e-01,  ...,  2.6348e+00,\n",
       "           -8.6572e-01, -6.6406e-01],\n",
       "          [ 5.2197e-01,  6.9482e-01,  4.2041e-01,  ..., -5.2832e-01,\n",
       "           -9.2920e-01, -1.2520e+00],\n",
       "          ...,\n",
       "          [ 5.7861e-01,  8.3398e-01, -3.2764e-01,  ..., -5.1416e-01,\n",
       "           -7.4854e-01, -7.5928e-01],\n",
       "          [-2.3608e-01,  9.7119e-01,  1.6724e-01,  ..., -6.9678e-01,\n",
       "           -9.1357e-01, -7.1973e-01],\n",
       "          [-7.5146e-01,  4.8438e-01,  5.3467e-01,  ..., -7.7246e-01,\n",
       "           -8.4912e-01, -5.4736e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-3.2539e-03, -1.3245e-02,  1.1230e-02,  ...,  6.3400e-03,\n",
       "            4.2458e-03,  3.8223e-03],\n",
       "          [-6.7749e-02, -1.7822e-01, -2.4231e-01,  ...,  6.1707e-02,\n",
       "            1.0443e-01,  1.3440e-01],\n",
       "          [-3.3545e-01, -8.3191e-02,  9.4482e-02,  ..., -8.7036e-02,\n",
       "            1.9141e-01,  3.7628e-02],\n",
       "          ...,\n",
       "          [ 2.9126e-01,  9.6924e-02, -1.1310e-01,  ..., -1.0632e-01,\n",
       "           -1.0449e-01,  2.8687e-02],\n",
       "          [-4.4336e-01,  2.0630e-01, -7.1716e-02,  ..., -1.1169e-01,\n",
       "           -5.2765e-02,  7.1838e-02],\n",
       "          [-1.1584e-01,  2.4109e-01, -3.2080e-01,  ..., -1.4893e-02,\n",
       "           -1.3184e-01, -4.0820e-01]],\n",
       "\n",
       "         [[ 2.7542e-03,  6.3400e-03,  7.4692e-03,  ...,  6.8569e-04,\n",
       "           -2.8572e-03,  1.4084e-02],\n",
       "          [ 3.0960e-02, -2.1484e-01,  3.0054e-01,  ..., -7.5073e-03,\n",
       "            7.6172e-02, -3.0518e-02],\n",
       "          [ 2.7344e-02,  2.3572e-01, -1.0193e-01,  ...,  2.6093e-02,\n",
       "            5.1807e-01, -4.5624e-02],\n",
       "          ...,\n",
       "          [ 1.3000e-01,  1.2024e-01, -1.2109e-01,  ...,  1.0138e-01,\n",
       "            2.7637e-01, -6.3293e-02],\n",
       "          [ 2.5452e-02,  5.8105e-01, -1.9745e-02,  ..., -2.6562e-01,\n",
       "            4.6533e-01, -2.0844e-02],\n",
       "          [-8.9294e-02, -1.7896e-01, -1.4258e-01,  ..., -2.8833e-01,\n",
       "            4.5435e-01, -2.0004e-02]],\n",
       "\n",
       "         [[-8.1543e-02, -4.2847e-02, -8.1177e-03,  ..., -4.1229e-02,\n",
       "           -2.9678e-02, -3.2959e-02],\n",
       "          [ 6.6504e-01, -1.6882e-01, -4.7510e-01,  ...,  1.0858e-01,\n",
       "           -1.7615e-01,  1.1603e-01],\n",
       "          [-1.1887e-02,  1.8738e-01,  1.0400e-01,  ...,  3.6426e-01,\n",
       "           -2.4255e-01,  1.8933e-01],\n",
       "          ...,\n",
       "          [ 1.1115e-01, -1.6479e-01, -3.6426e-01,  ..., -1.2183e-01,\n",
       "           -2.9492e-01, -8.5742e-01],\n",
       "          [-1.6406e-01, -1.0962e-01,  6.8970e-03,  ..., -1.9263e-01,\n",
       "           -4.3677e-01, -6.9287e-01],\n",
       "          [ 2.7710e-01,  1.5991e-01,  1.3306e-01,  ...,  2.2607e-01,\n",
       "           -3.9990e-01,  2.2888e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.9775e-02,  3.0594e-03,  5.0774e-03,  ...,  9.2163e-03,\n",
       "            3.0518e-05, -1.5526e-02],\n",
       "          [ 2.1423e-01, -4.5459e-01,  5.5908e-01,  ...,  4.6967e-02,\n",
       "           -1.8384e-01, -5.1483e-02],\n",
       "          [-1.2097e-01,  1.4114e-02,  2.2278e-02,  ...,  6.0394e-02,\n",
       "            3.2129e-01,  1.4514e-01],\n",
       "          ...,\n",
       "          [ 9.4604e-02, -2.5000e-01,  1.7603e-01,  ...,  9.3933e-02,\n",
       "            1.1035e-01, -1.6968e-02],\n",
       "          [-2.8091e-02, -1.3708e-01, -3.2178e-01,  ...,  1.4490e-01,\n",
       "            7.4902e-01, -2.8174e-01],\n",
       "          [ 2.6886e-02, -2.5830e-01, -1.9531e-01,  ...,  1.4001e-01,\n",
       "            6.9641e-02, -7.1411e-02]],\n",
       "\n",
       "         [[-5.6190e-03, -1.3000e-02, -1.0712e-02,  ...,  1.2863e-02,\n",
       "            1.5602e-03,  1.7014e-02],\n",
       "          [-5.1392e-02, -1.9714e-01,  3.1348e-01,  ...,  3.1006e-01,\n",
       "           -4.5264e-01, -3.8574e-01],\n",
       "          [-3.0054e-01,  1.6040e-01,  3.4302e-02,  ...,  4.4336e-01,\n",
       "            5.2673e-02, -6.0010e-01],\n",
       "          ...,\n",
       "          [ 1.4783e-01,  2.1582e-01,  4.6240e-01,  ...,  6.9824e-02,\n",
       "           -1.8542e-01, -2.1069e-01],\n",
       "          [ 1.3940e-01,  1.7688e-01,  2.7417e-01,  ...,  2.4365e-01,\n",
       "            2.7771e-02,  4.2725e-04],\n",
       "          [ 2.8198e-01,  4.9536e-01,  9.4666e-02,  ...,  4.4653e-01,\n",
       "           -2.5620e-02, -1.0437e-01]],\n",
       "\n",
       "         [[ 4.2305e-03,  1.1887e-02, -6.0959e-03,  ...,  1.9043e-02,\n",
       "           -1.4816e-02,  5.5847e-02],\n",
       "          [ 4.1675e-01,  4.1382e-02, -5.1514e-02,  ...,  3.2422e-01,\n",
       "            3.3423e-01, -5.0244e-01],\n",
       "          [-3.9673e-03,  9.7351e-03,  3.8892e-01,  ..., -1.3123e-01,\n",
       "           -3.4229e-01, -2.3816e-01],\n",
       "          ...,\n",
       "          [-4.6997e-03,  5.8496e-01, -2.0654e-01,  ..., -6.0303e-02,\n",
       "            1.4771e-01,  4.9976e-01],\n",
       "          [ 3.9746e-01, -1.5088e-01, -3.7915e-01,  ..., -1.8250e-01,\n",
       "           -4.2603e-01,  2.8906e-01],\n",
       "          [-1.4014e-01,  4.8889e-02,  2.7661e-01,  ..., -1.2256e-01,\n",
       "           -2.9883e-01, -3.4180e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.2539e-03, -1.3245e-02,  1.1230e-02,  ...,  6.3400e-03,\n",
       "            4.2458e-03,  3.8223e-03],\n",
       "          [-6.7749e-02, -1.7822e-01, -2.4231e-01,  ...,  6.1707e-02,\n",
       "            1.0443e-01,  1.3440e-01],\n",
       "          [-4.6338e-01, -2.3535e-01,  1.2683e-01,  ..., -2.3486e-01,\n",
       "           -2.1149e-02,  2.1265e-01],\n",
       "          ...,\n",
       "          [ 1.3660e-01, -1.7236e-01, -3.9771e-01,  ..., -2.9999e-02,\n",
       "            1.1450e-01, -3.1891e-02],\n",
       "          [ 1.7981e-01, -1.1853e-01, -3.9722e-01,  ...,  1.3519e-02,\n",
       "            1.6699e-01, -6.6406e-02],\n",
       "          [ 1.9629e-01, -2.0935e-01, -3.9258e-01,  ...,  7.1228e-02,\n",
       "            1.7834e-01, -5.3375e-02]],\n",
       "\n",
       "         [[ 2.7542e-03,  6.3400e-03,  7.4692e-03,  ...,  6.8569e-04,\n",
       "           -2.8572e-03,  1.4084e-02],\n",
       "          [ 3.0960e-02, -2.1484e-01,  3.0054e-01,  ..., -7.5073e-03,\n",
       "            7.6172e-02, -3.0518e-02],\n",
       "          [ 1.9324e-01,  3.9795e-01,  7.8430e-02,  ..., -3.9612e-02,\n",
       "            4.9170e-01, -1.1823e-01],\n",
       "          ...,\n",
       "          [ 3.0298e-01,  4.2456e-01,  4.2084e-02,  ..., -1.8481e-01,\n",
       "            1.5906e-01, -1.3513e-01],\n",
       "          [ 2.6855e-01,  5.0195e-01,  7.4463e-03,  ..., -1.9263e-01,\n",
       "            1.4758e-01, -1.8250e-01],\n",
       "          [ 3.5352e-01,  4.4482e-01, -4.6783e-02,  ..., -2.3267e-01,\n",
       "            1.2103e-01, -1.9348e-01]],\n",
       "\n",
       "         [[-8.1543e-02, -4.2847e-02, -8.1177e-03,  ..., -4.1229e-02,\n",
       "           -2.9678e-02, -3.2959e-02],\n",
       "          [ 6.6504e-01, -1.6882e-01, -4.7510e-01,  ...,  1.0858e-01,\n",
       "           -1.7615e-01,  1.1603e-01],\n",
       "          [-2.7319e-01, -9.4299e-02,  7.7698e-02,  ...,  5.0977e-01,\n",
       "           -1.9543e-01,  2.8125e-01],\n",
       "          ...,\n",
       "          [ 1.6992e-01,  2.8610e-03, -8.6670e-02,  ..., -2.1350e-01,\n",
       "           -2.1942e-02,  2.4524e-01],\n",
       "          [ 1.6748e-01,  1.3077e-02, -1.3817e-02,  ..., -2.3389e-01,\n",
       "           -2.8397e-02,  2.6123e-01],\n",
       "          [ 1.7981e-01,  4.6631e-02, -1.1436e-02,  ..., -2.0349e-01,\n",
       "           -3.9612e-02,  2.5366e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.9775e-02,  3.0594e-03,  5.0774e-03,  ...,  9.2163e-03,\n",
       "            3.0518e-05, -1.5526e-02],\n",
       "          [ 2.1423e-01, -4.5459e-01,  5.5908e-01,  ...,  4.6967e-02,\n",
       "           -1.8384e-01, -5.1483e-02],\n",
       "          [ 1.9958e-02,  1.8933e-01,  3.1128e-03,  ...,  9.2163e-02,\n",
       "            1.0150e-01,  1.9092e-01],\n",
       "          ...,\n",
       "          [ 3.6938e-01, -3.5286e-03, -1.0999e-01,  ...,  9.7168e-02,\n",
       "            7.0251e-02,  2.0972e-01],\n",
       "          [ 3.9331e-01, -8.8562e-02, -1.3940e-01,  ...,  7.4463e-02,\n",
       "            1.4648e-01,  1.6431e-01],\n",
       "          [ 3.8721e-01, -2.7496e-02, -1.2805e-01,  ...,  5.8411e-02,\n",
       "            1.0492e-01,  1.0681e-01]],\n",
       "\n",
       "         [[-5.6190e-03, -1.3000e-02, -1.0712e-02,  ...,  1.2863e-02,\n",
       "            1.5602e-03,  1.7014e-02],\n",
       "          [-5.1392e-02, -1.9714e-01,  3.1348e-01,  ...,  3.1006e-01,\n",
       "           -4.5264e-01, -3.8574e-01],\n",
       "          [-2.4207e-01,  1.9470e-01, -2.9688e-01,  ...,  3.8086e-01,\n",
       "           -2.8174e-01, -2.3120e-01],\n",
       "          ...,\n",
       "          [-9.3079e-02,  8.7463e-02,  4.2969e-01,  ...,  1.8140e-01,\n",
       "           -4.2334e-01, -2.7783e-01],\n",
       "          [-1.5698e-01,  6.9275e-02,  5.1270e-01,  ...,  2.0178e-01,\n",
       "           -4.5459e-01, -2.8540e-01],\n",
       "          [-1.8091e-01, -6.1676e-02,  4.8315e-01,  ...,  2.1045e-01,\n",
       "           -4.7070e-01, -1.9702e-01]],\n",
       "\n",
       "         [[ 4.2305e-03,  1.1887e-02, -6.0959e-03,  ...,  1.9043e-02,\n",
       "           -1.4816e-02,  5.5847e-02],\n",
       "          [ 4.1675e-01,  4.1382e-02, -5.1514e-02,  ...,  3.2422e-01,\n",
       "            3.3423e-01, -5.0244e-01],\n",
       "          [-3.3600e-02,  4.1077e-02,  4.2725e-02,  ...,  7.9346e-04,\n",
       "           -3.1592e-01, -3.3008e-01],\n",
       "          ...,\n",
       "          [-1.1337e-02,  7.9163e-02,  5.2881e-01,  ..., -1.2030e-01,\n",
       "           -2.1655e-01, -1.6870e-01],\n",
       "          [-4.8866e-03,  9.4299e-02,  5.1416e-01,  ..., -1.1133e-01,\n",
       "           -1.7004e-01, -1.3367e-01],\n",
       "          [-1.1765e-02,  2.7283e-02,  4.9707e-01,  ..., -8.1055e-02,\n",
       "           -2.2571e-01, -1.3538e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 7.3242e-04, -3.5400e-03,  1.9318e-02,  ...,  5.3809e-01,\n",
       "           -6.4941e-01, -1.1816e-01],\n",
       "          [-1.2461e+00, -6.2305e-01, -1.1592e+00,  ..., -2.4492e+00,\n",
       "            5.8154e-01,  9.5898e-01],\n",
       "          [ 1.0791e-01,  8.9307e-01,  7.4365e-01,  ..., -2.0508e+00,\n",
       "            1.4209e+00,  7.6221e-01],\n",
       "          ...,\n",
       "          [ 1.3965e-01, -1.9121e+00, -8.3545e-01,  ..., -4.0703e+00,\n",
       "            1.3457e+00,  3.4316e+00],\n",
       "          [-7.4805e-01,  1.4648e+00, -9.8096e-01,  ..., -4.1211e+00,\n",
       "            2.6221e-01,  3.5977e+00],\n",
       "          [-1.7979e+00,  1.6270e+00, -1.1719e+00,  ..., -4.2656e+00,\n",
       "           -2.4329e-01,  4.5781e+00]],\n",
       "\n",
       "         [[-2.1973e-02, -1.0010e-02,  1.8921e-03,  ..., -6.2891e-01,\n",
       "            6.5527e-01,  9.8340e-01],\n",
       "          [ 8.1836e-01, -1.8274e-01, -6.0205e-01,  ...,  1.6045e+00,\n",
       "           -4.1748e-02,  4.7607e-02],\n",
       "          [-1.0625e+00, -4.3823e-01, -8.0664e-01,  ...,  2.8418e+00,\n",
       "           -1.0928e+00,  9.0576e-01],\n",
       "          ...,\n",
       "          [-6.2549e-01, -1.3672e-02,  4.8462e-01,  ...,  2.0605e+00,\n",
       "            6.9727e-01,  1.4561e+00],\n",
       "          [-5.3467e-01, -9.6973e-01,  7.1240e-01,  ...,  4.0352e+00,\n",
       "            9.7656e-01,  1.8828e+00],\n",
       "          [ 8.8916e-01, -3.7231e-01,  4.3915e-02,  ...,  3.3086e+00,\n",
       "            1.4238e+00,  3.1494e-01]],\n",
       "\n",
       "         [[-6.6833e-03, -8.6975e-03,  2.0447e-03,  ..., -3.1494e-01,\n",
       "            1.2634e-01,  2.0459e-01],\n",
       "          [ 1.2266e+00,  6.8750e-01, -7.2363e-01,  ..., -2.2402e+00,\n",
       "           -5.4395e-01, -2.5684e-01],\n",
       "          [ 1.6670e+00,  2.0264e-01, -1.0283e+00,  ..., -2.6367e+00,\n",
       "            2.8516e-01,  2.9126e-01],\n",
       "          ...,\n",
       "          [-1.0283e+00,  9.9268e-01,  1.7500e+00,  ..., -2.0520e-01,\n",
       "           -3.7256e-01, -2.7637e-01],\n",
       "          [-8.6621e-01, -1.4685e-01,  8.8916e-01,  ..., -7.1094e-01,\n",
       "            4.4946e-01, -7.0947e-01],\n",
       "          [-3.7012e-01, -5.3662e-01,  2.9883e-01,  ..., -1.2715e+00,\n",
       "           -1.1597e-03,  1.4014e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.6245e-03, -2.1912e-02, -4.4373e-02,  ..., -2.2717e-01,\n",
       "            2.5635e-03,  4.7363e-01],\n",
       "          [-3.7891e-01,  5.7617e-01,  9.3262e-02,  ...,  5.0586e-01,\n",
       "            9.0723e-01, -3.2305e+00],\n",
       "          [-2.0918e+00,  4.2993e-01,  9.2163e-02,  ...,  9.6777e-01,\n",
       "            9.3018e-02, -1.8984e+00],\n",
       "          ...,\n",
       "          [ 1.3145e+00,  2.0752e-01, -9.8633e-01,  ..., -2.5293e+00,\n",
       "           -5.0000e-01, -1.3926e+00],\n",
       "          [ 1.1416e+00, -1.1877e-01, -2.2339e-01,  ...,  3.3960e-01,\n",
       "           -6.1670e-01, -1.5918e+00],\n",
       "          [ 3.7549e-01, -4.0845e-01, -2.5391e-01,  ...,  6.8604e-01,\n",
       "           -2.5903e-01, -2.3105e+00]],\n",
       "\n",
       "         [[-2.3804e-03,  6.1951e-03,  8.4229e-03,  ..., -8.1482e-02,\n",
       "            3.6377e-01, -2.4048e-01],\n",
       "          [-1.6821e-01,  3.2080e-01, -1.8372e-01,  ...,  2.7271e-01,\n",
       "            4.7852e-02, -6.4502e-01],\n",
       "          [-1.3604e+00, -2.4890e-01, -7.6953e-01,  ...,  7.0166e-01,\n",
       "            1.0146e+00, -1.8555e-01],\n",
       "          ...,\n",
       "          [ 1.0137e+00, -8.9062e-01,  8.3008e-01,  ..., -3.7969e+00,\n",
       "            2.8833e-01, -2.2852e-01],\n",
       "          [ 1.3633e+00, -7.9736e-01,  1.2188e+00,  ..., -2.0195e+00,\n",
       "            1.0654e+00,  1.6760e-01],\n",
       "          [ 5.3613e-01, -3.7061e-01,  9.8633e-02,  ..., -1.4268e+00,\n",
       "            1.3711e+00,  8.5938e-01]],\n",
       "\n",
       "         [[-3.2837e-02,  2.0004e-02,  2.2583e-02,  ..., -6.9824e-01,\n",
       "            2.4634e-01, -1.9446e-01],\n",
       "          [ 1.8525e+00, -4.0723e-01, -1.6123e+00,  ...,  1.1094e+00,\n",
       "           -6.0156e-01,  1.1670e-01],\n",
       "          [-9.3896e-01,  1.4492e+00, -8.1348e-01,  ...,  1.6387e+00,\n",
       "           -1.8662e+00,  2.3203e+00],\n",
       "          ...,\n",
       "          [-1.3896e+00, -7.6709e-01,  2.5879e-01,  ...,  5.4258e+00,\n",
       "           -4.0942e-01,  2.3301e+00],\n",
       "          [-1.1855e+00, -5.4639e-01,  8.9355e-01,  ...,  4.2539e+00,\n",
       "           -1.7383e-01,  3.7539e+00],\n",
       "          [ 1.5439e+00,  7.8906e-01, -9.9805e-01,  ...,  2.8672e+00,\n",
       "            1.0687e-01,  4.9805e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 7.3242e-04, -3.5400e-03,  1.9318e-02,  ...,  5.3809e-01,\n",
       "           -6.4941e-01, -1.1816e-01],\n",
       "          [-1.2461e+00, -6.2305e-01, -1.1592e+00,  ..., -2.4492e+00,\n",
       "            5.8154e-01,  9.5898e-01],\n",
       "          [-1.4648e-01,  1.1670e+00,  7.9932e-01,  ..., -1.9492e+00,\n",
       "            1.0449e+00,  4.2017e-01],\n",
       "          ...,\n",
       "          [ 1.1797e+00, -4.6436e-01,  8.1982e-01,  ..., -1.6855e+00,\n",
       "            6.3574e-01,  2.8516e+00],\n",
       "          [ 5.6348e-01,  8.0469e-01, -3.4814e-01,  ..., -1.7168e+00,\n",
       "            6.0449e-01,  2.9805e+00],\n",
       "          [-4.4971e-01,  1.5625e+00, -1.3506e+00,  ..., -1.6816e+00,\n",
       "            4.8999e-01,  3.0527e+00]],\n",
       "\n",
       "         [[-2.1973e-02, -1.0010e-02,  1.8921e-03,  ..., -6.2891e-01,\n",
       "            6.5527e-01,  9.8340e-01],\n",
       "          [ 8.1836e-01, -1.8274e-01, -6.0205e-01,  ...,  1.6045e+00,\n",
       "           -4.1748e-02,  4.7607e-02],\n",
       "          [-1.1006e+00, -6.3232e-01, -7.4707e-01,  ...,  2.4434e+00,\n",
       "           -9.6680e-01,  7.5293e-01],\n",
       "          ...,\n",
       "          [-4.3555e-01,  6.3965e-01,  5.7959e-01,  ...,  2.8867e+00,\n",
       "            3.7500e-01,  1.0088e+00],\n",
       "          [-6.7822e-01,  5.1660e-01,  6.6992e-01,  ...,  2.7930e+00,\n",
       "            2.4475e-01,  9.0332e-01],\n",
       "          [-3.5229e-01,  1.2866e-01,  4.3921e-01,  ...,  2.8301e+00,\n",
       "            4.2554e-01,  1.0312e+00]],\n",
       "\n",
       "         [[-6.6833e-03, -8.6975e-03,  2.0447e-03,  ..., -3.1494e-01,\n",
       "            1.2634e-01,  2.0459e-01],\n",
       "          [ 1.2266e+00,  6.8750e-01, -7.2363e-01,  ..., -2.2402e+00,\n",
       "           -5.4395e-01, -2.5684e-01],\n",
       "          [ 1.6885e+00,  5.6335e-02, -1.0527e+00,  ..., -2.4043e+00,\n",
       "            4.3701e-02,  2.6660e-01],\n",
       "          ...,\n",
       "          [-2.6831e-01,  3.3667e-01,  2.5244e-01,  ..., -5.9961e-01,\n",
       "            5.4297e-01,  7.2119e-01],\n",
       "          [-1.0469e+00, -5.1758e-01,  3.0859e-01,  ..., -5.1270e-01,\n",
       "            5.4639e-01,  7.2510e-01],\n",
       "          [-9.5215e-01, -1.1670e+00,  2.4512e-01,  ..., -5.6689e-01,\n",
       "            6.5283e-01,  7.9443e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.6245e-03, -2.1912e-02, -4.4373e-02,  ..., -2.2717e-01,\n",
       "            2.5635e-03,  4.7363e-01],\n",
       "          [-3.7891e-01,  5.7617e-01,  9.3262e-02,  ...,  5.0586e-01,\n",
       "            9.0723e-01, -3.2305e+00],\n",
       "          [-2.4492e+00,  6.5088e-01,  2.0044e-01,  ...,  7.9688e-01,\n",
       "            5.4639e-01, -2.7305e+00],\n",
       "          ...,\n",
       "          [ 1.0664e+00,  1.5137e+00, -4.8657e-01,  ...,  1.2227e+00,\n",
       "            2.6709e-01,  3.9032e-02],\n",
       "          [ 1.8418e+00,  5.9570e-01, -4.7583e-01,  ...,  1.1680e+00,\n",
       "           -1.0229e-01, -1.4795e-01],\n",
       "          [ 9.2432e-01, -6.7480e-01, -3.1689e-01,  ...,  1.0635e+00,\n",
       "            3.3203e-02,  5.7587e-02]],\n",
       "\n",
       "         [[-2.3804e-03,  6.1951e-03,  8.4229e-03,  ..., -8.1482e-02,\n",
       "            3.6377e-01, -2.4048e-01],\n",
       "          [-1.6821e-01,  3.2080e-01, -1.8372e-01,  ...,  2.7271e-01,\n",
       "            4.7852e-02, -6.4502e-01],\n",
       "          [-1.6191e+00, -2.2437e-01, -8.3301e-01,  ...,  1.0225e+00,\n",
       "            7.4316e-01,  6.5430e-02],\n",
       "          ...,\n",
       "          [ 8.4180e-01, -1.5515e-01,  1.3418e+00,  ..., -4.5581e-01,\n",
       "            6.5479e-01, -1.5498e+00],\n",
       "          [ 1.7168e+00, -1.2732e-01,  3.1299e-01,  ..., -4.7583e-01,\n",
       "            8.3643e-01, -1.3301e+00],\n",
       "          [ 9.4629e-01, -1.4893e-01, -8.1689e-01,  ..., -4.9268e-01,\n",
       "            7.4121e-01, -1.3535e+00]],\n",
       "\n",
       "         [[-3.2837e-02,  2.0004e-02,  2.2583e-02,  ..., -6.9824e-01,\n",
       "            2.4634e-01, -1.9446e-01],\n",
       "          [ 1.8525e+00, -4.0723e-01, -1.6123e+00,  ...,  1.1094e+00,\n",
       "           -6.0156e-01,  1.1670e-01],\n",
       "          [-9.9902e-01,  1.5381e+00, -7.7197e-01,  ...,  1.6504e+00,\n",
       "           -1.5312e+00,  2.0703e+00],\n",
       "          ...,\n",
       "          [-1.7373e+00, -1.3252e+00,  1.3809e+00,  ...,  2.7676e+00,\n",
       "           -1.4392e-01,  1.5762e+00],\n",
       "          [-7.2363e-01,  3.2654e-03,  6.5625e-01,  ...,  2.8750e+00,\n",
       "           -3.1952e-02,  1.3779e+00],\n",
       "          [ 1.0664e+00,  1.2256e+00, -4.8608e-01,  ...,  2.9102e+00,\n",
       "            1.1456e-01,  1.4004e+00]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-1.1520e-03,  4.6387e-03, -2.7847e-03,  ...,  3.7060e-03,\n",
       "            8.3542e-04,  2.5120e-03],\n",
       "          [ 2.3633e-01, -1.8433e-01, -6.4990e-01,  ..., -1.6162e-01,\n",
       "           -4.6436e-01,  3.6597e-01],\n",
       "          [-2.3767e-01, -3.3203e-01,  5.5469e-01,  ...,  8.9905e-02,\n",
       "            1.3831e-01, -1.2744e-01],\n",
       "          ...,\n",
       "          [-2.6538e-01,  4.8035e-02,  1.5039e-01,  ..., -1.0602e-01,\n",
       "           -3.0811e-01, -1.9604e-01],\n",
       "          [ 2.0898e-01,  2.5269e-01,  8.9294e-02,  ..., -3.9307e-02,\n",
       "           -8.7219e-02, -6.1493e-02],\n",
       "          [ 2.2278e-01,  7.5537e-01, -3.3643e-01,  ..., -2.9419e-01,\n",
       "            2.6221e-01,  3.4375e-01]],\n",
       "\n",
       "         [[-9.6130e-03, -2.7561e-03, -8.0109e-03,  ..., -1.3405e-02,\n",
       "           -2.9411e-03, -3.6560e-02],\n",
       "          [-1.1670e-01, -1.2903e-01,  1.7517e-01,  ...,  1.6943e-01,\n",
       "           -7.3730e-02, -1.3403e-01],\n",
       "          [ 2.9517e-01,  9.5703e-02, -2.0422e-01,  ...,  4.3701e-02,\n",
       "           -3.1934e-01, -2.1497e-01],\n",
       "          ...,\n",
       "          [-7.3547e-02, -6.0730e-02,  1.6937e-02,  ..., -3.6719e-01,\n",
       "            5.1941e-02,  2.6050e-01],\n",
       "          [-7.2754e-02, -5.7739e-02, -6.9580e-02,  ..., -5.2197e-01,\n",
       "            4.0741e-02,  4.6680e-01],\n",
       "          [ 1.2323e-01, -4.4617e-02,  1.8054e-01,  ..., -5.6152e-03,\n",
       "           -9.7656e-04,  1.1490e-02]],\n",
       "\n",
       "         [[-1.4626e-02, -9.4757e-03, -9.8038e-03,  ...,  1.7725e-01,\n",
       "            1.1116e-02,  8.6975e-04],\n",
       "          [-2.3352e-01, -2.6343e-01, -2.9028e-01,  ..., -2.6709e-01,\n",
       "           -1.9031e-01, -1.0718e-01],\n",
       "          [ 1.7017e-01,  4.5990e-02,  4.0283e-03,  ...,  7.0374e-02,\n",
       "           -1.3184e-01, -2.2046e-01],\n",
       "          ...,\n",
       "          [ 1.0864e-01, -8.2321e-03, -5.1636e-02,  ..., -3.0615e-01,\n",
       "            1.4270e-01, -3.2910e-01],\n",
       "          [-7.5562e-02,  2.2412e-01,  3.1909e-01,  ...,  7.3730e-01,\n",
       "           -1.9531e-01, -4.6606e-01],\n",
       "          [-9.5947e-02,  1.5649e-01,  6.4575e-02,  ...,  1.1163e-01,\n",
       "           -4.1821e-01,  1.7822e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.7204e-03, -1.2505e-02, -2.1622e-02,  ..., -1.4099e-02,\n",
       "           -7.7553e-03,  6.5002e-03],\n",
       "          [ 1.1328e-01,  5.2588e-01,  2.8003e-01,  ...,  5.9961e-01,\n",
       "           -3.5461e-02,  1.8542e-01],\n",
       "          [-1.6833e-01, -5.8105e-02, -7.6599e-03,  ...,  2.1179e-01,\n",
       "            2.2131e-01, -1.7285e-01],\n",
       "          ...,\n",
       "          [-1.3562e-01,  6.1340e-02,  1.8530e-01,  ..., -1.4465e-01,\n",
       "           -4.6216e-01, -1.9119e-02],\n",
       "          [ 6.3293e-02, -1.8372e-01, -1.0840e-01,  ...,  7.5623e-02,\n",
       "           -1.5674e-01,  2.8467e-01],\n",
       "          [-1.2524e-01, -3.9966e-01, -3.1445e-01,  ...,  1.6602e-01,\n",
       "            1.3062e-01,  7.0251e-02]],\n",
       "\n",
       "         [[ 5.3635e-03,  2.2888e-05,  6.3972e-03,  ..., -3.4821e-02,\n",
       "           -2.6535e-02, -1.7181e-02],\n",
       "          [ 1.6510e-02, -2.8662e-01, -8.8867e-02,  ...,  1.0205e-01,\n",
       "            6.1157e-02, -2.6709e-01],\n",
       "          [ 9.1492e-02, -9.4238e-02, -8.2031e-02,  ...,  9.9487e-02,\n",
       "           -2.8534e-02, -3.6926e-03],\n",
       "          ...,\n",
       "          [-1.0574e-02,  1.1658e-02, -5.9753e-02,  ..., -2.0557e-01,\n",
       "            1.9678e-01, -3.6938e-01],\n",
       "          [-2.5732e-01,  6.2744e-02, -3.3643e-01,  ...,  4.3091e-01,\n",
       "           -6.8848e-02, -3.1812e-01],\n",
       "          [-1.5881e-01,  2.3413e-01, -7.2571e-02,  ...,  8.2715e-01,\n",
       "            1.2036e-01, -3.8281e-01]],\n",
       "\n",
       "         [[ 2.5635e-03,  1.3809e-03, -1.0376e-02,  ..., -6.2523e-03,\n",
       "            2.0485e-03,  1.4633e-02],\n",
       "          [ 3.1543e-01, -4.9438e-02,  1.8951e-02,  ...,  1.6113e-01,\n",
       "            1.5900e-02,  4.6582e-01],\n",
       "          [ 3.3295e-02, -4.0137e-01, -2.6001e-01,  ...,  2.0850e-01,\n",
       "           -4.2310e-01, -3.6572e-01],\n",
       "          ...,\n",
       "          [ 5.2948e-02,  2.7246e-01,  5.0098e-01,  ...,  1.4819e-01,\n",
       "            7.5195e-02, -6.4258e-01],\n",
       "          [ 1.5356e-01,  2.0532e-01,  1.9189e-01,  ...,  3.1860e-01,\n",
       "            4.8157e-02, -1.0211e-01],\n",
       "          [ 1.0339e-01, -3.7671e-01,  1.6272e-01,  ...,  6.1621e-01,\n",
       "            2.0898e-01,  1.4941e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.1520e-03,  4.6387e-03, -2.7847e-03,  ...,  3.7060e-03,\n",
       "            8.3542e-04,  2.5120e-03],\n",
       "          [ 2.3633e-01, -1.8433e-01, -6.4990e-01,  ..., -1.6162e-01,\n",
       "           -4.6436e-01,  3.6597e-01],\n",
       "          [ 2.1509e-01,  5.6787e-01, -9.9548e-02,  ...,  2.4207e-01,\n",
       "           -5.9906e-02, -2.7588e-01],\n",
       "          ...,\n",
       "          [ 1.8750e-01,  5.5078e-01, -2.4597e-01,  ..., -3.2080e-01,\n",
       "            4.1211e-01, -7.3303e-02],\n",
       "          [ 1.5417e-01,  6.4258e-01, -2.8491e-01,  ..., -3.1299e-01,\n",
       "            4.7803e-01, -4.2206e-02],\n",
       "          [ 1.9153e-01,  7.0996e-01, -2.6147e-01,  ..., -3.0786e-01,\n",
       "            4.5996e-01,  9.5978e-03]],\n",
       "\n",
       "         [[-9.6130e-03, -2.7561e-03, -8.0109e-03,  ..., -1.3405e-02,\n",
       "           -2.9411e-03, -3.6560e-02],\n",
       "          [-1.1670e-01, -1.2903e-01,  1.7517e-01,  ...,  1.6943e-01,\n",
       "           -7.3730e-02, -1.3403e-01],\n",
       "          [ 2.8491e-01,  1.6998e-02, -2.1814e-01,  ...,  6.8726e-02,\n",
       "           -2.3401e-01, -1.7529e-01],\n",
       "          ...,\n",
       "          [ 1.0956e-01,  3.0106e-02, -1.1681e-02,  ..., -2.3755e-01,\n",
       "           -6.1462e-02, -2.1149e-02],\n",
       "          [ 9.3079e-02,  5.5847e-03, -1.3901e-02,  ..., -2.7686e-01,\n",
       "           -8.3679e-02, -2.0142e-02],\n",
       "          [ 6.3477e-02,  2.3651e-02, -6.5002e-02,  ..., -2.9395e-01,\n",
       "           -9.7595e-02, -1.0841e-02]],\n",
       "\n",
       "         [[-1.4626e-02, -9.4757e-03, -9.8038e-03,  ...,  1.7725e-01,\n",
       "            1.1116e-02,  8.6975e-04],\n",
       "          [-2.3352e-01, -2.6343e-01, -2.9028e-01,  ..., -2.6709e-01,\n",
       "           -1.9031e-01, -1.0718e-01],\n",
       "          [ 2.5659e-01, -1.6357e-01, -8.2153e-02,  ..., -2.7344e-02,\n",
       "           -7.7454e-02, -7.5989e-02],\n",
       "          ...,\n",
       "          [ 6.5613e-03,  3.3643e-01, -2.1289e-01,  ...,  6.6504e-01,\n",
       "           -2.8833e-01,  5.9784e-02],\n",
       "          [ 1.9058e-02,  2.8857e-01, -2.3987e-01,  ...,  6.4209e-01,\n",
       "           -3.3203e-01,  2.3804e-02],\n",
       "          [ 2.0493e-02,  2.6416e-01, -1.8542e-01,  ...,  6.2451e-01,\n",
       "           -3.8330e-01,  4.7729e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.7204e-03, -1.2505e-02, -2.1622e-02,  ..., -1.4099e-02,\n",
       "           -7.7553e-03,  6.5002e-03],\n",
       "          [ 1.1328e-01,  5.2588e-01,  2.8003e-01,  ...,  5.9961e-01,\n",
       "           -3.5461e-02,  1.8542e-01],\n",
       "          [-2.3840e-01,  9.0088e-02,  1.0559e-01,  ...,  5.5908e-02,\n",
       "           -4.0253e-02,  1.6492e-01],\n",
       "          ...,\n",
       "          [ 1.2283e-02, -5.0049e-03,  9.8999e-02,  ...,  2.8735e-01,\n",
       "           -2.9053e-01,  6.6223e-03],\n",
       "          [-5.4016e-03, -1.1560e-01,  1.0718e-01,  ...,  3.1348e-01,\n",
       "           -3.4644e-01, -5.8899e-03],\n",
       "          [ 2.2873e-02, -1.0193e-01,  1.0419e-01,  ...,  3.0249e-01,\n",
       "           -3.5791e-01, -1.3428e-03]],\n",
       "\n",
       "         [[ 5.3635e-03,  2.2888e-05,  6.3972e-03,  ..., -3.4821e-02,\n",
       "           -2.6535e-02, -1.7181e-02],\n",
       "          [ 1.6510e-02, -2.8662e-01, -8.8867e-02,  ...,  1.0205e-01,\n",
       "            6.1157e-02, -2.6709e-01],\n",
       "          [ 2.0966e-02, -2.5879e-01, -2.6001e-01,  ..., -3.1665e-01,\n",
       "            2.0972e-01, -8.5999e-02],\n",
       "          ...,\n",
       "          [-3.9551e-02, -1.3037e-01, -2.4414e-01,  ..., -9.5947e-02,\n",
       "           -1.3843e-01,  1.7883e-02],\n",
       "          [-1.1060e-01, -1.1835e-01, -2.9102e-01,  ..., -1.4990e-01,\n",
       "           -8.5693e-02,  6.1096e-02],\n",
       "          [-6.1981e-02, -7.9346e-02, -2.6440e-01,  ..., -1.2891e-01,\n",
       "           -9.2407e-02,  7.4585e-02]],\n",
       "\n",
       "         [[ 2.5635e-03,  1.3809e-03, -1.0376e-02,  ..., -6.2523e-03,\n",
       "            2.0485e-03,  1.4633e-02],\n",
       "          [ 3.1543e-01, -4.9438e-02,  1.8951e-02,  ...,  1.6113e-01,\n",
       "            1.5900e-02,  4.6582e-01],\n",
       "          [ 2.4097e-01, -7.0557e-02, -3.1763e-01,  ...,  4.9133e-02,\n",
       "           -1.3965e-01,  7.3914e-02],\n",
       "          ...,\n",
       "          [ 2.4805e-01, -7.0312e-02,  7.2168e-01,  ...,  3.3228e-01,\n",
       "           -9.5825e-02, -4.0869e-01],\n",
       "          [ 2.4634e-01, -5.3040e-02,  7.0557e-01,  ...,  3.4961e-01,\n",
       "           -1.1407e-01, -3.8135e-01],\n",
       "          [ 2.5513e-01, -1.5601e-01,  7.9102e-01,  ...,  3.5547e-01,\n",
       "           -1.2939e-01, -3.9746e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-9.8877e-03, -1.2085e-02,  1.2390e-02,  ..., -2.2217e-01,\n",
       "            9.0454e-02, -4.3604e-01],\n",
       "          [ 1.4316e+00,  5.4639e-01,  2.7832e-01,  ...,  1.0137e+00,\n",
       "           -1.6528e-01,  2.1211e+00],\n",
       "          [ 4.0430e-01, -1.3184e+00, -8.5693e-01,  ...,  2.0508e-02,\n",
       "           -9.1260e-01,  3.7036e-01],\n",
       "          ...,\n",
       "          [-1.2207e+00,  8.8928e-02,  2.5488e-01,  ...,  4.0381e-01,\n",
       "           -1.4971e+00,  7.6318e-01],\n",
       "          [ 3.2251e-01,  4.7363e-01,  1.1426e+00,  ..., -2.0386e-01,\n",
       "           -6.9434e-01, -1.3418e+00],\n",
       "          [-4.5410e-02,  7.2168e-01,  8.9941e-01,  ...,  8.5303e-01,\n",
       "           -2.5332e+00, -2.0586e+00]],\n",
       "\n",
       "         [[ 1.1108e-02, -2.3926e-02, -2.9907e-03,  ...,  7.2815e-02,\n",
       "           -1.6309e+00,  2.8516e-01],\n",
       "          [ 2.0625e+00,  1.5303e+00, -1.1416e+00,  ..., -2.2144e-01,\n",
       "            4.0469e+00,  3.6157e-01],\n",
       "          [ 3.1934e+00, -4.5923e-01, -5.4297e-01,  ..., -6.5674e-01,\n",
       "            3.6348e+00,  2.4170e-02],\n",
       "          ...,\n",
       "          [-1.1787e+00,  5.1953e-01,  1.2236e+00,  ...,  7.4072e-01,\n",
       "            2.7266e+00,  4.7119e-01],\n",
       "          [-1.7607e+00, -5.3906e-01,  1.8857e+00,  ...,  2.1692e-01,\n",
       "            4.7227e+00, -8.0029e-01],\n",
       "          [-1.3398e+00, -2.0215e+00,  4.3359e-01,  ...,  6.2793e-01,\n",
       "            3.6680e+00,  3.2715e-01]],\n",
       "\n",
       "         [[ 9.1553e-03,  1.7700e-03,  8.7433e-03,  ...,  4.3518e-02,\n",
       "           -2.8198e-01, -3.9502e-01],\n",
       "          [-2.0781e+00, -1.1426e+00,  2.3560e-01,  ...,  3.2471e-02,\n",
       "           -8.2227e-01, -1.2295e+00],\n",
       "          [-3.3447e-01, -1.1309e+00,  2.8125e-01,  ...,  1.4346e+00,\n",
       "           -2.6230e+00, -8.3105e-01],\n",
       "          ...,\n",
       "          [ 2.8906e-01, -1.3652e+00, -5.3369e-01,  ..., -8.6670e-01,\n",
       "           -4.4629e-01,  6.6309e-01],\n",
       "          [ 7.4902e-01, -1.7793e+00,  3.5913e-01,  ..., -2.5195e-01,\n",
       "            1.8809e+00,  1.7773e-01],\n",
       "          [-9.1504e-01, -1.4121e+00, -7.0557e-01,  ..., -6.2891e-01,\n",
       "            5.8691e-01,  7.6318e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.4038e-02,  5.0354e-03, -6.9885e-03,  ..., -6.0645e-01,\n",
       "            2.1660e+00, -1.1810e-01],\n",
       "          [-9.2871e-01,  6.4209e-02,  4.5850e-01,  ..., -1.3818e+00,\n",
       "           -3.4805e+00,  1.0859e+00],\n",
       "          [ 1.8262e+00,  1.0566e+00, -6.6309e-01,  ..., -1.1084e+00,\n",
       "           -2.9355e+00,  2.3096e-01],\n",
       "          ...,\n",
       "          [ 3.6792e-01,  1.2246e+00, -3.1006e-01,  ...,  5.9180e-01,\n",
       "           -2.7227e+00, -2.3743e-01],\n",
       "          [-5.0201e-02,  1.0391e+00,  8.4045e-02,  ..., -2.0410e+00,\n",
       "           -4.0820e+00, -1.4023e+00],\n",
       "          [-2.8809e-01,  1.6582e+00,  5.0098e-01,  ..., -2.5742e+00,\n",
       "           -4.4922e+00, -1.1396e+00]],\n",
       "\n",
       "         [[ 1.4626e-02, -1.8799e-02,  2.4200e-02,  ..., -9.9805e-01,\n",
       "           -1.7041e+00,  6.1572e-01],\n",
       "          [-7.8003e-02,  1.5405e-01,  1.4795e-01,  ...,  1.3047e+00,\n",
       "            4.5820e+00,  3.5156e-02],\n",
       "          [-3.7872e-02, -1.4246e-01,  1.2952e-01,  ...,  8.0664e-01,\n",
       "            3.6523e+00,  1.4727e+00],\n",
       "          ...,\n",
       "          [-2.1194e-02, -5.6299e-01, -4.8737e-02,  ...,  1.1982e+00,\n",
       "            7.3789e+00, -4.3984e+00],\n",
       "          [ 3.6230e-01, -4.1650e-01, -2.7441e-01,  ...,  3.5703e+00,\n",
       "            4.6406e+00, -2.0117e+00],\n",
       "          [-1.8628e-01,  4.3030e-02,  1.2439e-01,  ...,  5.3125e+00,\n",
       "            1.5156e+00,  3.6224e-02]],\n",
       "\n",
       "         [[-7.1411e-03, -3.0502e-02,  9.3994e-03,  ...,  9.6387e-01,\n",
       "            7.4268e-01,  1.2051e+00],\n",
       "          [-1.7188e-01,  4.5117e-01, -5.3564e-01,  ...,  7.3438e-01,\n",
       "           -2.7100e-01, -8.6279e-01],\n",
       "          [-1.0439e+00,  1.4463e+00, -2.1719e+00,  ..., -3.5498e-01,\n",
       "           -1.3672e-02, -5.4590e-01],\n",
       "          ...,\n",
       "          [-3.3887e-01,  4.3164e-01,  1.2988e+00,  ..., -2.0664e+00,\n",
       "           -1.1318e+00, -4.8906e+00],\n",
       "          [ 1.0107e+00,  5.9570e-01,  1.0938e+00,  ..., -1.3457e+00,\n",
       "           -1.0781e+00, -3.1562e+00],\n",
       "          [ 6.3477e-01,  1.8286e-01,  6.3086e-01,  ..., -4.6631e-01,\n",
       "           -6.4062e-01, -1.6396e+00]]],\n",
       "\n",
       "\n",
       "        [[[-9.8877e-03, -1.2085e-02,  1.2390e-02,  ..., -2.2217e-01,\n",
       "            9.0454e-02, -4.3604e-01],\n",
       "          [ 1.4316e+00,  5.4639e-01,  2.7832e-01,  ...,  1.0137e+00,\n",
       "           -1.6528e-01,  2.1211e+00],\n",
       "          [ 3.5889e-01, -2.0527e+00, -1.0195e+00,  ...,  3.8965e-01,\n",
       "           -6.2109e-01,  2.1426e+00],\n",
       "          ...,\n",
       "          [-9.6826e-01,  3.8745e-01,  7.4707e-02,  ...,  1.7725e+00,\n",
       "           -2.0566e+00,  6.4844e-01],\n",
       "          [-9.7705e-01, -9.5093e-02,  7.2705e-01,  ...,  1.5498e+00,\n",
       "           -1.7998e+00,  6.9678e-01],\n",
       "          [ 3.2715e-02, -6.0352e-01,  1.0137e+00,  ...,  1.5459e+00,\n",
       "           -1.7344e+00,  7.2266e-01]],\n",
       "\n",
       "         [[ 1.1108e-02, -2.3926e-02, -2.9907e-03,  ...,  7.2815e-02,\n",
       "           -1.6309e+00,  2.8516e-01],\n",
       "          [ 2.0625e+00,  1.5303e+00, -1.1416e+00,  ..., -2.2144e-01,\n",
       "            4.0469e+00,  3.6157e-01],\n",
       "          [ 3.1621e+00, -3.8281e-01, -2.1094e-01,  ..., -3.7891e-01,\n",
       "            3.9844e+00, -5.6689e-01],\n",
       "          ...,\n",
       "          [-7.7686e-01,  7.6758e-01,  1.8271e+00,  ..., -9.9463e-01,\n",
       "            3.3340e+00, -1.2998e+00],\n",
       "          [-2.1113e+00, -2.1924e-01,  1.4561e+00,  ..., -9.9316e-01,\n",
       "            3.3711e+00, -1.4033e+00],\n",
       "          [-1.5098e+00, -1.0605e+00,  3.7598e-01,  ..., -9.8633e-01,\n",
       "            3.2383e+00, -1.3418e+00]],\n",
       "\n",
       "         [[ 9.1553e-03,  1.7700e-03,  8.7433e-03,  ...,  4.3518e-02,\n",
       "           -2.8198e-01, -3.9502e-01],\n",
       "          [-2.0781e+00, -1.1426e+00,  2.3560e-01,  ...,  3.2471e-02,\n",
       "           -8.2227e-01, -1.2295e+00],\n",
       "          [-5.5713e-01, -1.4258e+00,  4.9463e-01,  ...,  1.0176e+00,\n",
       "           -2.7539e+00, -1.1289e+00],\n",
       "          ...,\n",
       "          [ 1.1855e+00, -9.5312e-01,  1.1694e-01,  ...,  1.9531e-01,\n",
       "            7.6611e-01,  9.7266e-01],\n",
       "          [ 7.3535e-01, -1.4072e+00, -3.6646e-01,  ...,  2.2461e-01,\n",
       "            1.1328e+00,  8.1104e-01],\n",
       "          [-3.3252e-01, -8.7451e-01, -6.7285e-01,  ...,  3.2910e-01,\n",
       "            1.0605e+00,  7.6172e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.4038e-02,  5.0354e-03, -6.9885e-03,  ..., -6.0645e-01,\n",
       "            2.1660e+00, -1.1810e-01],\n",
       "          [-9.2871e-01,  6.4209e-02,  4.5850e-01,  ..., -1.3818e+00,\n",
       "           -3.4805e+00,  1.0859e+00],\n",
       "          [ 1.7139e+00,  1.3457e+00, -7.7637e-01,  ..., -9.4922e-01,\n",
       "           -2.9844e+00,  1.0117e+00],\n",
       "          ...,\n",
       "          [ 1.0479e+00, -1.2964e-01, -1.5869e-03,  ...,  6.4014e-01,\n",
       "           -4.3477e+00,  1.0840e-01],\n",
       "          [ 2.8662e-01,  6.5527e-01,  3.2495e-01,  ...,  5.6348e-01,\n",
       "           -4.1562e+00, -6.3477e-02],\n",
       "          [-7.3535e-01,  1.0449e+00,  4.4385e-01,  ...,  6.9482e-01,\n",
       "           -4.1211e+00,  1.1621e-01]],\n",
       "\n",
       "         [[ 1.4626e-02, -1.8799e-02,  2.4200e-02,  ..., -9.9805e-01,\n",
       "           -1.7041e+00,  6.1572e-01],\n",
       "          [-7.8003e-02,  1.5405e-01,  1.4795e-01,  ...,  1.3047e+00,\n",
       "            4.5820e+00,  3.5156e-02],\n",
       "          [ 2.2266e-01, -4.6814e-02,  8.8501e-02,  ...,  1.5850e+00,\n",
       "            2.8398e+00,  8.0518e-01],\n",
       "          ...,\n",
       "          [ 5.0684e-01, -1.4624e-01, -4.0088e-01,  ...,  1.9834e+00,\n",
       "            1.9062e+00,  3.4082e-01],\n",
       "          [ 3.0005e-01, -5.1807e-01, -2.9297e-01,  ...,  1.7754e+00,\n",
       "            1.8945e+00,  2.6538e-01],\n",
       "          [-2.5806e-01, -5.0830e-01, -4.8706e-02,  ...,  1.7607e+00,\n",
       "            1.9580e+00,  2.8516e-01]],\n",
       "\n",
       "         [[-7.1411e-03, -3.0502e-02,  9.3994e-03,  ...,  9.6387e-01,\n",
       "            7.4268e-01,  1.2051e+00],\n",
       "          [-1.7188e-01,  4.5117e-01, -5.3564e-01,  ...,  7.3438e-01,\n",
       "           -2.7100e-01, -8.6279e-01],\n",
       "          [-1.2812e+00,  1.4883e+00, -1.7822e+00,  ...,  4.0869e-01,\n",
       "            1.0254e+00, -4.2798e-01],\n",
       "          ...,\n",
       "          [-8.3862e-02,  1.0400e+00,  7.7051e-01,  ..., -2.3572e-01,\n",
       "           -9.2871e-01, -1.4766e+00],\n",
       "          [ 3.6035e-01,  8.8574e-01,  9.1797e-01,  ..., -2.5879e-01,\n",
       "           -8.5840e-01, -1.5244e+00],\n",
       "          [ 5.2539e-01,  2.2827e-01,  6.3477e-01,  ..., -3.7262e-02,\n",
       "           -8.8037e-01, -1.5352e+00]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 2.9480e-02,  3.6850e-03, -1.5640e-02,  ...,  1.2558e-02,\n",
       "           -1.9302e-03, -1.9178e-03],\n",
       "          [-3.2990e-02, -9.7900e-02, -5.2588e-01,  ...,  1.4563e-01,\n",
       "           -2.7393e-01, -8.8135e-01],\n",
       "          [ 1.6528e-01,  4.6326e-02, -3.6108e-01,  ...,  1.6040e-01,\n",
       "           -1.2494e-01, -5.4395e-01],\n",
       "          ...,\n",
       "          [-1.4526e-01,  2.9126e-01, -1.7578e-01,  ..., -2.2986e-01,\n",
       "           -5.6000e-02, -2.9956e-01],\n",
       "          [ 6.3904e-02,  1.0388e-01, -2.0569e-01,  ..., -1.1011e-01,\n",
       "           -8.0505e-02, -4.8364e-01],\n",
       "          [-2.0093e-01, -1.3123e-02,  2.5360e-02,  ...,  5.7861e-02,\n",
       "           -5.7526e-02,  1.6284e-01]],\n",
       "\n",
       "         [[-1.9464e-03, -1.1749e-03, -1.1902e-02,  ..., -1.3138e-02,\n",
       "            1.3489e-02, -1.0345e-02],\n",
       "          [ 3.2135e-02, -3.8574e-02,  1.1157e-01,  ...,  3.8965e-01,\n",
       "           -6.8604e-02,  1.7542e-01],\n",
       "          [ 2.4512e-01,  1.7090e-01,  7.4707e-02,  ..., -2.5220e-01,\n",
       "           -2.3804e-01,  1.2115e-02],\n",
       "          ...,\n",
       "          [ 1.0913e-01,  5.4102e-01, -1.3885e-02,  ..., -8.1482e-02,\n",
       "            9.8206e-02,  8.2458e-02],\n",
       "          [-2.1460e-01,  2.4805e-01, -1.9836e-04,  ...,  6.8604e-02,\n",
       "            1.3603e-02,  9.4849e-02],\n",
       "          [-1.1090e-01,  3.5449e-01,  7.8278e-03,  ..., -1.0114e-01,\n",
       "            2.6074e-01, -7.2449e-02]],\n",
       "\n",
       "         [[-1.0719e-02,  1.2215e-02, -3.7460e-03,  ...,  2.3270e-03,\n",
       "            1.1810e-02, -8.8730e-03],\n",
       "          [-1.9690e-01,  3.3350e-01, -4.4873e-01,  ...,  1.2671e-01,\n",
       "            5.2148e-01,  2.3975e-01],\n",
       "          [ 2.5220e-01, -7.0801e-02,  7.7209e-02,  ..., -3.9429e-02,\n",
       "           -7.8430e-02, -1.7627e-01],\n",
       "          ...,\n",
       "          [ 3.5596e-01, -6.8665e-04,  2.9736e-01,  ...,  1.1047e-01,\n",
       "            1.0114e-01, -1.9885e-01],\n",
       "          [-3.8940e-01,  1.2537e-01,  9.4177e-02,  ...,  1.1792e-01,\n",
       "           -3.2153e-01, -4.7046e-01],\n",
       "          [-2.6196e-01, -2.1155e-01, -1.9849e-01,  ..., -2.0251e-01,\n",
       "           -4.0088e-01,  1.0638e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.4658e-02, -4.7493e-03, -8.2550e-03,  ...,  9.6970e-03,\n",
       "           -1.0080e-03,  2.1484e-02],\n",
       "          [-1.2207e-02,  2.0972e-01,  2.1887e-01,  ..., -2.0447e-01,\n",
       "           -2.3645e-01,  2.2009e-01],\n",
       "          [ 3.6304e-01, -1.9202e-01,  9.2590e-02,  ...,  7.5073e-02,\n",
       "           -5.1074e-01,  5.1147e-02],\n",
       "          ...,\n",
       "          [ 3.8422e-02, -6.9641e-02,  9.2163e-02,  ...,  1.4697e-01,\n",
       "           -1.4877e-03,  2.9517e-01],\n",
       "          [-3.4863e-01,  3.4253e-01,  1.8530e-01,  ..., -7.2998e-02,\n",
       "            1.4111e-01,  1.1383e-01],\n",
       "          [ 1.6663e-01,  2.5854e-01,  7.3425e-02,  ..., -1.3208e-01,\n",
       "            8.1177e-02,  2.2278e-02]],\n",
       "\n",
       "         [[ 1.6464e-02,  2.0187e-02, -8.3771e-03,  ...,  9.4070e-03,\n",
       "            7.8125e-03,  2.2659e-03],\n",
       "          [-2.1802e-01,  1.4502e-01,  1.4785e+00,  ..., -6.8237e-02,\n",
       "            1.0394e-01, -2.0850e-01],\n",
       "          [ 4.7168e-01, -6.7676e-01, -1.1182e-01,  ...,  5.3662e-01,\n",
       "            3.3838e-01, -1.8274e-01],\n",
       "          ...,\n",
       "          [-2.8223e-01,  2.9102e-01, -5.0195e-01,  ..., -4.3457e-01,\n",
       "           -6.0596e-01,  2.9419e-01],\n",
       "          [ 1.7725e-01, -1.1237e-01, -6.4111e-01,  ..., -1.0669e-01,\n",
       "            3.0054e-01, -1.3428e-01],\n",
       "          [ 1.1652e-01,  1.7700e-02, -3.1958e-01,  ...,  2.4072e-01,\n",
       "            3.1201e-01, -9.9609e-02]],\n",
       "\n",
       "         [[ 5.3711e-03, -1.1429e-02, -3.2177e-03,  ...,  1.4553e-03,\n",
       "           -1.3290e-02,  6.4087e-03],\n",
       "          [-1.7041e-01, -3.4888e-01, -4.5044e-01,  ..., -2.5098e-01,\n",
       "           -1.0834e-01, -2.3865e-01],\n",
       "          [-3.8916e-01,  6.4850e-04,  1.8481e-01,  ..., -3.9038e-01,\n",
       "           -1.5454e-01, -2.1631e-01],\n",
       "          ...,\n",
       "          [ 4.3555e-01,  3.1250e-01, -4.2664e-02,  ...,  4.3042e-01,\n",
       "            4.3481e-01,  2.0544e-01],\n",
       "          [-2.5586e-01,  1.6211e-01, -2.4829e-01,  ...,  7.0679e-02,\n",
       "           -8.5999e-02, -1.6943e-01],\n",
       "          [-4.2285e-01, -9.1431e-02, -3.1201e-01,  ..., -2.2546e-01,\n",
       "           -2.4951e-01, -7.3792e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.9480e-02,  3.6850e-03, -1.5640e-02,  ...,  1.2558e-02,\n",
       "           -1.9302e-03, -1.9178e-03],\n",
       "          [-3.2990e-02, -9.7900e-02, -5.2588e-01,  ...,  1.4563e-01,\n",
       "           -2.7393e-01, -8.8135e-01],\n",
       "          [ 1.0034e-01, -1.6162e-01, -2.1683e-02,  ...,  1.2474e-02,\n",
       "            5.0476e-02, -5.8350e-01],\n",
       "          ...,\n",
       "          [-1.0986e-01, -7.1045e-02,  9.8511e-02,  ...,  1.3269e-01,\n",
       "           -3.6670e-01, -1.5796e-01],\n",
       "          [-1.8628e-01, -4.8065e-02,  6.5796e-02,  ...,  1.5479e-01,\n",
       "           -3.2080e-01, -1.9189e-01],\n",
       "          [-1.4478e-01,  1.1200e-02,  7.9895e-02,  ...,  1.3281e-01,\n",
       "           -3.3618e-01, -2.5244e-01]],\n",
       "\n",
       "         [[-1.9464e-03, -1.1749e-03, -1.1902e-02,  ..., -1.3138e-02,\n",
       "            1.3489e-02, -1.0345e-02],\n",
       "          [ 3.2135e-02, -3.8574e-02,  1.1157e-01,  ...,  3.8965e-01,\n",
       "           -6.8604e-02,  1.7542e-01],\n",
       "          [ 1.9272e-02,  1.7322e-01,  1.8005e-02,  ..., -3.8696e-01,\n",
       "           -4.5654e-02, -7.6782e-02],\n",
       "          ...,\n",
       "          [ 1.8738e-02,  9.2468e-03,  2.8198e-01,  ...,  1.2451e-01,\n",
       "           -1.8298e-01,  6.3324e-03],\n",
       "          [ 4.6753e-02, -2.0905e-02,  2.6050e-01,  ...,  7.3059e-02,\n",
       "           -1.4734e-01,  6.8481e-02],\n",
       "          [ 5.5145e-02,  5.0049e-03,  2.6416e-01,  ...,  1.3257e-01,\n",
       "           -1.5820e-01,  1.0278e-01]],\n",
       "\n",
       "         [[-1.0719e-02,  1.2215e-02, -3.7460e-03,  ...,  2.3270e-03,\n",
       "            1.1810e-02, -8.8730e-03],\n",
       "          [-1.9690e-01,  3.3350e-01, -4.4873e-01,  ...,  1.2671e-01,\n",
       "            5.2148e-01,  2.3975e-01],\n",
       "          [ 4.0894e-01,  1.7188e-01,  3.0518e-01,  ...,  3.2520e-01,\n",
       "           -1.2524e-01, -2.3901e-01],\n",
       "          ...,\n",
       "          [-9.9670e-02,  3.7891e-01,  2.2864e-01,  ..., -5.6641e-01,\n",
       "            4.6936e-02,  3.0121e-02],\n",
       "          [-1.3367e-01,  2.8491e-01,  2.6001e-01,  ..., -6.2402e-01,\n",
       "            1.0632e-01,  6.3477e-02],\n",
       "          [-2.0007e-01,  2.5684e-01,  2.3901e-01,  ..., -6.0547e-01,\n",
       "            3.6865e-02,  1.5259e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.4658e-02, -4.7493e-03, -8.2550e-03,  ...,  9.6970e-03,\n",
       "           -1.0080e-03,  2.1484e-02],\n",
       "          [-1.2207e-02,  2.0972e-01,  2.1887e-01,  ..., -2.0447e-01,\n",
       "           -2.3645e-01,  2.2009e-01],\n",
       "          [-1.0645e-01,  4.8615e-02, -2.1167e-01,  ...,  8.1970e-02,\n",
       "           -4.1650e-01, -1.2354e-01],\n",
       "          ...,\n",
       "          [ 8.9874e-03,  3.5474e-01, -1.9324e-01,  ...,  1.4795e-01,\n",
       "           -2.1423e-01, -2.4707e-01],\n",
       "          [-9.1553e-03,  3.6987e-01, -2.1301e-01,  ...,  1.4075e-01,\n",
       "           -1.7505e-01, -3.2397e-01],\n",
       "          [-8.6792e-02,  3.8257e-01, -2.2864e-01,  ...,  2.0923e-01,\n",
       "           -1.6675e-01, -3.3057e-01]],\n",
       "\n",
       "         [[ 1.6464e-02,  2.0187e-02, -8.3771e-03,  ...,  9.4070e-03,\n",
       "            7.8125e-03,  2.2659e-03],\n",
       "          [-2.1802e-01,  1.4502e-01,  1.4785e+00,  ..., -6.8237e-02,\n",
       "            1.0394e-01, -2.0850e-01],\n",
       "          [ 6.0150e-02, -3.6426e-01,  2.5586e-01,  ..., -5.8533e-02,\n",
       "            1.2183e-01, -2.5195e-01],\n",
       "          ...,\n",
       "          [-2.3666e-02, -3.3447e-01,  2.8882e-01,  ...,  1.6284e-01,\n",
       "           -1.2927e-01,  3.0005e-01],\n",
       "          [ 1.7960e-02, -3.3154e-01,  2.7490e-01,  ...,  1.3354e-01,\n",
       "           -1.3599e-01,  2.8345e-01],\n",
       "          [-6.1340e-02, -3.6475e-01,  2.4573e-01,  ...,  1.2006e-01,\n",
       "           -3.8788e-02,  3.2227e-01]],\n",
       "\n",
       "         [[ 5.3711e-03, -1.1429e-02, -3.2177e-03,  ...,  1.4553e-03,\n",
       "           -1.3290e-02,  6.4087e-03],\n",
       "          [-1.7041e-01, -3.4888e-01, -4.5044e-01,  ..., -2.5098e-01,\n",
       "           -1.0834e-01, -2.3865e-01],\n",
       "          [-5.1953e-01,  3.9062e-02,  1.4062e-01,  ..., -4.3921e-01,\n",
       "            1.6553e-01,  4.8828e-04],\n",
       "          ...,\n",
       "          [-3.4454e-02,  3.0029e-01, -1.8579e-01,  ...,  2.2583e-01,\n",
       "           -2.8229e-02, -1.1731e-01],\n",
       "          [-4.1809e-02,  3.1738e-01, -2.2205e-01,  ...,  2.1704e-01,\n",
       "           -2.5177e-02, -6.1401e-02],\n",
       "          [-8.3130e-02,  2.5635e-01, -1.9128e-01,  ...,  2.8589e-01,\n",
       "           -7.0068e-02, -5.9601e-02]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-2.1545e-02,  6.2256e-03, -3.8300e-02,  ...,  5.7764e-01,\n",
       "            6.1890e-02,  2.4834e-03],\n",
       "          [ 2.5732e-01,  1.6504e+00,  8.0713e-01,  ...,  3.7671e-01,\n",
       "           -1.7891e+00, -8.8330e-01],\n",
       "          [ 1.1035e+00,  5.0244e-01, -3.2446e-01,  ...,  1.8525e+00,\n",
       "            4.3335e-02, -1.3867e+00],\n",
       "          ...,\n",
       "          [ 8.6914e-02,  6.3770e-01,  3.5449e-01,  ..., -1.3057e+00,\n",
       "           -3.5059e-01, -1.5596e+00],\n",
       "          [-7.4756e-01,  5.5029e-01,  4.2017e-01,  ..., -4.5605e-01,\n",
       "            2.2156e-01, -1.0938e+00],\n",
       "          [-1.4814e+00,  5.9082e-01,  2.0469e+00,  ...,  1.6543e+00,\n",
       "            8.3105e-01, -7.3828e-01]],\n",
       "\n",
       "         [[ 1.4221e-02, -1.8311e-03,  1.6754e-02,  ..., -2.5558e-02,\n",
       "           -1.4258e-01, -1.0236e-01],\n",
       "          [-1.2402e+00,  3.6865e-02, -1.4336e+00,  ...,  3.0200e-01,\n",
       "            2.4375e+00, -2.9492e-01],\n",
       "          [-2.4824e+00,  8.4082e-01, -3.4912e-01,  ..., -6.5527e-01,\n",
       "            5.0488e-01,  4.5825e-01],\n",
       "          ...,\n",
       "          [ 9.9023e-01,  1.4414e+00,  1.1074e+00,  ...,  8.8965e-01,\n",
       "            1.5703e+00, -1.1664e-01],\n",
       "          [ 1.2275e+00, -2.2583e-01, -6.9189e-01,  ...,  6.6992e-01,\n",
       "           -4.4403e-02,  6.0547e-01],\n",
       "          [-7.4316e-01, -1.3594e+00, -1.4551e+00,  ...,  5.4492e-01,\n",
       "            1.0059e+00,  1.3086e+00]],\n",
       "\n",
       "         [[-8.6365e-03, -1.7090e-02, -9.2773e-03,  ...,  2.6428e-02,\n",
       "            3.0615e-01,  1.4551e-01],\n",
       "          [ 4.8682e-01, -3.9746e-01,  9.9414e-01,  ...,  2.2461e+00,\n",
       "           -8.8867e-01,  8.5254e-01],\n",
       "          [ 8.2812e-01, -6.7383e-01,  1.5176e+00,  ..., -1.3633e+00,\n",
       "           -2.3242e+00,  1.4238e+00],\n",
       "          ...,\n",
       "          [-1.0244e+00, -7.9395e-01, -1.4531e+00,  ..., -2.2090e+00,\n",
       "            1.6299e+00, -5.5273e-01],\n",
       "          [-4.1699e-01, -5.2686e-01, -1.1334e-01,  ...,  2.4084e-01,\n",
       "           -4.7046e-01,  1.2393e+00],\n",
       "          [ 6.4111e-01,  2.0508e-01,  3.1128e-02,  ..., -9.9902e-01,\n",
       "           -7.2900e-01,  6.9922e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.8402e-02,  5.0354e-04, -1.5091e-02,  ...,  9.9512e-01,\n",
       "            8.9258e-01, -5.5176e-01],\n",
       "          [ 5.3162e-02, -1.1902e-02, -1.0291e-01,  ..., -1.5068e+00,\n",
       "           -1.1104e+00,  3.1738e-01],\n",
       "          [-4.1357e-01,  4.1821e-01,  1.3855e-02,  ..., -2.4883e+00,\n",
       "           -9.9561e-01,  1.6553e-01],\n",
       "          ...,\n",
       "          [ 2.1619e-01,  2.8320e-01, -1.5137e-01,  ..., -3.5625e+00,\n",
       "           -6.1719e+00,  1.7568e+00],\n",
       "          [ 6.4014e-01,  2.7734e-01, -2.9272e-01,  ..., -3.0938e+00,\n",
       "           -2.9395e+00, -6.2061e-01],\n",
       "          [ 2.3950e-01, -9.7473e-02, -2.3352e-01,  ..., -3.5762e+00,\n",
       "           -3.3809e+00,  4.7363e-01]],\n",
       "\n",
       "         [[-3.0212e-03, -1.4404e-02, -2.3682e-02,  ..., -1.4539e-01,\n",
       "            2.7954e-01, -1.4417e-01],\n",
       "          [-1.4160e-02,  2.0859e+00,  1.4697e+00,  ..., -8.7598e-01,\n",
       "           -1.4844e-01, -5.3125e-01],\n",
       "          [ 2.5449e+00, -2.4768e-01,  2.2046e-01,  ...,  9.7754e-01,\n",
       "            3.8403e-01, -2.8955e-01],\n",
       "          ...,\n",
       "          [-3.1348e-01,  8.7891e-01, -7.8613e-01,  ..., -9.6143e-01,\n",
       "           -1.0889e+00, -7.8369e-01],\n",
       "          [-1.7861e+00, -4.6460e-01,  4.4409e-01,  ..., -6.7529e-01,\n",
       "            5.6934e-01,  4.7754e-01],\n",
       "          [-1.6680e+00, -1.8574e+00,  1.6514e+00,  ..., -6.5039e-01,\n",
       "            6.6895e-01,  8.5254e-01]],\n",
       "\n",
       "         [[ 2.0142e-03,  9.8877e-03,  4.6814e-02,  ..., -6.1035e-02,\n",
       "           -2.7100e-01, -9.1797e-02],\n",
       "          [ 8.3740e-01,  7.7295e-01, -4.6460e-01,  ...,  1.8340e+00,\n",
       "           -8.0566e-01, -1.5605e+00],\n",
       "          [ 1.1934e+00, -1.5273e+00, -1.2188e+00,  ...,  1.2695e+00,\n",
       "            1.1729e+00, -1.4609e+00],\n",
       "          ...,\n",
       "          [ 1.6162e-01,  9.7021e-01,  1.0547e+00,  ...,  2.8223e-01,\n",
       "           -3.1250e-01,  8.2886e-02],\n",
       "          [-6.4502e-01,  3.7292e-02,  8.7939e-01,  ...,  3.9001e-02,\n",
       "            1.3916e-01,  3.0713e-01],\n",
       "          [ 2.9736e-01, -1.7354e+00,  5.2393e-01,  ...,  3.7891e-01,\n",
       "            8.8379e-01,  1.2305e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.1545e-02,  6.2256e-03, -3.8300e-02,  ...,  5.7764e-01,\n",
       "            6.1890e-02,  2.4834e-03],\n",
       "          [ 2.5732e-01,  1.6504e+00,  8.0713e-01,  ...,  3.7671e-01,\n",
       "           -1.7891e+00, -8.8330e-01],\n",
       "          [ 1.6309e+00,  5.6055e-01,  4.9756e-01,  ...,  2.8809e-01,\n",
       "            5.9180e-01, -6.1084e-01],\n",
       "          ...,\n",
       "          [-2.0398e-01,  2.9272e-01, -6.8701e-01,  ...,  1.6484e+00,\n",
       "            6.6406e-01,  2.8052e-01],\n",
       "          [-7.5977e-01,  8.6133e-01,  5.9937e-02,  ...,  1.7891e+00,\n",
       "            5.3076e-01,  3.1494e-01],\n",
       "          [-4.9707e-01,  8.5791e-01,  5.5615e-01,  ...,  1.8184e+00,\n",
       "            7.4414e-01,  2.1851e-01]],\n",
       "\n",
       "         [[ 1.4221e-02, -1.8311e-03,  1.6754e-02,  ..., -2.5558e-02,\n",
       "           -1.4258e-01, -1.0236e-01],\n",
       "          [-1.2402e+00,  3.6865e-02, -1.4336e+00,  ...,  3.0200e-01,\n",
       "            2.4375e+00, -2.9492e-01],\n",
       "          [-1.9961e+00,  1.1055e+00,  2.3145e-01,  ..., -7.5928e-02,\n",
       "            2.2363e+00,  2.1953e+00],\n",
       "          ...,\n",
       "          [ 1.3848e+00,  1.7793e+00, -3.8232e-01,  ...,  3.1006e-01,\n",
       "            1.6309e+00, -7.4707e-01],\n",
       "          [ 1.5859e+00,  3.9038e-01, -1.4170e+00,  ...,  2.2656e-01,\n",
       "            1.5469e+00, -5.7227e-01],\n",
       "          [ 3.3765e-01, -1.1143e+00, -1.7422e+00,  ...,  2.4707e-01,\n",
       "            1.7480e+00, -6.0107e-01]],\n",
       "\n",
       "         [[-8.6365e-03, -1.7090e-02, -9.2773e-03,  ...,  2.6428e-02,\n",
       "            3.0615e-01,  1.4551e-01],\n",
       "          [ 4.8682e-01, -3.9746e-01,  9.9414e-01,  ...,  2.2461e+00,\n",
       "           -8.8867e-01,  8.5254e-01],\n",
       "          [ 1.1816e+00, -7.1191e-01,  1.2549e+00,  ..., -1.0752e+00,\n",
       "           -1.1924e+00,  1.5293e+00],\n",
       "          ...,\n",
       "          [-7.7783e-01, -1.1670e+00, -2.9785e-01,  ..., -9.4299e-02,\n",
       "            3.0289e-02,  1.6211e+00],\n",
       "          [-1.4941e-01, -5.0293e-01, -1.5796e-01,  ..., -2.6660e-01,\n",
       "            1.8457e-01,  1.7900e+00],\n",
       "          [ 6.0742e-01,  4.9414e-01,  8.6914e-02,  ..., -1.3391e-01,\n",
       "            2.3242e-01,  1.7461e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.8402e-02,  5.0354e-04, -1.5091e-02,  ...,  9.9512e-01,\n",
       "            8.9258e-01, -5.5176e-01],\n",
       "          [ 5.3162e-02, -1.1902e-02, -1.0291e-01,  ..., -1.5068e+00,\n",
       "           -1.1104e+00,  3.1738e-01],\n",
       "          [-3.7671e-01,  3.0811e-01,  6.2256e-02,  ..., -1.2227e+00,\n",
       "           -1.8408e-01,  8.6182e-01],\n",
       "          ...,\n",
       "          [ 6.3623e-01,  7.4805e-01,  8.9966e-02,  ..., -2.5996e+00,\n",
       "           -1.1611e+00,  1.9795e+00],\n",
       "          [ 6.7480e-01,  6.7188e-01, -1.7383e-01,  ..., -2.6523e+00,\n",
       "           -1.1094e+00,  1.7715e+00],\n",
       "          [ 9.5215e-03,  1.3672e-01, -3.5059e-01,  ..., -2.7090e+00,\n",
       "           -1.2266e+00,  1.7939e+00]],\n",
       "\n",
       "         [[-3.0212e-03, -1.4404e-02, -2.3682e-02,  ..., -1.4539e-01,\n",
       "            2.7954e-01, -1.4417e-01],\n",
       "          [-1.4160e-02,  2.0859e+00,  1.4697e+00,  ..., -8.7598e-01,\n",
       "           -1.4844e-01, -5.3125e-01],\n",
       "          [ 2.4766e+00, -1.5442e-01,  2.5635e-01,  ...,  5.7324e-01,\n",
       "            7.4609e-01, -4.9438e-02],\n",
       "          ...,\n",
       "          [-6.4062e-01, -1.2988e-01, -7.9932e-01,  ..., -7.2876e-02,\n",
       "            3.0444e-01, -1.6748e-01],\n",
       "          [-1.4072e+00, -1.0342e+00,  1.4124e-01,  ...,  4.0527e-02,\n",
       "            2.0117e-01, -4.7394e-02],\n",
       "          [-9.3018e-01, -1.3555e+00,  1.0391e+00,  ...,  8.5327e-02,\n",
       "            1.5869e-01, -1.1310e-01]],\n",
       "\n",
       "         [[ 2.0142e-03,  9.8877e-03,  4.6814e-02,  ..., -6.1035e-02,\n",
       "           -2.7100e-01, -9.1797e-02],\n",
       "          [ 8.3740e-01,  7.7295e-01, -4.6460e-01,  ...,  1.8340e+00,\n",
       "           -8.0566e-01, -1.5605e+00],\n",
       "          [ 9.0576e-01, -1.2363e+00, -1.0781e+00,  ...,  5.6543e-01,\n",
       "           -3.0176e-01, -3.0420e-01],\n",
       "          ...,\n",
       "          [-1.4785e+00,  1.0332e+00,  5.5469e-01,  ...,  4.7021e-01,\n",
       "            2.0391e+00, -4.5312e-01],\n",
       "          [-1.3682e+00, -9.7949e-01,  8.6914e-01,  ...,  4.0210e-01,\n",
       "            1.9941e+00, -3.4741e-01],\n",
       "          [ 1.4307e-01, -2.2383e+00,  7.2559e-01,  ...,  4.3506e-01,\n",
       "            1.8740e+00, -4.0234e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-8.7280e-03,  8.4763e-03, -1.3666e-03,  ..., -9.4223e-03,\n",
       "           -9.9640e-03, -2.5673e-03],\n",
       "          [ 3.1470e-01, -2.7612e-01, -6.7749e-02,  ..., -8.9417e-03,\n",
       "            3.1689e-01,  2.4292e-01],\n",
       "          [ 2.0020e-01,  5.5713e-01, -9.8083e-02,  ...,  3.6987e-01,\n",
       "           -3.0981e-01,  8.9355e-02],\n",
       "          ...,\n",
       "          [-3.6304e-01,  2.0483e-01, -2.4744e-01,  ...,  1.1481e-01,\n",
       "           -1.2524e-01,  4.2786e-02],\n",
       "          [-2.3352e-01,  3.9526e-01,  1.1688e-01,  ...,  4.9463e-01,\n",
       "           -9.3567e-02, -8.4229e-02],\n",
       "          [-4.2725e-01,  2.9614e-01, -3.1372e-02,  ...,  4.4800e-02,\n",
       "           -4.1479e-01, -3.1421e-01]],\n",
       "\n",
       "         [[-2.1408e-02, -1.6739e-02,  5.0888e-03,  ..., -1.6144e-02,\n",
       "           -3.1662e-03,  9.6283e-03],\n",
       "          [ 2.7026e-01, -5.7281e-02,  4.0381e-01,  ..., -3.1128e-02,\n",
       "            1.4282e-01,  3.9331e-01],\n",
       "          [-2.1606e-01,  1.4404e-01, -3.4009e-01,  ...,  2.3376e-01,\n",
       "            2.5830e-01,  3.4717e-01],\n",
       "          ...,\n",
       "          [-1.9055e-01, -5.7098e-02, -1.4465e-02,  ..., -8.0688e-02,\n",
       "           -2.3999e-01,  1.3281e-01],\n",
       "          [ 2.7734e-01,  1.3208e-01, -2.9999e-02,  ...,  3.0225e-01,\n",
       "            3.2471e-01, -5.0000e-01],\n",
       "          [ 4.5471e-02,  3.5547e-01,  8.4900e-02,  ..., -2.6367e-01,\n",
       "            5.1855e-01, -6.9727e-01]],\n",
       "\n",
       "         [[ 1.6861e-02,  5.4346e-01,  7.6447e-03,  ...,  1.7181e-02,\n",
       "           -7.0419e-03, -1.4725e-02],\n",
       "          [-4.0100e-02, -7.6660e-01,  1.5244e-02,  ...,  1.7236e-01,\n",
       "           -4.1504e-01, -1.8335e-01],\n",
       "          [-1.3708e-01, -3.9185e-01, -9.4147e-03,  ...,  8.6060e-02,\n",
       "           -3.2166e-02,  4.0863e-02],\n",
       "          ...,\n",
       "          [-1.6064e-01, -6.9678e-01, -2.8174e-01,  ...,  4.7705e-01,\n",
       "           -3.5303e-01, -3.4570e-01],\n",
       "          [ 2.9590e-01, -3.4229e-01,  1.6638e-01,  ..., -5.0244e-01,\n",
       "           -1.9455e-02,  3.2349e-01],\n",
       "          [ 1.5515e-01, -2.4414e-01,  4.8706e-02,  ..., -9.8694e-02,\n",
       "            1.5771e-01,  3.3740e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-6.5804e-03, -9.6970e-03,  9.7198e-03,  ...,  9.0942e-03,\n",
       "            1.0361e-02,  7.6904e-03],\n",
       "          [-2.8000e-02, -7.2656e-01, -5.0781e-02,  ..., -1.7908e-01,\n",
       "            3.7183e-01, -4.5923e-01],\n",
       "          [-3.7988e-01, -6.8604e-02,  4.5898e-02,  ..., -1.8750e-01,\n",
       "           -3.1641e-01, -8.7158e-02],\n",
       "          ...,\n",
       "          [ 4.1162e-01, -5.8533e-02,  5.1367e-01,  ..., -9.3750e-02,\n",
       "           -2.1570e-01, -3.9624e-01],\n",
       "          [-8.9966e-02,  2.1606e-01, -1.1462e-01,  ..., -2.5073e-01,\n",
       "            4.8389e-01, -1.4990e-01],\n",
       "          [-2.1948e-01,  3.9697e-01,  2.2510e-01,  ...,  1.2402e-01,\n",
       "            2.9150e-01, -4.3652e-01]],\n",
       "\n",
       "         [[-1.3817e-02,  2.4376e-03, -4.4751e-01,  ..., -3.7251e-03,\n",
       "            1.3657e-03, -1.8127e-02],\n",
       "          [-2.9834e-01,  1.9678e-01,  9.3262e-01,  ...,  9.2102e-02,\n",
       "           -4.3750e-01,  5.2490e-01],\n",
       "          [-2.0325e-02,  2.1814e-01,  1.3513e-01,  ...,  2.7197e-01,\n",
       "            2.4994e-02,  2.3267e-01],\n",
       "          ...,\n",
       "          [-3.6792e-01, -5.8105e-01,  7.1191e-01,  ...,  1.5002e-01,\n",
       "           -2.4927e-01, -3.4229e-01],\n",
       "          [-2.7393e-01, -3.4741e-01,  8.4106e-02,  ...,  5.1172e-01,\n",
       "           -1.8799e-02, -1.7578e-01],\n",
       "          [-3.7939e-01, -1.3574e-01, -2.3901e-01,  ...,  3.6682e-02,\n",
       "           -2.7026e-01, -3.4424e-01]],\n",
       "\n",
       "         [[ 3.6087e-03, -8.0185e-03, -1.0376e-03,  ..., -9.3689e-03,\n",
       "            3.3493e-03,  5.4474e-03],\n",
       "          [-6.4990e-01, -3.9624e-01, -7.1240e-01,  ..., -1.8201e-01,\n",
       "           -6.4392e-02, -4.4775e-01],\n",
       "          [ 7.0312e-02, -7.9346e-04, -2.5342e-01,  ..., -3.6255e-01,\n",
       "           -4.4214e-01, -4.7119e-01],\n",
       "          ...,\n",
       "          [-5.0323e-02,  2.2400e-01,  4.1675e-01,  ..., -5.9424e-01,\n",
       "            5.7373e-02, -7.3608e-02],\n",
       "          [ 2.7466e-01, -8.7646e-02, -3.4497e-01,  ...,  2.8760e-01,\n",
       "           -2.2241e-01, -5.3516e-01],\n",
       "          [-2.3413e-01, -3.8330e-01, -4.5850e-01,  ...,  2.9443e-01,\n",
       "            2.2400e-01, -3.3447e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.7280e-03,  8.4763e-03, -1.3666e-03,  ..., -9.4223e-03,\n",
       "           -9.9640e-03, -2.5673e-03],\n",
       "          [ 3.1470e-01, -2.7612e-01, -6.7749e-02,  ..., -8.9417e-03,\n",
       "            3.1689e-01,  2.4292e-01],\n",
       "          [-4.5288e-02,  4.6875e-01,  2.0264e-01,  ...,  1.9434e-01,\n",
       "           -2.0581e-01, -1.6187e-01],\n",
       "          ...,\n",
       "          [-3.6670e-01, -3.8269e-02, -6.3232e-02,  ...,  4.3823e-01,\n",
       "            1.2988e-01, -5.3375e-02],\n",
       "          [-3.4619e-01, -1.9958e-02, -1.1292e-01,  ...,  4.5752e-01,\n",
       "            1.0138e-01, -3.4607e-02],\n",
       "          [-2.6685e-01, -4.4464e-02, -3.8361e-02,  ...,  4.8901e-01,\n",
       "            9.8938e-02, -2.0844e-02]],\n",
       "\n",
       "         [[-2.1408e-02, -1.6739e-02,  5.0888e-03,  ..., -1.6144e-02,\n",
       "           -3.1662e-03,  9.6283e-03],\n",
       "          [ 2.7026e-01, -5.7281e-02,  4.0381e-01,  ..., -3.1128e-02,\n",
       "            1.4282e-01,  3.9331e-01],\n",
       "          [-4.1235e-01, -1.6772e-01, -5.1367e-01,  ...,  6.6895e-01,\n",
       "            5.4688e-01, -3.5181e-01],\n",
       "          ...,\n",
       "          [-3.5913e-01, -3.2324e-01,  5.0537e-01,  ..., -4.2041e-01,\n",
       "            9.8145e-01, -6.7139e-01],\n",
       "          [-3.9893e-01, -3.3545e-01,  5.2881e-01,  ..., -4.2090e-01,\n",
       "            9.6875e-01, -6.9873e-01],\n",
       "          [-2.8711e-01, -3.4644e-01,  5.5664e-01,  ..., -4.3164e-01,\n",
       "            8.8330e-01, -6.6309e-01]],\n",
       "\n",
       "         [[ 1.6861e-02,  5.4346e-01,  7.6447e-03,  ...,  1.7181e-02,\n",
       "           -7.0419e-03, -1.4725e-02],\n",
       "          [-4.0100e-02, -7.6660e-01,  1.5244e-02,  ...,  1.7236e-01,\n",
       "           -4.1504e-01, -1.8335e-01],\n",
       "          [ 7.9834e-02, -1.9653e-01,  4.6143e-02,  ...,  7.1716e-02,\n",
       "           -1.6992e-01, -3.8727e-02],\n",
       "          ...,\n",
       "          [-2.7734e-01,  1.9775e-02, -1.3281e-01,  ..., -3.7891e-01,\n",
       "            2.7148e-01,  1.4746e-01],\n",
       "          [-2.7368e-01,  8.2642e-02, -1.4709e-01,  ..., -4.0918e-01,\n",
       "            3.6035e-01,  1.6064e-01],\n",
       "          [-2.3169e-01,  8.9417e-02, -1.3342e-01,  ..., -3.4766e-01,\n",
       "            2.9736e-01,  1.4038e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-6.5804e-03, -9.6970e-03,  9.7198e-03,  ...,  9.0942e-03,\n",
       "            1.0361e-02,  7.6904e-03],\n",
       "          [-2.8000e-02, -7.2656e-01, -5.0781e-02,  ..., -1.7908e-01,\n",
       "            3.7183e-01, -4.5923e-01],\n",
       "          [-3.8525e-01, -6.5674e-02, -1.4771e-01,  ...,  9.0759e-02,\n",
       "           -6.1914e-01, -3.2080e-01],\n",
       "          ...,\n",
       "          [-3.1787e-01,  3.7915e-01,  1.6785e-02,  ...,  3.5010e-01,\n",
       "           -1.8384e-01,  2.9883e-01],\n",
       "          [-3.2153e-01,  3.8794e-01,  4.4556e-03,  ...,  4.1309e-01,\n",
       "           -2.6807e-01,  2.5977e-01],\n",
       "          [-2.7808e-01,  4.2310e-01,  1.4038e-03,  ...,  4.3921e-01,\n",
       "           -1.9067e-01,  2.1533e-01]],\n",
       "\n",
       "         [[-1.3817e-02,  2.4376e-03, -4.4751e-01,  ..., -3.7251e-03,\n",
       "            1.3657e-03, -1.8127e-02],\n",
       "          [-2.9834e-01,  1.9678e-01,  9.3262e-01,  ...,  9.2102e-02,\n",
       "           -4.3750e-01,  5.2490e-01],\n",
       "          [ 3.7170e-02, -3.9185e-02,  1.5747e-01,  ..., -9.3307e-03,\n",
       "            1.0553e-01,  2.0801e-01],\n",
       "          ...,\n",
       "          [-6.1707e-02,  3.0914e-02, -8.7585e-03,  ..., -1.7212e-01,\n",
       "           -7.4585e-02, -1.3025e-01],\n",
       "          [-7.6416e-02,  6.7383e-02, -8.8501e-04,  ..., -1.7358e-01,\n",
       "           -1.4648e-01, -1.3989e-01],\n",
       "          [-7.1045e-02,  5.2887e-02, -5.3955e-02,  ..., -9.4177e-02,\n",
       "           -1.1346e-01, -1.1292e-01]],\n",
       "\n",
       "         [[ 3.6087e-03, -8.0185e-03, -1.0376e-03,  ..., -9.3689e-03,\n",
       "            3.3493e-03,  5.4474e-03],\n",
       "          [-6.4990e-01, -3.9624e-01, -7.1240e-01,  ..., -1.8201e-01,\n",
       "           -6.4392e-02, -4.4775e-01],\n",
       "          [-6.5625e-01,  2.5903e-01, -3.8696e-01,  ..., -1.2812e+00,\n",
       "            5.1221e-01,  1.4136e-01],\n",
       "          ...,\n",
       "          [-4.2065e-01, -3.0615e-01, -4.4116e-01,  ...,  1.9958e-01,\n",
       "            2.4194e-01, -6.3574e-01],\n",
       "          [-3.9917e-01, -3.6084e-01, -4.6411e-01,  ...,  1.3855e-01,\n",
       "            2.5024e-01, -6.6406e-01],\n",
       "          [-4.0137e-01, -3.8428e-01, -5.0977e-01,  ...,  7.6294e-02,\n",
       "            2.4548e-01, -7.0166e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-2.0035e-02, -2.0355e-02,  6.0730e-03,  ...,  5.6152e-02,\n",
       "           -3.9478e-01, -2.9932e-01],\n",
       "          [ 9.8047e-01, -2.4268e-01, -4.3896e-01,  ..., -1.7500e+00,\n",
       "           -9.8633e-02, -1.2178e+00],\n",
       "          [ 8.7012e-01, -5.3418e-01, -1.1426e+00,  ..., -3.0591e-01,\n",
       "           -1.1221e+00,  4.9609e-01],\n",
       "          ...,\n",
       "          [-3.0640e-02,  7.3242e-02,  1.1533e+00,  ..., -6.2793e-01,\n",
       "            2.6602e+00, -1.1650e+00],\n",
       "          [-1.3965e-01, -5.4016e-03,  4.1626e-01,  ..., -2.7612e-01,\n",
       "            2.9517e-01, -1.2384e-01],\n",
       "          [-5.1074e-01, -6.1914e-01, -3.4155e-01,  ..., -6.9629e-01,\n",
       "            4.2725e-01, -1.2451e+00]],\n",
       "\n",
       "         [[ 2.7344e-02,  1.6357e-02, -1.9348e-02,  ..., -4.3457e-02,\n",
       "            1.0565e-01,  3.3643e-01],\n",
       "          [-2.0137e+00, -5.3809e-01, -1.3164e+00,  ...,  1.3096e+00,\n",
       "            1.3159e-01,  5.5566e-01],\n",
       "          [-3.8330e-01,  1.6895e-01, -6.1401e-02,  ...,  1.0703e+00,\n",
       "            1.9409e-01,  8.0908e-01],\n",
       "          ...,\n",
       "          [ 1.2656e+00, -1.6377e+00, -1.3896e+00,  ..., -1.1633e-01,\n",
       "           -2.4746e+00, -4.5703e-01],\n",
       "          [ 3.3496e-01, -6.1426e-01, -7.0361e-01,  ...,  8.1787e-01,\n",
       "           -2.0117e+00, -2.9688e+00],\n",
       "          [-1.6709e+00,  1.9707e+00, -3.2886e-01,  ...,  3.3594e-01,\n",
       "           -1.7988e+00, -3.2109e+00]],\n",
       "\n",
       "         [[-2.9236e-02, -2.6123e-02,  4.0527e-02,  ..., -5.4443e-02,\n",
       "            8.7646e-02, -2.1240e-01],\n",
       "          [-1.6865e+00, -2.0020e-02,  9.9805e-01,  ...,  1.0898e+00,\n",
       "            7.5781e-01, -9.7949e-01],\n",
       "          [-3.8984e+00, -1.9751e-01, -1.6367e+00,  ...,  4.6729e-01,\n",
       "            1.5215e+00,  1.9702e-01],\n",
       "          ...,\n",
       "          [ 1.4443e+00,  5.9668e-01,  7.0996e-01,  ...,  2.8066e+00,\n",
       "           -1.3892e-01,  9.8096e-01],\n",
       "          [ 1.6494e+00,  3.2861e-01,  1.7246e+00,  ...,  9.0820e-01,\n",
       "            1.8340e+00,  6.2158e-01],\n",
       "          [ 7.5732e-01, -3.7378e-01,  2.7520e+00,  ...,  2.8125e-01,\n",
       "            7.8076e-01, -7.1228e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.4229e-03, -1.9775e-02, -1.2512e-02,  ...,  3.1586e-02,\n",
       "           -6.7749e-02,  2.9224e-01],\n",
       "          [ 2.5269e-01, -2.1118e-01,  1.0820e+00,  ...,  3.8379e-01,\n",
       "            9.4434e-01,  1.5576e+00],\n",
       "          [ 3.0195e+00, -1.1758e+00,  1.8086e+00,  ...,  1.9160e+00,\n",
       "            3.2532e-02,  2.3071e-01],\n",
       "          ...,\n",
       "          [-2.7490e-01, -1.1211e+00, -1.7734e+00,  ..., -2.8164e+00,\n",
       "           -1.1758e+00,  7.2021e-01],\n",
       "          [-1.3652e+00, -7.4658e-01, -7.4756e-01,  ..., -2.2266e+00,\n",
       "            2.3575e-02, -9.0186e-01],\n",
       "          [-1.6113e+00, -7.3547e-02,  2.4658e-01,  ...,  2.6831e-01,\n",
       "            9.0430e-01,  7.0996e-01]],\n",
       "\n",
       "         [[ 1.5259e-03, -2.7008e-02,  3.3478e-02,  ...,  2.0703e-01,\n",
       "           -6.2683e-02,  1.7603e-01],\n",
       "          [-2.5410e+00, -5.2539e-01, -1.6201e+00,  ..., -1.4531e+00,\n",
       "            9.6777e-01, -1.0566e+00],\n",
       "          [-3.7617e+00, -1.2793e+00, -2.8301e+00,  ..., -7.7148e-01,\n",
       "           -6.0352e-01, -2.0166e-01],\n",
       "          ...,\n",
       "          [ 3.6836e+00,  1.1484e+00,  2.8828e+00,  ...,  6.5723e-01,\n",
       "           -1.4832e-01, -3.0352e+00],\n",
       "          [ 3.2344e+00, -8.5400e-01,  1.2070e+00,  ..., -1.8848e+00,\n",
       "           -6.7041e-01, -2.6733e-01],\n",
       "          [ 1.2559e+00, -3.9727e+00,  4.6973e-01,  ..., -3.2227e+00,\n",
       "           -1.4434e+00,  1.2140e-01]],\n",
       "\n",
       "         [[-3.6621e-03, -4.3793e-03, -1.9775e-02,  ..., -1.6719e+00,\n",
       "            1.5039e-01, -1.3623e-01],\n",
       "          [ 1.4395e+00, -1.5625e-01, -1.1543e+00,  ...,  4.0938e+00,\n",
       "           -5.4443e-01,  1.0977e+00],\n",
       "          [ 2.1406e+00, -1.6719e+00, -8.5205e-01,  ...,  4.3359e+00,\n",
       "            9.0527e-01, -1.8408e-01],\n",
       "          ...,\n",
       "          [-1.5195e+00, -9.1016e-01, -9.8535e-01,  ...,  5.5469e+00,\n",
       "            5.6348e-01, -3.5254e-01],\n",
       "          [-1.2979e+00, -1.1270e+00, -3.5083e-01,  ...,  4.0625e+00,\n",
       "           -8.8086e-01,  5.3564e-01],\n",
       "          [-7.1289e-01, -1.2305e+00, -7.2461e-01,  ...,  2.6836e+00,\n",
       "           -8.3545e-01, -5.9912e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.0035e-02, -2.0355e-02,  6.0730e-03,  ...,  5.6152e-02,\n",
       "           -3.9478e-01, -2.9932e-01],\n",
       "          [ 9.8047e-01, -2.4268e-01, -4.3896e-01,  ..., -1.7500e+00,\n",
       "           -9.8633e-02, -1.2178e+00],\n",
       "          [ 9.6240e-01, -6.1279e-01, -9.0430e-01,  ..., -3.4814e-01,\n",
       "           -3.9404e-01, -1.9922e-01],\n",
       "          ...,\n",
       "          [-1.6040e-01,  1.6626e-01,  4.3115e-01,  ...,  1.9128e-01,\n",
       "           -1.4131e+00, -1.7695e+00],\n",
       "          [-6.4893e-01, -6.2158e-01,  2.8125e-01,  ...,  2.8687e-01,\n",
       "           -1.3818e+00, -1.7178e+00],\n",
       "          [-5.6689e-01, -1.0117e+00, -6.9946e-02,  ...,  2.8809e-01,\n",
       "           -1.2148e+00, -1.9375e+00]],\n",
       "\n",
       "         [[ 2.7344e-02,  1.6357e-02, -1.9348e-02,  ..., -4.3457e-02,\n",
       "            1.0565e-01,  3.3643e-01],\n",
       "          [-2.0137e+00, -5.3809e-01, -1.3164e+00,  ...,  1.3096e+00,\n",
       "            1.3159e-01,  5.5566e-01],\n",
       "          [ 1.5283e-01, -1.5527e-01, -1.6345e-01,  ...,  1.2539e+00,\n",
       "           -1.7578e-01,  1.6514e+00],\n",
       "          ...,\n",
       "          [ 1.3232e+00, -1.6836e+00,  4.6387e-01,  ...,  3.8867e-01,\n",
       "           -2.1074e+00, -2.3047e+00],\n",
       "          [-3.4741e-01, -6.1133e-01,  2.1252e-01,  ...,  3.0664e-01,\n",
       "           -2.1309e+00, -2.2539e+00],\n",
       "          [-1.7041e+00,  9.2480e-01,  8.0566e-02,  ...,  2.6074e-01,\n",
       "           -2.1328e+00, -2.3711e+00]],\n",
       "\n",
       "         [[-2.9236e-02, -2.6123e-02,  4.0527e-02,  ..., -5.4443e-02,\n",
       "            8.7646e-02, -2.1240e-01],\n",
       "          [-1.6865e+00, -2.0020e-02,  9.9805e-01,  ...,  1.0898e+00,\n",
       "            7.5781e-01, -9.7949e-01],\n",
       "          [-4.1523e+00, -4.4995e-01, -1.2598e+00,  ..., -5.3613e-01,\n",
       "            3.9917e-01,  7.8979e-02],\n",
       "          ...,\n",
       "          [ 2.8662e-01,  1.3945e+00, -5.3906e-01,  ..., -5.7227e-01,\n",
       "            9.3652e-01, -2.7075e-01],\n",
       "          [ 1.3545e+00,  4.4263e-01,  8.7598e-01,  ..., -4.5386e-01,\n",
       "            9.4336e-01, -1.1963e-01],\n",
       "          [ 1.1816e+00, -6.2500e-01,  1.8887e+00,  ..., -4.2432e-01,\n",
       "            1.0293e+00, -2.1936e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.4229e-03, -1.9775e-02, -1.2512e-02,  ...,  3.1586e-02,\n",
       "           -6.7749e-02,  2.9224e-01],\n",
       "          [ 2.5269e-01, -2.1118e-01,  1.0820e+00,  ...,  3.8379e-01,\n",
       "            9.4434e-01,  1.5576e+00],\n",
       "          [ 2.9727e+00, -1.0312e+00,  1.6055e+00,  ...,  1.5879e+00,\n",
       "           -5.2832e-01,  4.6045e-01],\n",
       "          ...,\n",
       "          [ 7.2119e-01,  4.7485e-02, -1.1807e+00,  ...,  2.7344e-01,\n",
       "            1.1377e+00, -2.3975e-01],\n",
       "          [-1.4551e+00, -3.0591e-01, -7.5732e-01,  ...,  4.9219e-01,\n",
       "            9.9805e-01, -1.7224e-01],\n",
       "          [-2.2324e+00, -2.6416e-01,  6.5430e-02,  ...,  4.8462e-01,\n",
       "            9.7363e-01, -1.0791e-01]],\n",
       "\n",
       "         [[ 1.5259e-03, -2.7008e-02,  3.3478e-02,  ...,  2.0703e-01,\n",
       "           -6.2683e-02,  1.7603e-01],\n",
       "          [-2.5410e+00, -5.2539e-01, -1.6201e+00,  ..., -1.4531e+00,\n",
       "            9.6777e-01, -1.0566e+00],\n",
       "          [-3.6309e+00, -5.2539e-01, -3.4844e+00,  ..., -2.1973e+00,\n",
       "            3.5034e-01, -4.1064e-01],\n",
       "          ...,\n",
       "          [ 8.8086e-01,  1.1348e+00,  2.0840e+00,  ..., -1.4766e+00,\n",
       "           -2.0000e+00,  6.7236e-01],\n",
       "          [ 2.7480e+00, -7.7100e-01,  1.6133e+00,  ..., -1.4746e+00,\n",
       "           -2.0312e+00,  7.1729e-01],\n",
       "          [ 1.9854e+00, -2.3223e+00,  4.6191e-01,  ..., -1.4785e+00,\n",
       "           -1.9854e+00,  5.3711e-01]],\n",
       "\n",
       "         [[-3.6621e-03, -4.3793e-03, -1.9775e-02,  ..., -1.6719e+00,\n",
       "            1.5039e-01, -1.3623e-01],\n",
       "          [ 1.4395e+00, -1.5625e-01, -1.1543e+00,  ...,  4.0938e+00,\n",
       "           -5.4443e-01,  1.0977e+00],\n",
       "          [ 2.4277e+00, -2.0645e+00, -3.6011e-01,  ...,  3.7930e+00,\n",
       "           -1.6064e-01, -6.4648e-01],\n",
       "          ...,\n",
       "          [-8.8086e-01, -7.6514e-01,  2.3438e-01,  ...,  1.3955e+00,\n",
       "            1.9189e-01, -1.5684e+00],\n",
       "          [-1.8887e+00, -1.2412e+00, -6.2842e-01,  ...,  1.4453e+00,\n",
       "            3.7476e-01, -1.5410e+00],\n",
       "          [-1.1465e+00, -9.8975e-01, -1.2324e+00,  ...,  1.2637e+00,\n",
       "            3.0566e-01, -1.4766e+00]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-3.5095e-04, -3.4485e-03,  6.3705e-03,  ..., -5.0201e-03,\n",
       "            4.7913e-03, -6.8321e-03],\n",
       "          [ 1.9116e-01,  2.4963e-02,  3.4570e-01,  ..., -8.0994e-02,\n",
       "            1.3110e-01, -2.9614e-01],\n",
       "          [ 4.0698e-01,  3.6157e-01,  9.7461e-01,  ..., -1.6492e-01,\n",
       "           -1.8970e-01, -3.3179e-01],\n",
       "          ...,\n",
       "          [-7.0508e-01,  4.0405e-02, -2.2168e-01,  ...,  2.3853e-01,\n",
       "           -6.0400e-01,  3.1763e-01],\n",
       "          [-1.5918e-01, -2.0203e-01, -7.5317e-02,  ...,  6.5820e-01,\n",
       "           -7.9498e-03, -2.0035e-02],\n",
       "          [-2.0862e-01, -4.3121e-02,  2.0227e-01,  ..., -4.1321e-02,\n",
       "           -1.2494e-01, -1.1871e-02]],\n",
       "\n",
       "         [[-1.1879e-02, -3.1281e-03, -9.4986e-03,  ..., -4.3945e-03,\n",
       "            2.2316e-03,  3.1433e-03],\n",
       "          [ 4.6045e-01,  4.7925e-01,  2.6074e-01,  ...,  1.0559e-01,\n",
       "           -3.4033e-01,  2.9028e-01],\n",
       "          [ 2.7124e-01,  7.4463e-01,  3.8055e-02,  ...,  4.1357e-01,\n",
       "           -3.5254e-01,  6.4014e-01],\n",
       "          ...,\n",
       "          [ 5.7275e-01,  3.6328e-01,  6.2402e-01,  ...,  3.8403e-01,\n",
       "            1.2939e-01, -5.4834e-01],\n",
       "          [ 4.1650e-01, -1.3782e-01, -4.6533e-01,  ...,  6.4209e-01,\n",
       "           -9.8022e-02, -5.3906e-01],\n",
       "          [-1.1914e-01,  3.9038e-01, -8.9722e-02,  ...,  6.9727e-01,\n",
       "            3.4546e-01, -2.4707e-01]],\n",
       "\n",
       "         [[ 3.1525e-02,  4.2343e-04, -4.7302e-03,  ..., -7.7820e-04,\n",
       "           -1.1444e-05,  2.7374e-02],\n",
       "          [-2.6270e-01,  1.0663e-01,  2.8564e-01,  ...,  7.3669e-02,\n",
       "            1.3611e-01,  4.4751e-01],\n",
       "          [-6.2646e-01, -3.7231e-02,  1.5857e-01,  ..., -8.1726e-02,\n",
       "            1.7603e-01,  3.1464e-02],\n",
       "          ...,\n",
       "          [-3.1494e-01,  1.2207e-01,  1.3489e-01,  ...,  1.1230e-01,\n",
       "            1.3477e-01, -2.2009e-01],\n",
       "          [ 1.4136e-01, -1.4368e-01, -3.4619e-01,  ..., -2.5830e-01,\n",
       "            8.5022e-02,  3.1641e-01],\n",
       "          [ 1.9702e-01, -4.2053e-02, -2.8564e-01,  ..., -1.7548e-02,\n",
       "           -3.3081e-02,  9.1553e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.4676e-03, -7.5645e-03,  1.2379e-03,  ...,  2.9507e-03,\n",
       "           -5.2071e-04,  5.4970e-03],\n",
       "          [-4.7095e-01, -2.1509e-01,  2.7612e-01,  ...,  8.0713e-01,\n",
       "           -3.4399e-01, -3.1299e-01],\n",
       "          [-1.8396e-01, -3.3203e-02, -4.0283e-03,  ..., -5.0446e-02,\n",
       "            1.0962e-01, -1.3965e-01],\n",
       "          ...,\n",
       "          [ 1.5515e-01, -3.9136e-01,  1.9800e-01,  ..., -9.0881e-02,\n",
       "            2.3438e-01, -1.1121e-01],\n",
       "          [ 1.7896e-01, -1.4478e-01,  4.1779e-02,  ..., -2.5537e-01,\n",
       "            6.0577e-03, -4.6722e-02],\n",
       "          [-6.5869e-01,  4.6680e-01,  9.0881e-02,  ..., -1.2213e-01,\n",
       "            5.2979e-01,  1.5686e-01]],\n",
       "\n",
       "         [[ 1.0849e-02,  5.6458e-03, -1.9684e-03,  ..., -1.3245e-02,\n",
       "           -1.0674e-02, -2.4368e-02],\n",
       "          [-2.4426e-01, -2.0813e-02, -8.9014e-01,  ...,  1.0992e-01,\n",
       "           -9.7900e-02, -2.4854e-01],\n",
       "          [-2.5726e-02,  3.2288e-02,  3.9886e-02,  ...,  7.8418e-01,\n",
       "            3.0908e-01,  2.6147e-01],\n",
       "          ...,\n",
       "          [-3.9307e-01, -1.8219e-02, -1.5869e-01,  ...,  1.7181e-02,\n",
       "            3.6011e-02, -6.0303e-02],\n",
       "          [-3.5132e-01,  3.1738e-01,  7.3120e-02,  ...,  1.6211e-01,\n",
       "            7.2449e-02,  2.0721e-02],\n",
       "          [ 2.1851e-01,  3.4741e-01, -4.0137e-01,  ...,  2.9297e-01,\n",
       "            1.8091e-01, -1.2703e-02]],\n",
       "\n",
       "         [[-2.5940e-04,  7.1754e-03,  2.8801e-03,  ...,  5.2643e-03,\n",
       "            1.4130e-02,  2.9469e-03],\n",
       "          [ 7.1411e-02,  1.7261e-01, -8.9722e-02,  ..., -1.2274e-01,\n",
       "           -1.7822e-01,  2.8516e-01],\n",
       "          [ 1.5125e-01,  3.9282e-01, -1.7090e-01,  ..., -2.0386e-01,\n",
       "           -3.3691e-02,  3.0469e-01],\n",
       "          ...,\n",
       "          [-2.1411e-01,  5.8691e-01, -2.5391e-01,  ..., -5.1416e-01,\n",
       "            8.1726e-02, -6.7627e-01],\n",
       "          [ 1.9885e-01, -8.6182e-02,  5.1953e-01,  ..., -1.5515e-01,\n",
       "            4.2261e-01, -2.1045e-01],\n",
       "          [-1.7603e-01,  2.8809e-01,  1.8054e-01,  ..., -2.1240e-01,\n",
       "            1.9806e-02, -4.5190e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.5095e-04, -3.4485e-03,  6.3705e-03,  ..., -5.0201e-03,\n",
       "            4.7913e-03, -6.8321e-03],\n",
       "          [ 1.9116e-01,  2.4963e-02,  3.4570e-01,  ..., -8.0994e-02,\n",
       "            1.3110e-01, -2.9614e-01],\n",
       "          [-3.5034e-01,  6.0547e-01,  7.4756e-01,  ..., -1.0901e-01,\n",
       "            3.4302e-01, -3.1470e-01],\n",
       "          ...,\n",
       "          [-2.0776e-01, -3.3398e-01,  2.5415e-01,  ...,  2.1863e-01,\n",
       "           -6.9189e-01, -1.0788e-02],\n",
       "          [-2.0093e-01, -2.5146e-01,  2.6465e-01,  ...,  2.7100e-01,\n",
       "           -7.0508e-01,  2.8961e-02],\n",
       "          [-1.8152e-01, -2.2485e-01,  3.0664e-01,  ...,  2.5000e-01,\n",
       "           -6.5674e-01,  1.3329e-02]],\n",
       "\n",
       "         [[-1.1879e-02, -3.1281e-03, -9.4986e-03,  ..., -4.3945e-03,\n",
       "            2.2316e-03,  3.1433e-03],\n",
       "          [ 4.6045e-01,  4.7925e-01,  2.6074e-01,  ...,  1.0559e-01,\n",
       "           -3.4033e-01,  2.9028e-01],\n",
       "          [ 8.8623e-02,  8.4082e-01, -4.1968e-01,  ...,  4.6631e-01,\n",
       "           -2.7734e-01,  6.1084e-01],\n",
       "          ...,\n",
       "          [-1.7627e-01,  7.8003e-02,  6.3477e-03,  ...,  5.1172e-01,\n",
       "            6.0059e-01,  2.1033e-01],\n",
       "          [-2.5854e-01,  8.6182e-02,  9.3262e-02,  ...,  5.8154e-01,\n",
       "            5.5908e-01,  2.3047e-01],\n",
       "          [-2.1912e-01,  1.0675e-01,  1.5344e-01,  ...,  5.0146e-01,\n",
       "            5.7422e-01,  2.1399e-01]],\n",
       "\n",
       "         [[ 3.1525e-02,  4.2343e-04, -4.7302e-03,  ..., -7.7820e-04,\n",
       "           -1.1444e-05,  2.7374e-02],\n",
       "          [-2.6270e-01,  1.0663e-01,  2.8564e-01,  ...,  7.3669e-02,\n",
       "            1.3611e-01,  4.4751e-01],\n",
       "          [-3.6670e-01,  3.9282e-01, -3.2373e-01,  ..., -4.4037e-02,\n",
       "            8.2764e-02,  1.8750e-01],\n",
       "          ...,\n",
       "          [ 2.1387e-01,  1.7102e-01,  8.9233e-02,  ...,  3.0664e-01,\n",
       "           -1.0632e-01,  4.3896e-01],\n",
       "          [ 2.1069e-01,  1.9287e-01,  8.4106e-02,  ...,  3.2446e-01,\n",
       "           -9.1675e-02,  5.0537e-01],\n",
       "          [ 2.5293e-01,  1.6711e-01,  1.3660e-01,  ...,  2.6514e-01,\n",
       "           -1.8481e-01,  4.9780e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.4676e-03, -7.5645e-03,  1.2379e-03,  ...,  2.9507e-03,\n",
       "           -5.2071e-04,  5.4970e-03],\n",
       "          [-4.7095e-01, -2.1509e-01,  2.7612e-01,  ...,  8.0713e-01,\n",
       "           -3.4399e-01, -3.1299e-01],\n",
       "          [ 1.8396e-01, -3.2324e-01,  6.1328e-01,  ...,  7.9468e-02,\n",
       "            1.8054e-01, -1.5312e-02],\n",
       "          ...,\n",
       "          [-2.3193e-01,  8.8928e-02,  3.3789e-01,  ...,  8.3984e-02,\n",
       "            2.9395e-01, -2.0728e-01],\n",
       "          [-2.9102e-01,  8.4839e-02,  2.7930e-01,  ...,  1.2439e-01,\n",
       "            3.0640e-01, -1.6858e-01],\n",
       "          [-2.2888e-01,  1.1084e-01,  2.8442e-01,  ...,  1.4478e-01,\n",
       "            2.9150e-01, -2.2046e-01]],\n",
       "\n",
       "         [[ 1.0849e-02,  5.6458e-03, -1.9684e-03,  ..., -1.3245e-02,\n",
       "           -1.0674e-02, -2.4368e-02],\n",
       "          [-2.4426e-01, -2.0813e-02, -8.9014e-01,  ...,  1.0992e-01,\n",
       "           -9.7900e-02, -2.4854e-01],\n",
       "          [ 6.7432e-01,  4.4751e-01,  1.7676e-01,  ...,  7.2363e-01,\n",
       "            2.3352e-01,  8.5205e-01],\n",
       "          ...,\n",
       "          [-9.5459e-02, -1.0651e-01, -6.1646e-02,  ...,  3.2129e-01,\n",
       "            1.9263e-01,  2.0233e-02],\n",
       "          [-8.5388e-02, -1.8042e-01, -3.0243e-02,  ...,  3.4058e-01,\n",
       "            1.3489e-01,  6.5613e-02],\n",
       "          [-1.0925e-01, -1.6589e-01, -9.8816e-02,  ...,  3.7891e-01,\n",
       "            1.5039e-01, -2.5635e-03]],\n",
       "\n",
       "         [[-2.5940e-04,  7.1754e-03,  2.8801e-03,  ...,  5.2643e-03,\n",
       "            1.4130e-02,  2.9469e-03],\n",
       "          [ 7.1411e-02,  1.7261e-01, -8.9722e-02,  ..., -1.2274e-01,\n",
       "           -1.7822e-01,  2.8516e-01],\n",
       "          [-5.2673e-02,  1.5930e-01,  2.3621e-02,  ...,  2.1484e-01,\n",
       "           -2.6001e-01,  1.4502e-01],\n",
       "          ...,\n",
       "          [-4.9658e-01,  1.6309e-01,  2.4634e-01,  ...,  8.0444e-02,\n",
       "            2.8833e-01, -1.3794e-01],\n",
       "          [-4.5435e-01,  1.5869e-01,  2.1912e-01,  ...,  1.0883e-01,\n",
       "            2.2717e-01, -7.9590e-02],\n",
       "          [-3.8428e-01,  1.7212e-01,  1.3635e-01,  ..., -1.2207e-03,\n",
       "            2.3743e-01, -7.0496e-02]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.7151e-02, -1.6052e-02, -1.2619e-02,  ..., -2.2217e-01,\n",
       "            1.7031e+00, -2.4512e-01],\n",
       "          [ 1.5723e+00,  5.9961e-01,  1.4189e+00,  ...,  2.8613e-01,\n",
       "           -4.6758e+00,  1.1006e+00],\n",
       "          [ 2.8633e+00, -3.1406e+00,  1.6621e+00,  ...,  6.0254e-01,\n",
       "           -5.5195e+00, -6.0010e-01],\n",
       "          ...,\n",
       "          [-1.4102e+00,  3.0273e-01, -7.8320e-01,  ...,  1.6768e+00,\n",
       "           -4.6641e+00,  2.7368e-01],\n",
       "          [-1.7559e+00, -7.9102e-01, -6.4697e-01,  ..., -7.7637e-02,\n",
       "           -5.7891e+00,  6.3184e-01],\n",
       "          [ 4.4238e-01, -1.9844e+00,  9.2773e-02,  ...,  3.5205e-01,\n",
       "           -5.5898e+00, -1.9287e-01]],\n",
       "\n",
       "         [[-2.8320e-02,  2.5452e-02,  7.6294e-03,  ..., -5.9766e-01,\n",
       "           -1.1682e-01, -5.6305e-02],\n",
       "          [ 2.3887e+00, -5.7520e-01, -8.1201e-01,  ...,  1.0010e+00,\n",
       "            5.1855e-01,  1.1292e-01],\n",
       "          [ 8.7207e-01, -5.5078e-01, -1.8867e+00,  ...,  1.5918e-01,\n",
       "            1.8091e-01, -1.6431e-01],\n",
       "          ...,\n",
       "          [-1.1611e+00, -1.2656e+00,  9.2676e-01,  ...,  1.8223e+00,\n",
       "            2.0532e-01, -1.4648e+00],\n",
       "          [-1.0791e+00, -9.5947e-02,  5.7959e-01,  ..., -3.6914e-01,\n",
       "           -3.4326e-01,  6.3379e-01],\n",
       "          [ 1.2129e+00,  1.3047e+00,  1.4316e+00,  ..., -4.6045e-01,\n",
       "            4.3091e-01, -6.8262e-01]],\n",
       "\n",
       "         [[-3.2410e-02,  1.3611e-02, -3.6621e-03,  ...,  1.4209e-01,\n",
       "            1.6016e-01, -9.6436e-02],\n",
       "          [ 1.3799e+00, -1.3037e+00,  1.0889e+00,  ...,  2.3083e-01,\n",
       "            1.6230e+00,  2.3242e+00],\n",
       "          [ 8.0908e-01, -2.0586e+00,  2.0566e+00,  ..., -8.2715e-01,\n",
       "            1.2461e+00,  2.5547e+00],\n",
       "          ...,\n",
       "          [-9.7119e-01, -1.1553e+00, -4.4971e-01,  ..., -1.1543e+00,\n",
       "           -1.2002e+00,  4.6533e-01],\n",
       "          [-1.9502e+00, -1.0967e+00, -6.7773e-01,  ..., -7.3975e-01,\n",
       "           -1.3350e+00,  1.8984e+00],\n",
       "          [-1.4746e-01, -1.0518e+00, -7.3438e-01,  ..., -1.7988e+00,\n",
       "           -5.1562e-01,  1.8867e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.2217e-02,  2.6588e-03, -9.6436e-03,  ..., -1.2139e+00,\n",
       "           -3.4668e-01, -1.9202e-01],\n",
       "          [ 5.5127e-01,  4.0869e-01,  1.2656e+00,  ...,  2.9238e+00,\n",
       "            1.3730e+00, -1.1904e+00],\n",
       "          [ 1.8496e+00,  2.1953e+00,  1.3721e+00,  ...,  2.5898e+00,\n",
       "           -5.5908e-02, -2.4951e-01],\n",
       "          ...,\n",
       "          [ 7.1289e-02,  3.9551e-01, -1.6201e+00,  ...,  5.8281e+00,\n",
       "            2.9932e-01, -1.6211e+00],\n",
       "          [-1.2520e+00, -8.4534e-02, -1.1289e+00,  ...,  3.6777e+00,\n",
       "            8.5840e-01,  2.1753e-01],\n",
       "          [-1.9219e+00,  1.9922e-01,  3.5547e-01,  ...,  4.1172e+00,\n",
       "            2.6484e+00, -3.5919e-02]],\n",
       "\n",
       "         [[-1.3824e-02, -3.5461e-02,  6.7139e-04,  ..., -8.6548e-02,\n",
       "            2.4146e-01, -2.5830e-01],\n",
       "          [-1.1572e-01, -6.2354e-01,  3.6426e-01,  ...,  7.3242e-01,\n",
       "           -5.6445e-01,  1.4805e+00],\n",
       "          [ 3.4180e-03, -8.4619e-01,  6.5723e-01,  ..., -1.3242e+00,\n",
       "            2.8320e-01,  1.8809e+00],\n",
       "          ...,\n",
       "          [ 7.4609e-01, -3.2349e-01,  2.0776e-01,  ..., -1.5635e+00,\n",
       "            9.2773e-02, -1.9922e+00],\n",
       "          [ 7.8076e-01, -3.1763e-01, -1.3171e-01,  ..., -6.1426e-01,\n",
       "           -1.4590e+00, -3.2520e-01],\n",
       "          [ 1.5234e-01,  1.7615e-01, -1.9165e-01,  ...,  5.1025e-01,\n",
       "           -2.3477e+00,  1.8350e+00]],\n",
       "\n",
       "         [[-3.3447e-02, -3.2654e-02, -6.7139e-03,  ...,  4.8187e-02,\n",
       "            1.4880e-01,  9.0698e-02],\n",
       "          [ 1.8750e+00,  1.8477e+00, -6.7773e-01,  ...,  1.6289e+00,\n",
       "            1.3477e+00, -2.2302e-01],\n",
       "          [-3.9746e-01,  2.1230e+00, -1.5889e+00,  ...,  3.8989e-01,\n",
       "            1.8467e+00,  5.8594e-01],\n",
       "          ...,\n",
       "          [-1.6523e+00,  2.1777e+00,  1.2520e+00,  ...,  1.3789e+00,\n",
       "            1.1484e+00,  4.3945e-01],\n",
       "          [-5.3027e-01,  5.8398e-01,  1.4883e+00,  ...,  5.2246e-01,\n",
       "           -6.0840e-01,  9.5703e-02],\n",
       "          [ 1.3125e+00,  8.7109e-01,  1.1719e-02,  ..., -7.4072e-01,\n",
       "            3.7891e-01,  1.7033e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.7151e-02, -1.6052e-02, -1.2619e-02,  ..., -2.2217e-01,\n",
       "            1.7031e+00, -2.4512e-01],\n",
       "          [ 1.5723e+00,  5.9961e-01,  1.4189e+00,  ...,  2.8613e-01,\n",
       "           -4.6758e+00,  1.1006e+00],\n",
       "          [ 3.0762e+00, -2.3809e+00,  1.3467e+00,  ...,  4.8535e-01,\n",
       "           -4.3320e+00, -1.1211e+00],\n",
       "          ...,\n",
       "          [-1.7227e+00,  4.1016e-01, -1.0586e+00,  ..., -4.3335e-01,\n",
       "           -3.9258e+00, -1.3604e+00],\n",
       "          [-1.6855e+00, -6.4453e-01, -8.1201e-01,  ..., -2.0459e-01,\n",
       "           -3.8672e+00, -1.3008e+00],\n",
       "          [-6.1035e-02, -1.4316e+00, -8.6914e-02,  ..., -1.4893e-01,\n",
       "           -3.8281e+00, -1.3955e+00]],\n",
       "\n",
       "         [[-2.8320e-02,  2.5452e-02,  7.6294e-03,  ..., -5.9766e-01,\n",
       "           -1.1682e-01, -5.6305e-02],\n",
       "          [ 2.3887e+00, -5.7520e-01, -8.1201e-01,  ...,  1.0010e+00,\n",
       "            5.1855e-01,  1.1292e-01],\n",
       "          [ 9.8633e-01, -8.2031e-01, -2.7344e+00,  ...,  1.0879e+00,\n",
       "            1.2871e+00, -3.3740e-01],\n",
       "          ...,\n",
       "          [-2.5117e+00, -5.4785e-01,  2.7344e-01,  ..., -5.2686e-01,\n",
       "            5.2930e-01, -2.9922e-02],\n",
       "          [-1.1230e+00,  4.3164e-01,  4.1895e-01,  ..., -6.3379e-01,\n",
       "            3.0225e-01, -1.6760e-01],\n",
       "          [ 1.1504e+00,  9.1357e-01,  4.5312e-01,  ..., -8.1836e-01,\n",
       "            5.8643e-01, -1.1359e-01]],\n",
       "\n",
       "         [[-3.2410e-02,  1.3611e-02, -3.6621e-03,  ...,  1.4209e-01,\n",
       "            1.6016e-01, -9.6436e-02],\n",
       "          [ 1.3799e+00, -1.3037e+00,  1.0889e+00,  ...,  2.3083e-01,\n",
       "            1.6230e+00,  2.3242e+00],\n",
       "          [ 7.1875e-01, -2.2129e+00,  1.8672e+00,  ..., -4.7461e-01,\n",
       "            7.0215e-01,  2.1328e+00],\n",
       "          ...,\n",
       "          [-1.7148e+00, -3.4473e-01, -1.1553e+00,  ..., -1.2881e+00,\n",
       "           -5.7587e-02,  2.4082e+00],\n",
       "          [-1.3828e+00, -8.6670e-01, -7.4609e-01,  ..., -1.1816e+00,\n",
       "            1.8219e-02,  2.3223e+00],\n",
       "          [ 2.8809e-01, -8.6719e-01, -5.6152e-02,  ..., -1.1455e+00,\n",
       "           -1.0083e-01,  2.3594e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.2217e-02,  2.6588e-03, -9.6436e-03,  ..., -1.2139e+00,\n",
       "           -3.4668e-01, -1.9202e-01],\n",
       "          [ 5.5127e-01,  4.0869e-01,  1.2656e+00,  ...,  2.9238e+00,\n",
       "            1.3730e+00, -1.1904e+00],\n",
       "          [ 2.4277e+00,  2.9531e+00,  1.6240e+00,  ...,  2.1562e+00,\n",
       "            2.2644e-02, -2.7100e-01],\n",
       "          ...,\n",
       "          [ 3.1787e-01,  7.8955e-01, -2.1934e+00,  ...,  1.8574e+00,\n",
       "            1.3789e+00,  2.3022e-01],\n",
       "          [-1.5352e+00,  6.1768e-01, -1.2568e+00,  ...,  1.9609e+00,\n",
       "            1.2783e+00,  1.6870e-01],\n",
       "          [-2.0078e+00,  7.7637e-02,  3.7842e-01,  ...,  1.9775e+00,\n",
       "            1.3086e+00,  2.3291e-01]],\n",
       "\n",
       "         [[-1.3824e-02, -3.5461e-02,  6.7139e-04,  ..., -8.6548e-02,\n",
       "            2.4146e-01, -2.5830e-01],\n",
       "          [-1.1572e-01, -6.2354e-01,  3.6426e-01,  ...,  7.3242e-01,\n",
       "           -5.6445e-01,  1.4805e+00],\n",
       "          [-2.6904e-01, -8.9111e-01,  1.8311e-01,  ...,  2.2754e-01,\n",
       "            1.5039e-01,  2.6836e+00],\n",
       "          ...,\n",
       "          [ 8.8428e-01, -4.6729e-01,  1.4819e-01,  ...,  1.1289e+00,\n",
       "           -2.1895e+00,  1.6074e+00],\n",
       "          [ 1.1445e+00, -4.5605e-01,  3.4326e-01,  ...,  8.1250e-01,\n",
       "           -2.3594e+00,  1.5703e+00],\n",
       "          [ 2.4341e-01, -1.4661e-01,  2.9932e-01,  ...,  8.8281e-01,\n",
       "           -2.3906e+00,  1.7383e+00]],\n",
       "\n",
       "         [[-3.3447e-02, -3.2654e-02, -6.7139e-03,  ...,  4.8187e-02,\n",
       "            1.4880e-01,  9.0698e-02],\n",
       "          [ 1.8750e+00,  1.8477e+00, -6.7773e-01,  ...,  1.6289e+00,\n",
       "            1.3477e+00, -2.2302e-01],\n",
       "          [-7.0459e-01,  2.0684e+00, -1.9004e+00,  ...,  1.2305e+00,\n",
       "            1.5781e+00,  2.3340e-01],\n",
       "          ...,\n",
       "          [-1.1641e+00,  6.5576e-01,  1.1152e+00,  ...,  1.9482e-01,\n",
       "            3.8892e-01,  8.7207e-01],\n",
       "          [-6.8787e-02,  6.4746e-01,  1.0576e+00,  ...,  5.3192e-02,\n",
       "            3.7744e-01,  9.9902e-01],\n",
       "          [ 1.1055e+00,  2.6025e-01,  5.7520e-01,  ...,  3.3875e-02,\n",
       "            6.0352e-01,  9.2578e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-2.6035e-03, -5.1880e-03, -1.1505e-02,  ..., -1.3962e-02,\n",
       "            2.1362e-03,  6.9542e-03],\n",
       "          [-4.2480e-01, -9.6533e-01, -3.4473e-01,  ..., -6.0547e-01,\n",
       "            5.8594e-01,  9.4238e-02],\n",
       "          [-6.5527e-01, -9.1553e-02,  4.0356e-01,  ...,  3.6206e-01,\n",
       "           -8.3936e-01, -4.9976e-01],\n",
       "          ...,\n",
       "          [-3.1836e-01, -3.9331e-01,  5.9277e-01,  ...,  3.9307e-01,\n",
       "            4.3915e-02,  1.0557e+00],\n",
       "          [ 6.1035e-02, -3.6646e-01,  1.2732e-01,  ..., -2.5928e-01,\n",
       "           -1.3892e-01,  2.0435e-01],\n",
       "          [-6.5430e-01, -2.0142e-01, -1.3940e-01,  ...,  2.5269e-01,\n",
       "            3.1104e-01, -8.6768e-01]],\n",
       "\n",
       "         [[ 3.5572e-03, -1.8406e-03, -2.0645e-02,  ..., -5.2490e-03,\n",
       "            1.1070e-02,  6.8130e-03],\n",
       "          [ 3.0884e-01,  6.2793e-01, -2.5879e-01,  ...,  1.0931e-01,\n",
       "           -1.9177e-01, -1.7847e-01],\n",
       "          [ 2.1338e-01,  9.5398e-02, -3.6426e-01,  ..., -1.9690e-01,\n",
       "           -1.2939e-01, -1.1023e-01],\n",
       "          ...,\n",
       "          [-1.6016e-01, -6.3477e-02, -3.9600e-01,  ...,  1.3916e-01,\n",
       "           -1.3684e-01,  4.7754e-01],\n",
       "          [-3.1586e-02,  4.9463e-01, -2.0386e-02,  ..., -1.8542e-01,\n",
       "            7.8906e-01, -5.0928e-01],\n",
       "          [ 2.0679e-01, -1.9238e-01,  1.1719e-01,  ..., -4.1748e-02,\n",
       "            2.3193e-02,  2.0801e-01]],\n",
       "\n",
       "         [[-1.6739e-02,  9.8419e-04, -7.4768e-03,  ...,  1.9493e-03,\n",
       "           -4.6806e-03,  5.7983e-03],\n",
       "          [-4.4727e-01,  2.4121e-01,  6.6406e-02,  ...,  1.1841e-02,\n",
       "            1.5417e-01,  7.5256e-02],\n",
       "          [ 2.0557e-01,  1.4246e-01, -1.9653e-01,  ..., -5.3271e-01,\n",
       "           -4.0991e-01,  2.9932e-01],\n",
       "          ...,\n",
       "          [ 3.3716e-01, -9.1064e-02, -1.0913e-01,  ..., -9.2896e-02,\n",
       "           -5.9131e-01,  2.9785e-01],\n",
       "          [-1.3794e-01, -2.0593e-01, -1.2549e-01,  ..., -6.1279e-01,\n",
       "           -1.9312e-01, -1.4966e-01],\n",
       "          [-2.6709e-01,  1.4709e-01,  1.1345e-02,  ..., -1.4136e-01,\n",
       "           -2.7686e-01,  2.6428e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.2817e-03, -5.1956e-03,  8.2092e-03,  ...,  4.3144e-03,\n",
       "            7.4539e-03, -2.0943e-03],\n",
       "          [-9.2725e-01, -8.5205e-01,  4.3579e-01,  ..., -2.2070e-01,\n",
       "           -6.6846e-01,  7.2852e-01],\n",
       "          [-3.0151e-01,  1.2207e-01, -3.6255e-01,  ..., -4.9219e-01,\n",
       "            3.8159e-01, -5.7129e-01],\n",
       "          ...,\n",
       "          [ 6.9336e-02, -4.7607e-01, -1.8280e-02,  ..., -2.4939e-01,\n",
       "           -5.2686e-01, -1.3916e-01],\n",
       "          [-3.6804e-02, -2.7808e-01, -1.0748e-01,  ..., -9.1309e-02,\n",
       "           -4.2310e-01,  1.4209e-01],\n",
       "          [-2.7417e-01, -3.4882e-02,  9.5642e-02,  ..., -2.1338e-01,\n",
       "           -2.5803e-02,  7.4072e-01]],\n",
       "\n",
       "         [[-1.3596e-02, -1.0452e-02,  1.4038e-02,  ...,  1.8082e-03,\n",
       "           -2.9793e-03, -1.0277e-02],\n",
       "          [ 1.4221e-02,  1.8799e-01,  3.7012e-01,  ...,  3.5400e-01,\n",
       "           -8.6426e-02,  5.9967e-02],\n",
       "          [-3.8483e-02,  2.8223e-01,  1.1572e-01,  ..., -4.2969e-01,\n",
       "            3.5675e-02,  3.0591e-01],\n",
       "          ...,\n",
       "          [ 3.3350e-01, -2.6294e-01, -9.2188e-01,  ..., -2.6270e-01,\n",
       "            9.1370e-02, -3.9282e-01],\n",
       "          [-2.9370e-01,  2.3926e-01, -4.7705e-01,  ..., -1.4343e-01,\n",
       "            3.2080e-01, -4.9536e-01],\n",
       "          [-9.4788e-02,  1.4172e-01, -9.2773e-03,  ...,  7.0679e-02,\n",
       "           -4.6295e-02, -5.5084e-02]],\n",
       "\n",
       "         [[ 2.0885e-03, -3.0518e-05, -2.2945e-03,  ..., -1.3374e-02,\n",
       "           -1.5411e-02,  1.5778e-02],\n",
       "          [-2.7490e-01, -2.0950e-02,  3.9612e-02,  ..., -3.9185e-02,\n",
       "           -1.2842e-01, -4.9042e-02],\n",
       "          [ 2.1558e-01, -2.2095e-01, -2.2058e-01,  ...,  4.8706e-02,\n",
       "           -3.1006e-01, -4.9408e-02],\n",
       "          ...,\n",
       "          [-1.0950e-01,  1.5649e-01,  1.2000e-01,  ...,  1.7737e-01,\n",
       "            2.6465e-01, -3.0469e-01],\n",
       "          [ 8.4595e-02, -1.5234e-01,  2.2253e-01,  ...,  1.4722e-01,\n",
       "           -3.7988e-01, -1.0919e-01],\n",
       "          [ 1.7426e-02,  1.4636e-01,  1.6052e-01,  ...,  2.7466e-01,\n",
       "           -3.2153e-01, -2.3071e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.6035e-03, -5.1880e-03, -1.1505e-02,  ..., -1.3962e-02,\n",
       "            2.1362e-03,  6.9542e-03],\n",
       "          [-4.2480e-01, -9.6533e-01, -3.4473e-01,  ..., -6.0547e-01,\n",
       "            5.8594e-01,  9.4238e-02],\n",
       "          [-1.3467e+00,  3.6182e-01, -2.1082e-01,  ..., -1.7310e-01,\n",
       "           -5.5664e-01, -1.2871e+00],\n",
       "          ...,\n",
       "          [-3.0664e-01, -8.5388e-02, -4.4531e-01,  ...,  6.2012e-02,\n",
       "            5.0781e-01, -2.3511e-01],\n",
       "          [-2.8320e-01, -1.9080e-01, -3.1689e-01,  ...,  4.8676e-02,\n",
       "            5.5713e-01, -2.4597e-01],\n",
       "          [-1.1377e-01, -2.2034e-01, -3.8330e-01,  ..., -7.2937e-03,\n",
       "            5.2930e-01, -2.1558e-01]],\n",
       "\n",
       "         [[ 3.5572e-03, -1.8406e-03, -2.0645e-02,  ..., -5.2490e-03,\n",
       "            1.1070e-02,  6.8130e-03],\n",
       "          [ 3.0884e-01,  6.2793e-01, -2.5879e-01,  ...,  1.0931e-01,\n",
       "           -1.9177e-01, -1.7847e-01],\n",
       "          [ 1.2708e-01, -2.5970e-02, -2.9810e-01,  ..., -8.6133e-01,\n",
       "            4.0131e-02, -4.0479e-01],\n",
       "          ...,\n",
       "          [ 8.9478e-02, -1.3684e-01, -3.4760e-02,  ..., -8.4106e-02,\n",
       "           -9.4299e-02, -1.5063e-01],\n",
       "          [ 1.6809e-01, -2.0007e-01,  6.2073e-02,  ..., -1.6431e-01,\n",
       "           -1.4978e-01, -1.3452e-01],\n",
       "          [ 3.2043e-02, -1.8298e-01,  6.2805e-02,  ..., -1.5771e-01,\n",
       "           -1.4429e-01, -9.9548e-02]],\n",
       "\n",
       "         [[-1.6739e-02,  9.8419e-04, -7.4768e-03,  ...,  1.9493e-03,\n",
       "           -4.6806e-03,  5.7983e-03],\n",
       "          [-4.4727e-01,  2.4121e-01,  6.6406e-02,  ...,  1.1841e-02,\n",
       "            1.5417e-01,  7.5256e-02],\n",
       "          [ 1.0797e-01, -1.8408e-01, -3.3423e-01,  ...,  3.0396e-01,\n",
       "            2.7905e-01,  2.3584e-01],\n",
       "          ...,\n",
       "          [-1.2671e-01,  5.3613e-01, -9.1797e-02,  ..., -2.4976e-01,\n",
       "            2.1240e-01,  2.0984e-01],\n",
       "          [-1.4136e-01,  6.2646e-01, -1.0608e-01,  ..., -2.0398e-01,\n",
       "            1.7834e-01,  2.0679e-01],\n",
       "          [-1.2048e-01,  5.1367e-01, -1.3965e-01,  ..., -1.4587e-01,\n",
       "            1.5918e-01,  1.4819e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.2817e-03, -5.1956e-03,  8.2092e-03,  ...,  4.3144e-03,\n",
       "            7.4539e-03, -2.0943e-03],\n",
       "          [-9.2725e-01, -8.5205e-01,  4.3579e-01,  ..., -2.2070e-01,\n",
       "           -6.6846e-01,  7.2852e-01],\n",
       "          [-5.3516e-01, -4.3750e-01, -3.9893e-01,  ..., -7.0947e-01,\n",
       "            5.7275e-01, -6.7578e-01],\n",
       "          ...,\n",
       "          [-1.3770e-01, -2.5049e-01,  8.8806e-02,  ..., -1.4984e-02,\n",
       "            2.9272e-01,  1.5588e-01],\n",
       "          [-9.0637e-02, -1.3989e-01,  6.8237e-02,  ...,  3.3325e-02,\n",
       "            2.8394e-01,  2.0715e-01],\n",
       "          [-1.0876e-01, -2.3047e-01,  7.9529e-02,  ...,  2.6596e-02,\n",
       "            2.7368e-01,  1.8152e-01]],\n",
       "\n",
       "         [[-1.3596e-02, -1.0452e-02,  1.4038e-02,  ...,  1.8082e-03,\n",
       "           -2.9793e-03, -1.0277e-02],\n",
       "          [ 1.4221e-02,  1.8799e-01,  3.7012e-01,  ...,  3.5400e-01,\n",
       "           -8.6426e-02,  5.9967e-02],\n",
       "          [ 1.7871e-01,  2.0874e-01,  3.8574e-02,  ..., -2.9517e-01,\n",
       "            1.8140e-01,  1.9946e-01],\n",
       "          ...,\n",
       "          [ 7.0068e-02, -1.0791e-01,  2.9785e-01,  ..., -6.7627e-02,\n",
       "            2.2302e-01, -3.4351e-01],\n",
       "          [ 3.4943e-02, -1.0333e-01,  3.2739e-01,  ..., -9.9182e-03,\n",
       "            1.8750e-01, -2.8296e-01],\n",
       "          [ 4.5502e-02, -6.9458e-02,  3.4863e-01,  ..., -4.1870e-02,\n",
       "            2.0276e-01, -2.5244e-01]],\n",
       "\n",
       "         [[ 2.0885e-03, -3.0518e-05, -2.2945e-03,  ..., -1.3374e-02,\n",
       "           -1.5411e-02,  1.5778e-02],\n",
       "          [-2.7490e-01, -2.0950e-02,  3.9612e-02,  ..., -3.9185e-02,\n",
       "           -1.2842e-01, -4.9042e-02],\n",
       "          [ 7.0610e-03, -6.7017e-02,  2.2607e-01,  ..., -4.6570e-02,\n",
       "           -1.1646e-01,  2.9449e-02],\n",
       "          ...,\n",
       "          [ 2.1753e-01,  3.9444e-03,  1.6125e-01,  ...,  2.9907e-03,\n",
       "           -2.2095e-01, -3.8037e-01],\n",
       "          [ 1.3843e-01,  3.2990e-02,  1.4294e-01,  ..., -4.0649e-02,\n",
       "           -1.8640e-01, -4.3237e-01],\n",
       "          [ 2.3438e-01,  4.9408e-02,  1.2384e-01,  ..., -5.4749e-02,\n",
       "           -1.7798e-01, -3.7476e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 2.0447e-02,  3.0518e-04,  2.4765e-02,  ..., -1.0870e-01,\n",
       "           -1.3647e-01,  4.4556e-03],\n",
       "          [-2.0410e+00, -1.2285e+00,  1.1807e+00,  ..., -5.2344e-01,\n",
       "            1.2480e+00, -1.7871e+00],\n",
       "          [-2.3438e+00,  1.5518e+00,  1.5811e+00,  ...,  3.7622e-01,\n",
       "            7.0068e-01,  1.3760e+00],\n",
       "          ...,\n",
       "          [ 2.7324e+00,  7.0068e-01, -1.2021e+00,  ..., -2.4121e+00,\n",
       "           -2.7852e+00, -1.6875e+00],\n",
       "          [ 2.2285e+00,  8.9209e-01,  7.7515e-02,  ...,  2.3340e+00,\n",
       "            5.3772e-02,  1.6855e+00],\n",
       "          [-7.2266e-02,  7.7734e-01, -8.1152e-01,  ...,  2.6562e+00,\n",
       "            7.3047e-01,  1.6084e+00]],\n",
       "\n",
       "         [[-2.9327e-02, -2.8320e-02,  1.4771e-02,  ...,  3.0853e-02,\n",
       "            1.4160e-01,  2.1655e-01],\n",
       "          [-1.8232e+00,  1.0303e-01, -3.9004e+00,  ...,  1.1810e-01,\n",
       "            7.8662e-01, -1.2964e-01],\n",
       "          [-5.8672e+00, -2.9707e+00, -2.2598e+00,  ..., -7.0801e-01,\n",
       "            6.3525e-01, -5.4150e-01],\n",
       "          ...,\n",
       "          [ 5.5762e-01, -8.3789e-01,  2.4219e+00,  ..., -3.7915e-01,\n",
       "           -8.6670e-01,  1.0156e+00],\n",
       "          [ 2.5879e+00, -2.2070e+00,  1.8457e+00,  ...,  2.5171e-01,\n",
       "           -2.1680e-01, -2.5977e-01],\n",
       "          [ 3.5293e+00, -4.1836e+00,  1.1250e+00,  ..., -1.7908e-01,\n",
       "            2.1240e-02,  7.2571e-02]],\n",
       "\n",
       "         [[-3.1433e-02, -3.0823e-02,  1.9653e-02,  ..., -2.5537e-01,\n",
       "           -2.0996e-01,  7.8125e-03],\n",
       "          [-1.5635e+00,  1.7793e+00, -2.6211e+00,  ..., -8.3008e-02,\n",
       "           -1.9219e+00,  9.1406e-01],\n",
       "          [-3.9453e+00,  3.0879e+00, -6.7871e-01,  ..., -3.5571e-01,\n",
       "           -2.2402e+00, -7.9346e-01],\n",
       "          ...,\n",
       "          [ 1.8906e+00,  2.3438e+00,  1.1729e+00,  ..., -7.3438e-01,\n",
       "           -7.0312e-01, -4.0332e-01],\n",
       "          [ 2.0059e+00,  1.2314e+00,  2.1875e-01,  ..., -1.4326e+00,\n",
       "           -2.0117e+00, -5.3223e-01],\n",
       "          [ 2.1445e+00,  7.4072e-01, -2.6172e+00,  ..., -1.4355e+00,\n",
       "           -2.3184e+00, -1.5742e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.5635e-03,  5.7373e-03, -9.2545e-03,  ...,  2.4731e-01,\n",
       "           -1.7812e+00, -5.3741e-02],\n",
       "          [ 1.0420e+00, -1.7383e-01, -6.2988e-01,  ..., -1.4043e+00,\n",
       "            5.7734e+00, -2.8457e+00],\n",
       "          [-3.9185e-02, -1.1650e+00, -5.3174e-01,  ..., -1.8896e+00,\n",
       "            4.7695e+00, -1.9521e+00],\n",
       "          ...,\n",
       "          [-4.0015e-01, -2.0862e-01, -4.9170e-01,  ...,  7.8174e-01,\n",
       "            6.0469e+00,  5.8545e-01],\n",
       "          [ 8.0225e-01, -3.2910e-01, -1.2510e+00,  ...,  2.8271e-01,\n",
       "            4.8828e+00, -2.1191e-01],\n",
       "          [ 1.3721e+00, -3.5596e-01, -6.6895e-01,  ..., -1.7407e-01,\n",
       "            6.2383e+00,  1.4922e+00]],\n",
       "\n",
       "         [[-1.3916e-02,  3.9276e-02, -1.4526e-02,  ...,  1.1584e-01,\n",
       "            4.0527e-01,  3.4424e-01],\n",
       "          [ 6.6895e-01,  8.8989e-02,  5.0586e-01,  ...,  8.7402e-01,\n",
       "            4.3125e+00,  1.0547e+00],\n",
       "          [ 4.6973e-01, -7.8760e-01, -8.1848e-02,  ...,  1.5303e+00,\n",
       "            3.5293e+00,  2.5146e-01],\n",
       "          ...,\n",
       "          [-1.1846e+00, -6.6406e-01, -6.1572e-01,  ...,  5.3076e-01,\n",
       "           -1.4917e-01, -1.0078e+00],\n",
       "          [-1.6006e+00,  1.2122e-01, -1.3037e+00,  ..., -5.6299e-01,\n",
       "            8.6719e-01, -7.9688e-01],\n",
       "          [ 2.4707e-01,  2.0032e-01, -5.0293e-01,  ...,  2.0068e-01,\n",
       "            6.0156e-01,  3.2812e-01]],\n",
       "\n",
       "         [[ 3.1128e-03,  4.4983e-02,  1.2695e-02,  ..., -3.0716e-02,\n",
       "            1.4124e-01,  1.1670e+00],\n",
       "          [-1.1426e+00, -6.6650e-02,  1.9629e-01,  ...,  3.0273e-01,\n",
       "           -6.3184e-01, -2.3418e+00],\n",
       "          [-5.1758e-01,  4.7534e-01,  4.5020e-01,  ...,  6.2988e-01,\n",
       "            3.2739e-01, -3.0820e+00],\n",
       "          ...,\n",
       "          [ 2.1765e-01, -6.5918e-02, -2.1631e-01,  ...,  1.5586e+00,\n",
       "           -2.8867e+00, -3.0039e+00],\n",
       "          [ 9.4775e-01,  1.0876e-01, -5.5273e-01,  ...,  1.6260e-01,\n",
       "           -3.7549e-01, -2.5918e+00],\n",
       "          [ 4.0918e-01,  9.7900e-01, -1.0713e+00,  ...,  6.0889e-01,\n",
       "            7.5781e-01, -2.9668e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0447e-02,  3.0518e-04,  2.4765e-02,  ..., -1.0870e-01,\n",
       "           -1.3647e-01,  4.4556e-03],\n",
       "          [-2.0410e+00, -1.2285e+00,  1.1807e+00,  ..., -5.2344e-01,\n",
       "            1.2480e+00, -1.7871e+00],\n",
       "          [-2.6992e+00,  1.2041e+00,  1.9512e+00,  ...,  9.0771e-01,\n",
       "            1.0586e+00,  1.0176e+00],\n",
       "          ...,\n",
       "          [ 2.5938e+00, -9.7363e-01, -7.9297e-01,  ...,  4.1211e+00,\n",
       "            1.2213e-01,  7.5488e-01],\n",
       "          [ 2.0605e+00,  3.5620e-01, -8.1445e-01,  ...,  3.8242e+00,\n",
       "           -1.2108e-02,  8.0127e-01],\n",
       "          [-3.2617e-01,  1.5801e+00, -5.9912e-01,  ...,  3.9766e+00,\n",
       "           -1.9751e-01,  8.3789e-01]],\n",
       "\n",
       "         [[-2.9327e-02, -2.8320e-02,  1.4771e-02,  ...,  3.0853e-02,\n",
       "            1.4160e-01,  2.1655e-01],\n",
       "          [-1.8232e+00,  1.0303e-01, -3.9004e+00,  ...,  1.1810e-01,\n",
       "            7.8662e-01, -1.2964e-01],\n",
       "          [-5.8945e+00, -3.0469e+00, -2.2754e+00,  ...,  5.6445e-01,\n",
       "            2.7124e-01,  3.7207e-01],\n",
       "          ...,\n",
       "          [ 1.0410e+00, -1.1201e+00,  2.6484e+00,  ..., -3.0249e-01,\n",
       "            2.0728e-01,  2.2461e-01],\n",
       "          [ 3.6367e+00, -2.9707e+00,  2.1191e+00,  ..., -3.2275e-01,\n",
       "            4.3701e-01,  9.6680e-02],\n",
       "          [ 2.9141e+00, -3.0898e+00,  8.2861e-01,  ..., -3.7329e-01,\n",
       "            4.1797e-01,  1.3916e-01]],\n",
       "\n",
       "         [[-3.1433e-02, -3.0823e-02,  1.9653e-02,  ..., -2.5537e-01,\n",
       "           -2.0996e-01,  7.8125e-03],\n",
       "          [-1.5635e+00,  1.7793e+00, -2.6211e+00,  ..., -8.3008e-02,\n",
       "           -1.9219e+00,  9.1406e-01],\n",
       "          [-3.7539e+00,  3.1172e+00, -1.2900e+00,  ..., -1.3635e-01,\n",
       "           -2.1445e+00, -8.0322e-01],\n",
       "          ...,\n",
       "          [ 1.2373e+00,  1.1416e+00,  1.9316e+00,  ..., -1.0469e+00,\n",
       "           -2.4316e+00, -5.5469e-01],\n",
       "          [ 2.7051e+00,  1.3779e+00,  3.6914e-01,  ..., -1.0840e+00,\n",
       "           -2.3926e+00, -5.4492e-01],\n",
       "          [ 1.6826e+00,  8.8477e-01, -1.2852e+00,  ..., -1.1055e+00,\n",
       "           -2.4434e+00, -5.0342e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.5635e-03,  5.7373e-03, -9.2545e-03,  ...,  2.4731e-01,\n",
       "           -1.7812e+00, -5.3741e-02],\n",
       "          [ 1.0420e+00, -1.7383e-01, -6.2988e-01,  ..., -1.4043e+00,\n",
       "            5.7734e+00, -2.8457e+00],\n",
       "          [-8.5449e-02, -1.0254e+00,  8.8330e-01,  ..., -1.3789e+00,\n",
       "            5.3047e+00, -1.7354e+00],\n",
       "          ...,\n",
       "          [-1.3496e+00, -8.8281e-01, -5.3564e-01,  ...,  2.3682e-01,\n",
       "            5.6484e+00,  7.1338e-01],\n",
       "          [ 3.4668e-01, -8.3350e-01, -5.7715e-01,  ...,  1.6418e-01,\n",
       "            5.7109e+00,  7.2217e-01],\n",
       "          [ 1.6484e+00, -2.7490e-01, -3.1592e-01,  ...,  2.3718e-01,\n",
       "            5.5938e+00,  5.4736e-01]],\n",
       "\n",
       "         [[-1.3916e-02,  3.9276e-02, -1.4526e-02,  ...,  1.1584e-01,\n",
       "            4.0527e-01,  3.4424e-01],\n",
       "          [ 6.6895e-01,  8.8989e-02,  5.0586e-01,  ...,  8.7402e-01,\n",
       "            4.3125e+00,  1.0547e+00],\n",
       "          [ 1.4307e-01, -9.0771e-01,  1.4050e-01,  ...,  1.6426e+00,\n",
       "            2.3652e+00, -9.2627e-01],\n",
       "          ...,\n",
       "          [-1.3848e+00, -7.9736e-01, -6.4844e-01,  ...,  7.2070e-01,\n",
       "            1.3403e-01,  2.1973e-01],\n",
       "          [-1.0771e+00, -1.4893e-01, -6.4404e-01,  ...,  7.0117e-01,\n",
       "            1.8823e-01,  3.1250e-01],\n",
       "          [ 3.0078e-01,  5.2686e-01, -5.0928e-01,  ...,  7.3633e-01,\n",
       "            2.9297e-01,  3.0664e-01]],\n",
       "\n",
       "         [[ 3.1128e-03,  4.4983e-02,  1.2695e-02,  ..., -3.0716e-02,\n",
       "            1.4124e-01,  1.1670e+00],\n",
       "          [-1.1426e+00, -6.6650e-02,  1.9629e-01,  ...,  3.0273e-01,\n",
       "           -6.3184e-01, -2.3418e+00],\n",
       "          [-1.1172e+00,  8.4229e-01,  4.4238e-01,  ...,  2.9419e-01,\n",
       "           -3.8477e-01, -3.3516e+00],\n",
       "          ...,\n",
       "          [ 1.2764e+00, -4.2480e-01, -2.0947e-01,  ...,  1.2490e+00,\n",
       "           -2.7344e-01, -2.3516e+00],\n",
       "          [ 1.4229e+00,  5.4736e-01, -9.0381e-01,  ...,  1.3691e+00,\n",
       "           -4.6704e-01, -2.3379e+00],\n",
       "          [ 4.3872e-01,  1.1426e+00, -1.2383e+00,  ...,  1.3125e+00,\n",
       "           -4.9023e-01, -2.2422e+00]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 5.0278e-03, -1.0513e-02,  4.6616e-03,  ...,  5.8212e-03,\n",
       "            1.0643e-02, -1.1520e-03],\n",
       "          [-6.8970e-03,  2.9565e-01,  1.2036e-01,  ...,  7.5073e-02,\n",
       "           -3.0127e-01, -1.7932e-01],\n",
       "          [-1.6199e-01, -2.8271e-01, -1.2238e-01,  ...,  4.7302e-02,\n",
       "            2.4719e-02,  2.4194e-01],\n",
       "          ...,\n",
       "          [ 1.2070e-02,  1.1505e-01,  5.5817e-02,  ..., -6.9824e-01,\n",
       "            5.2881e-01,  1.1438e-01],\n",
       "          [-4.0649e-01,  2.8564e-01,  5.4980e-01,  ..., -3.6450e-01,\n",
       "            3.4229e-01,  8.5510e-02],\n",
       "          [ 1.3269e-01,  1.9116e-01,  1.9714e-02,  ..., -2.1924e-01,\n",
       "            2.5439e-01, -3.4546e-02]],\n",
       "\n",
       "         [[-6.1226e-03,  4.1847e-03,  1.0452e-02,  ...,  3.1052e-03,\n",
       "           -1.1925e-02,  1.1101e-02],\n",
       "          [ 1.2622e-01,  2.5293e-01,  8.1238e-02,  ...,  2.9028e-01,\n",
       "           -3.4302e-02,  3.3875e-02],\n",
       "          [ 3.3051e-02, -1.1841e-01,  4.2999e-02,  ..., -1.2109e-01,\n",
       "            2.5903e-01,  1.2225e-01],\n",
       "          ...,\n",
       "          [ 6.6113e-01, -7.2607e-01,  2.1729e-01,  ..., -5.6934e-01,\n",
       "            2.3999e-01, -2.7002e-01],\n",
       "          [-8.7708e-02, -1.6260e-01,  2.5342e-01,  ..., -4.0479e-01,\n",
       "           -3.0298e-01, -5.7129e-02],\n",
       "          [ 3.1836e-01,  5.0873e-02, -3.4131e-01,  ..., -9.0088e-02,\n",
       "           -1.7578e-01, -1.0535e-01]],\n",
       "\n",
       "         [[-4.1351e-03, -2.9449e-03, -4.3640e-03,  ...,  9.0027e-03,\n",
       "            1.6510e-02, -9.7046e-03],\n",
       "          [-4.5947e-01,  2.6025e-01, -3.6206e-01,  ..., -1.6394e-01,\n",
       "            2.2729e-01, -3.0640e-01],\n",
       "          [ 5.7861e-02, -5.1514e-02, -9.1748e-01,  ...,  2.8369e-01,\n",
       "            9.0759e-02,  3.5095e-02],\n",
       "          ...,\n",
       "          [ 2.9541e-01,  1.9775e-01, -1.6809e-01,  ..., -2.9126e-01,\n",
       "           -4.4458e-01,  1.9727e-01],\n",
       "          [ 6.7810e-02,  1.1377e-01, -6.3721e-01,  ...,  4.7632e-01,\n",
       "           -2.6550e-02,  5.3662e-01],\n",
       "          [ 1.0944e-01,  2.2070e-01, -7.1094e-01,  ...,  8.3923e-02,\n",
       "           -1.5417e-01,  2.3901e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.0813e-02,  4.4479e-03,  7.1259e-03,  ...,  2.8763e-03,\n",
       "            1.6663e-02,  5.0354e-03],\n",
       "          [ 1.3389e+00, -8.4229e-01, -2.2009e-01,  ..., -4.2041e-01,\n",
       "           -4.3030e-03, -9.5996e-01],\n",
       "          [ 1.5979e-01,  4.4830e-02,  8.9783e-02,  ..., -3.8013e-01,\n",
       "           -5.6445e-01, -4.8462e-01],\n",
       "          ...,\n",
       "          [-4.6899e-01, -3.2935e-01,  9.5154e-02,  ...,  1.1749e-01,\n",
       "           -3.2227e-02, -5.1074e-01],\n",
       "          [ 4.2041e-01, -8.3313e-03,  6.4453e-02,  ..., -9.0210e-02,\n",
       "           -9.4543e-02,  5.6213e-02],\n",
       "          [-2.3499e-01, -3.1421e-01,  6.9434e-01,  ..., -3.3643e-01,\n",
       "            6.4404e-01, -4.7339e-01]],\n",
       "\n",
       "         [[ 1.5961e-02, -3.0136e-04, -2.0813e-02,  ..., -7.5722e-04,\n",
       "           -1.6464e-02,  1.9058e-02],\n",
       "          [ 5.0537e-01, -3.1641e-01, -6.0498e-01,  ..., -2.8076e-01,\n",
       "           -1.7651e-01,  5.1807e-01],\n",
       "          [-3.0103e-01,  3.6768e-01, -4.1309e-01,  ...,  1.5784e-01,\n",
       "           -1.4490e-01, -4.5593e-02],\n",
       "          ...,\n",
       "          [ 8.1848e-02,  4.0039e-01,  7.0953e-03,  ..., -2.2205e-01,\n",
       "            4.0253e-02,  2.4976e-01],\n",
       "          [ 1.1542e-01,  3.9941e-01, -2.4414e-02,  ..., -1.8567e-01,\n",
       "           -1.6138e-01, -2.1472e-01],\n",
       "          [ 1.0193e-01, -1.7432e-01, -7.9773e-02,  ...,  9.4360e-02,\n",
       "           -3.0975e-03, -1.3763e-02]],\n",
       "\n",
       "         [[-1.5419e-02,  2.7180e-03, -8.6288e-03,  ..., -4.3640e-03,\n",
       "           -4.5700e-03, -1.1978e-03],\n",
       "          [ 1.2695e-01, -4.9438e-03, -2.7002e-01,  ..., -1.6943e-01,\n",
       "            5.1483e-02,  4.6875e-01],\n",
       "          [-2.7124e-01, -7.7539e-01,  7.4902e-01,  ...,  8.2324e-01,\n",
       "           -3.7793e-01,  1.0088e+00],\n",
       "          ...,\n",
       "          [-3.9697e-01,  1.7358e-01, -2.3950e-01,  ..., -5.7129e-01,\n",
       "            5.9082e-01,  8.9355e-02],\n",
       "          [-1.8127e-01,  6.3867e-01,  2.8442e-01,  ..., -4.5898e-01,\n",
       "            2.3022e-01, -9.9854e-02],\n",
       "          [ 5.6213e-02,  4.4995e-01, -3.3862e-01,  ...,  3.3276e-01,\n",
       "            5.1270e-01, -2.6050e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.0278e-03, -1.0513e-02,  4.6616e-03,  ...,  5.8212e-03,\n",
       "            1.0643e-02, -1.1520e-03],\n",
       "          [-6.8970e-03,  2.9565e-01,  1.2036e-01,  ...,  7.5073e-02,\n",
       "           -3.0127e-01, -1.7932e-01],\n",
       "          [-1.7371e-01, -1.7847e-01,  4.5117e-01,  ..., -1.9946e-01,\n",
       "           -6.2256e-02, -4.0985e-02],\n",
       "          ...,\n",
       "          [-1.2276e-02,  1.9629e-01,  1.8665e-01,  ..., -1.7395e-01,\n",
       "            1.2421e-02,  1.1792e-01],\n",
       "          [ 5.2032e-03,  2.1655e-01,  1.7505e-01,  ..., -1.1560e-01,\n",
       "           -4.3152e-02,  1.3989e-01],\n",
       "          [-7.0496e-02,  2.0215e-01,  1.8164e-01,  ..., -9.6375e-02,\n",
       "           -4.4861e-03,  1.2390e-01]],\n",
       "\n",
       "         [[-6.1226e-03,  4.1847e-03,  1.0452e-02,  ...,  3.1052e-03,\n",
       "           -1.1925e-02,  1.1101e-02],\n",
       "          [ 1.2622e-01,  2.5293e-01,  8.1238e-02,  ...,  2.9028e-01,\n",
       "           -3.4302e-02,  3.3875e-02],\n",
       "          [-1.0767e-01, -1.9141e-01, -7.2754e-02,  ..., -2.1277e-01,\n",
       "           -2.1521e-01,  4.1309e-01],\n",
       "          ...,\n",
       "          [-1.4917e-01, -1.7102e-01, -3.9551e-01,  ..., -6.9214e-02,\n",
       "            2.2717e-01,  2.6001e-01],\n",
       "          [-1.1218e-01, -1.0193e-01, -4.8560e-01,  ..., -2.1942e-02,\n",
       "            2.5024e-01,  2.5342e-01],\n",
       "          [-1.0553e-01, -1.2354e-01, -4.5776e-01,  ..., -8.7402e-02,\n",
       "            2.2314e-01,  2.4890e-01]],\n",
       "\n",
       "         [[-4.1351e-03, -2.9449e-03, -4.3640e-03,  ...,  9.0027e-03,\n",
       "            1.6510e-02, -9.7046e-03],\n",
       "          [-4.5947e-01,  2.6025e-01, -3.6206e-01,  ..., -1.6394e-01,\n",
       "            2.2729e-01, -3.0640e-01],\n",
       "          [ 1.6382e-01,  1.5503e-02, -9.9121e-01,  ...,  5.6213e-02,\n",
       "            1.2646e-01,  1.9543e-01],\n",
       "          ...,\n",
       "          [ 1.4740e-02,  7.0312e-02, -6.9482e-01,  ...,  3.5352e-01,\n",
       "            2.5220e-01,  3.3862e-01],\n",
       "          [-3.2959e-03,  5.9387e-02, -6.9580e-01,  ...,  3.0566e-01,\n",
       "            1.6895e-01,  3.3545e-01],\n",
       "          [-8.6975e-03,  6.8604e-02, -6.7920e-01,  ...,  3.2910e-01,\n",
       "            1.5649e-01,  3.0737e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.0813e-02,  4.4479e-03,  7.1259e-03,  ...,  2.8763e-03,\n",
       "            1.6663e-02,  5.0354e-03],\n",
       "          [ 1.3389e+00, -8.4229e-01, -2.2009e-01,  ..., -4.2041e-01,\n",
       "           -4.3030e-03, -9.5996e-01],\n",
       "          [ 8.4900e-02,  3.3203e-01,  9.2957e-02,  ...,  1.7358e-01,\n",
       "           -1.1578e-01, -1.0479e+00],\n",
       "          ...,\n",
       "          [-2.0996e-01, -2.8613e-01,  1.4685e-01,  ...,  4.5093e-01,\n",
       "            5.6885e-01, -8.5254e-01],\n",
       "          [-2.4060e-01, -2.9199e-01,  1.6650e-01,  ...,  4.6753e-01,\n",
       "            6.1133e-01, -7.4268e-01],\n",
       "          [-1.3745e-01, -2.8931e-01,  1.7896e-01,  ...,  4.7656e-01,\n",
       "            4.1943e-01, -7.8125e-01]],\n",
       "\n",
       "         [[ 1.5961e-02, -3.0136e-04, -2.0813e-02,  ..., -7.5722e-04,\n",
       "           -1.6464e-02,  1.9058e-02],\n",
       "          [ 5.0537e-01, -3.1641e-01, -6.0498e-01,  ..., -2.8076e-01,\n",
       "           -1.7651e-01,  5.1807e-01],\n",
       "          [ 3.1714e-01, -2.6562e-01, -2.1326e-01,  ...,  5.4541e-01,\n",
       "            2.5195e-01,  6.3770e-01],\n",
       "          ...,\n",
       "          [-4.4891e-02,  1.8234e-02,  2.1851e-02,  ...,  3.5278e-01,\n",
       "           -2.7441e-01, -2.7710e-01],\n",
       "          [-1.3428e-02, -3.2654e-02, -2.3193e-02,  ...,  3.4644e-01,\n",
       "           -2.1411e-01, -3.2129e-01],\n",
       "          [ 2.5604e-02,  9.7809e-03, -1.3641e-02,  ...,  4.0332e-01,\n",
       "           -1.6833e-01, -2.6172e-01]],\n",
       "\n",
       "         [[-1.5419e-02,  2.7180e-03, -8.6288e-03,  ..., -4.3640e-03,\n",
       "           -4.5700e-03, -1.1978e-03],\n",
       "          [ 1.2695e-01, -4.9438e-03, -2.7002e-01,  ..., -1.6943e-01,\n",
       "            5.1483e-02,  4.6875e-01],\n",
       "          [-3.0396e-01, -1.0859e+00,  1.0381e+00,  ..., -3.1372e-01,\n",
       "           -6.8066e-01, -4.5776e-03],\n",
       "          ...,\n",
       "          [-1.4868e-01, -3.5675e-02, -1.4026e-01,  ...,  1.1761e-01,\n",
       "            3.4131e-01, -3.4363e-02],\n",
       "          [-2.1265e-01, -5.3833e-02, -1.4331e-01,  ...,  1.0950e-01,\n",
       "            2.5708e-01, -7.2266e-02],\n",
       "          [-1.9116e-01, -3.9124e-02, -1.4771e-01,  ...,  3.2654e-02,\n",
       "            2.8125e-01, -1.4124e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.1738e-03, -7.7515e-03,  3.6621e-03,  ..., -1.0223e-03,\n",
       "           -8.5388e-02, -1.4233e-01],\n",
       "          [-4.5142e-01, -7.1045e-02,  1.3037e-01,  ..., -1.4170e+00,\n",
       "            7.9407e-02,  9.7363e-01],\n",
       "          [ 1.0400e+00, -1.1338e+00,  1.2866e-01,  ..., -2.8076e-01,\n",
       "            4.3164e-01, -2.2046e-01],\n",
       "          ...,\n",
       "          [-1.5771e-01,  4.3408e-01, -5.5176e-01,  ...,  1.3955e+00,\n",
       "           -1.0977e+00, -9.1504e-01],\n",
       "          [-8.1201e-01,  5.6201e-01, -4.9390e-01,  ...,  6.4453e-02,\n",
       "           -4.9341e-01, -1.1729e+00],\n",
       "          [-7.1143e-01,  1.3257e-01,  3.5205e-01,  ..., -1.1924e+00,\n",
       "            6.4795e-01,  2.0288e-01]],\n",
       "\n",
       "         [[-6.7444e-03,  1.3992e-02,  8.2397e-04,  ...,  1.8164e-01,\n",
       "            2.1948e-01,  3.9429e-02],\n",
       "          [-1.3926e+00, -5.7861e-01,  3.0908e-01,  ..., -1.8975e+00,\n",
       "            1.1401e-01,  1.0273e+00],\n",
       "          [-1.2227e+00, -7.0953e-03, -2.5787e-03,  ...,  4.0625e-01,\n",
       "           -2.2156e-01,  8.5254e-01],\n",
       "          ...,\n",
       "          [ 5.6055e-01, -2.7930e-01, -2.8320e-01,  ...,  1.9980e+00,\n",
       "            2.0410e+00, -6.0449e-01],\n",
       "          [ 1.5215e+00, -1.3994e+00, -4.1553e-01,  ...,  1.4531e+00,\n",
       "           -5.3613e-01, -1.3184e-01],\n",
       "          [ 1.7920e-01, -9.7559e-01, -7.4341e-02,  ...,  2.5000e+00,\n",
       "            8.2227e-01,  9.0723e-01]],\n",
       "\n",
       "         [[-3.1677e-02, -3.1250e-02,  1.1475e-02,  ...,  1.4014e-01,\n",
       "            2.0020e-01, -9.1431e-02],\n",
       "          [ 4.0552e-01,  6.9434e-01, -1.5654e+00,  ..., -1.2549e+00,\n",
       "            8.7012e-01, -7.9541e-01],\n",
       "          [ 2.7031e+00,  2.4766e+00, -1.5420e+00,  ..., -4.1309e-01,\n",
       "            6.1035e-01,  2.2607e-01],\n",
       "          ...,\n",
       "          [-1.0498e+00,  6.9238e-01,  1.7500e+00,  ...,  1.2539e+00,\n",
       "           -2.6035e+00,  1.7891e+00],\n",
       "          [-1.4902e+00,  1.1602e+00,  1.2998e+00,  ..., -3.6816e-01,\n",
       "           -1.4766e+00,  1.8604e-01],\n",
       "          [-2.1250e+00,  1.1377e+00,  6.4648e-01,  ..., -7.9004e-01,\n",
       "            6.6699e-01, -2.4780e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3168e-02,  3.7231e-03, -9.3536e-03,  ...,  1.9336e-01,\n",
       "            1.4368e-01,  2.0234e+00],\n",
       "          [-1.4697e-01,  4.6655e-01,  4.4373e-02,  ..., -5.2881e-01,\n",
       "            1.6562e+00, -1.9697e+00],\n",
       "          [-3.5059e+00,  4.6924e-01, -1.8477e+00,  ..., -9.5312e-01,\n",
       "            3.5693e-01, -4.1680e+00],\n",
       "          ...,\n",
       "          [ 5.5469e-01, -2.7881e-01,  2.2363e-01,  ...,  2.0462e-02,\n",
       "            3.4229e-01, -8.1875e+00],\n",
       "          [ 3.1895e+00,  1.0049e+00,  1.0557e+00,  ..., -2.0723e+00,\n",
       "            3.7183e-01, -5.3594e+00],\n",
       "          [ 9.8828e-01, -4.5679e-01,  2.0195e+00,  ...,  8.5742e-01,\n",
       "            2.2278e-01, -4.2773e+00]],\n",
       "\n",
       "         [[ 1.6449e-02,  2.2110e-02,  3.6530e-02,  ..., -1.8726e-01,\n",
       "           -2.6929e-01, -2.9736e-01],\n",
       "          [ 4.6570e-02,  1.5308e-01,  4.7949e-01,  ...,  7.2070e-01,\n",
       "            7.3633e-01, -1.7651e-01],\n",
       "          [ 5.9521e-01,  7.8857e-01,  4.9780e-01,  ..., -2.0459e-01,\n",
       "           -1.5234e+00,  4.8877e-01],\n",
       "          ...,\n",
       "          [-2.7344e-01,  4.7217e-01,  5.7861e-02,  ...,  2.0801e+00,\n",
       "           -1.8389e+00,  8.9160e-01],\n",
       "          [-1.2021e+00,  1.4355e+00, -1.2500e+00,  ...,  4.4727e-01,\n",
       "           -4.4873e-01,  1.0186e+00],\n",
       "          [-8.8086e-01,  8.1299e-01, -1.0117e+00,  ...,  1.3857e+00,\n",
       "           -2.8906e+00,  2.8574e+00]],\n",
       "\n",
       "         [[ 3.3051e-02,  2.9022e-02, -2.2339e-02,  ..., -1.6699e-01,\n",
       "            1.0327e-01,  2.4902e-01],\n",
       "          [ 1.4385e+00,  7.2266e-01,  2.5527e+00,  ...,  6.5527e-01,\n",
       "           -1.2598e+00,  2.6031e-02],\n",
       "          [ 3.1211e+00,  2.1621e+00,  3.0234e+00,  ...,  1.0059e+00,\n",
       "           -1.9443e+00,  6.4307e-01],\n",
       "          ...,\n",
       "          [-2.8613e-01, -1.3066e+00, -2.8594e+00,  ..., -8.7402e-01,\n",
       "           -1.7676e+00,  1.9141e+00],\n",
       "          [-2.3457e+00,  1.8535e+00, -1.4102e+00,  ..., -3.9868e-01,\n",
       "           -9.4873e-01, -9.1248e-02],\n",
       "          [-2.3047e+00,  3.6895e+00, -5.3906e-01,  ...,  1.1973e+00,\n",
       "           -2.4316e+00, -4.7882e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 3.1738e-03, -7.7515e-03,  3.6621e-03,  ..., -1.0223e-03,\n",
       "           -8.5388e-02, -1.4233e-01],\n",
       "          [-4.5142e-01, -7.1045e-02,  1.3037e-01,  ..., -1.4170e+00,\n",
       "            7.9407e-02,  9.7363e-01],\n",
       "          [ 1.1055e+00, -9.5764e-02, -1.6284e-01,  ..., -1.9971e+00,\n",
       "            1.3340e+00, -9.7900e-01],\n",
       "          ...,\n",
       "          [ 5.5762e-01,  2.1948e-01, -4.1016e-02,  ..., -9.4531e-01,\n",
       "            3.3740e-01, -2.7686e-01],\n",
       "          [-2.6196e-01,  7.9736e-01,  2.4536e-01,  ..., -9.3994e-01,\n",
       "            2.8320e-01, -3.0054e-01],\n",
       "          [-9.6191e-01,  8.5352e-01,  4.0015e-01,  ..., -9.1211e-01,\n",
       "            2.6099e-01, -4.5312e-01]],\n",
       "\n",
       "         [[-6.7444e-03,  1.3992e-02,  8.2397e-04,  ...,  1.8164e-01,\n",
       "            2.1948e-01,  3.9429e-02],\n",
       "          [-1.3926e+00, -5.7861e-01,  3.0908e-01,  ..., -1.8975e+00,\n",
       "            1.1401e-01,  1.0273e+00],\n",
       "          [-9.7949e-01, -2.5635e-03, -5.1465e-01,  ..., -3.9990e-01,\n",
       "           -1.3945e+00,  6.6016e-01],\n",
       "          ...,\n",
       "          [ 2.3633e-01, -6.7188e-01, -3.4180e-01,  ...,  2.7207e+00,\n",
       "            6.5283e-01, -5.2344e-01],\n",
       "          [ 4.4727e-01, -9.4629e-01, -1.1047e-01,  ...,  2.8008e+00,\n",
       "            6.0205e-01, -5.7520e-01],\n",
       "          [ 2.2107e-01, -6.3232e-01,  1.0254e-01,  ...,  2.8145e+00,\n",
       "            6.4453e-01, -4.7778e-01]],\n",
       "\n",
       "         [[-3.1677e-02, -3.1250e-02,  1.1475e-02,  ...,  1.4014e-01,\n",
       "            2.0020e-01, -9.1431e-02],\n",
       "          [ 4.0552e-01,  6.9434e-01, -1.5654e+00,  ..., -1.2549e+00,\n",
       "            8.7012e-01, -7.9541e-01],\n",
       "          [ 2.2715e+00,  1.7246e+00, -1.3770e+00,  ..., -1.1309e+00,\n",
       "            2.7344e-02,  4.5117e-01],\n",
       "          ...,\n",
       "          [ 6.0254e-01,  6.4258e-01,  1.0762e+00,  ..., -2.0410e-01,\n",
       "           -2.5073e-01, -5.0342e-01],\n",
       "          [-6.5479e-01,  5.1270e-01,  1.0967e+00,  ..., -1.1084e-01,\n",
       "           -8.0444e-02, -3.3765e-01],\n",
       "          [-1.3350e+00, -3.5156e-02,  6.1475e-01,  ..., -9.0332e-02,\n",
       "            3.8818e-02, -3.7891e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3168e-02,  3.7231e-03, -9.3536e-03,  ...,  1.9336e-01,\n",
       "            1.4368e-01,  2.0234e+00],\n",
       "          [-1.4697e-01,  4.6655e-01,  4.4373e-02,  ..., -5.2881e-01,\n",
       "            1.6562e+00, -1.9697e+00],\n",
       "          [-2.7188e+00,  3.5675e-02, -1.6445e+00,  ..., -2.4609e+00,\n",
       "            3.0249e-01, -3.7324e+00],\n",
       "          ...,\n",
       "          [ 8.8770e-01,  5.0684e-01,  1.1895e+00,  ...,  1.0625e+00,\n",
       "            1.3748e-02, -3.5586e+00],\n",
       "          [ 2.3438e+00,  3.9990e-01,  1.5850e+00,  ...,  1.2949e+00,\n",
       "           -6.0730e-02, -3.4844e+00],\n",
       "          [ 1.6973e+00,  1.3403e-01,  1.2773e+00,  ...,  1.2461e+00,\n",
       "           -1.4783e-01, -3.5430e+00]],\n",
       "\n",
       "         [[ 1.6449e-02,  2.2110e-02,  3.6530e-02,  ..., -1.8726e-01,\n",
       "           -2.6929e-01, -2.9736e-01],\n",
       "          [ 4.6570e-02,  1.5308e-01,  4.7949e-01,  ...,  7.2070e-01,\n",
       "            7.3633e-01, -1.7651e-01],\n",
       "          [ 1.0664e+00,  1.0596e+00,  6.2402e-01,  ..., -1.2158e+00,\n",
       "           -6.9287e-01, -2.0007e-01],\n",
       "          ...,\n",
       "          [-8.4961e-02,  6.4209e-01, -8.3203e-01,  ...,  2.2031e+00,\n",
       "           -9.6777e-01,  2.7344e+00],\n",
       "          [-5.4736e-01,  7.3926e-01, -7.7734e-01,  ...,  2.2227e+00,\n",
       "           -1.2305e+00,  2.7520e+00],\n",
       "          [-5.2881e-01,  5.0391e-01, -4.5508e-01,  ...,  2.2598e+00,\n",
       "           -1.2744e+00,  2.8379e+00]],\n",
       "\n",
       "         [[ 3.3051e-02,  2.9022e-02, -2.2339e-02,  ..., -1.6699e-01,\n",
       "            1.0327e-01,  2.4902e-01],\n",
       "          [ 1.4385e+00,  7.2266e-01,  2.5527e+00,  ...,  6.5527e-01,\n",
       "           -1.2598e+00,  2.6031e-02],\n",
       "          [ 2.8965e+00,  1.9180e+00,  3.0566e+00,  ...,  1.1943e+00,\n",
       "           -1.8477e+00,  2.4585e-01],\n",
       "          ...,\n",
       "          [-4.6729e-01, -8.1836e-01, -2.3574e+00,  ...,  1.5635e+00,\n",
       "           -1.8721e+00, -3.9795e-01],\n",
       "          [-2.3809e+00,  1.8125e+00, -1.8604e+00,  ...,  1.5498e+00,\n",
       "           -1.6016e+00, -4.1699e-01],\n",
       "          [-1.9688e+00,  3.3633e+00, -4.8926e-01,  ...,  1.6455e+00,\n",
       "           -1.6846e+00, -4.5557e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-1.5945e-02, -2.7466e-03, -2.3193e-03,  ..., -1.0864e-02,\n",
       "           -4.4708e-03,  3.5858e-04],\n",
       "          [-3.9380e-01, -7.8491e-02, -2.5830e-01,  ...,  2.5146e-01,\n",
       "            1.9421e-01,  8.8672e-01],\n",
       "          [-3.8452e-01,  1.6516e-01, -1.2939e-02,  ...,  6.3354e-02,\n",
       "           -8.4082e-01,  3.6346e-02],\n",
       "          ...,\n",
       "          [ 4.0820e-01, -6.3428e-01,  1.6956e-01,  ...,  4.4751e-01,\n",
       "           -2.8809e-01,  2.6245e-01],\n",
       "          [ 2.3413e-01,  5.1318e-01,  2.9541e-02,  ..., -3.4485e-03,\n",
       "           -8.2092e-02,  8.3154e-01],\n",
       "          [-4.5483e-01,  5.9131e-01,  2.4084e-01,  ...,  1.1609e-01,\n",
       "           -2.3193e-01,  3.6743e-02]],\n",
       "\n",
       "         [[ 1.3351e-02, -5.1613e-03,  8.4305e-03,  ...,  1.6403e-03,\n",
       "            4.2191e-03, -4.2915e-03],\n",
       "          [-6.5283e-01,  3.6499e-01,  5.0926e-03,  ..., -3.8257e-01,\n",
       "           -4.9097e-01, -4.1992e-01],\n",
       "          [-2.1836e+00, -7.8809e-01,  1.0968e-01,  ..., -1.1456e-01,\n",
       "            6.5869e-01,  9.0234e-01],\n",
       "          ...,\n",
       "          [-4.1626e-02,  6.5063e-02, -6.9031e-02,  ...,  8.8135e-01,\n",
       "           -6.4258e-01,  7.0459e-01],\n",
       "          [ 2.4109e-01,  1.9360e-01,  2.4988e-01,  ..., -2.7359e-02,\n",
       "           -3.9307e-01,  3.1641e-01],\n",
       "          [ 8.5205e-02, -5.8887e-01, -2.6611e-01,  ...,  6.6162e-02,\n",
       "            2.8247e-01, -1.8127e-01]],\n",
       "\n",
       "         [[-4.3221e-03,  1.0887e-02, -1.8127e-02,  ...,  1.2047e-02,\n",
       "            1.3733e-04,  1.0460e-02],\n",
       "          [ 3.1763e-01, -3.0591e-01, -7.4036e-02,  ...,  2.9321e-01,\n",
       "           -2.2009e-01,  2.7075e-01],\n",
       "          [-3.5461e-02,  2.1143e-01, -9.4604e-04,  ..., -3.8501e-01,\n",
       "           -3.0469e-01,  2.7808e-01],\n",
       "          ...,\n",
       "          [-7.2461e-01,  1.3843e-01, -1.6144e-02,  ..., -2.5854e-01,\n",
       "           -1.8530e-01,  1.0602e-01],\n",
       "          [ 1.7883e-02,  9.1431e-02, -1.9434e-01,  ...,  3.2886e-01,\n",
       "           -2.4506e-02,  3.3887e-01],\n",
       "          [-2.1753e-01,  1.1694e-01,  2.3407e-02,  ...,  2.0935e-01,\n",
       "            3.5425e-01,  2.9077e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.3755e-03, -7.2632e-03,  8.5983e-03,  ..., -8.7261e-05,\n",
       "           -5.4169e-04, -1.0063e-02],\n",
       "          [-3.4961e-01,  3.4717e-01,  5.0635e-01,  ..., -2.8101e-01,\n",
       "           -2.0972e-01,  1.6858e-01],\n",
       "          [ 2.2864e-01,  7.6660e-01, -3.0371e-01,  ...,  6.3574e-01,\n",
       "            3.8550e-01,  3.0103e-01],\n",
       "          ...,\n",
       "          [ 1.6309e-01,  1.3037e-01,  2.6221e-01,  ...,  9.2871e-01,\n",
       "            3.0518e-01, -3.0688e-01],\n",
       "          [-2.2241e-01, -2.8900e-02,  2.6440e-01,  ..., -5.0568e-02,\n",
       "           -1.9385e-01, -5.8167e-02],\n",
       "          [-3.2275e-01, -7.7148e-02,  4.7070e-01,  ..., -4.1504e-01,\n",
       "           -5.1611e-01,  5.9424e-01]],\n",
       "\n",
       "         [[-8.8501e-03,  1.1063e-04,  1.6861e-03,  ...,  1.8646e-02,\n",
       "           -1.4740e-02,  3.3455e-03],\n",
       "          [-1.7834e-01, -2.4829e-01,  3.6548e-01,  ..., -3.6694e-01,\n",
       "            1.7212e-01, -2.3315e-02],\n",
       "          [ 9.4971e-02,  2.3669e-01,  8.9355e-02,  ...,  3.8965e-01,\n",
       "            1.6632e-02,  9.1003e-02],\n",
       "          ...,\n",
       "          [-1.6382e-01,  2.6953e-01,  5.1855e-01,  ..., -7.3975e-02,\n",
       "            4.4067e-02,  1.3025e-01],\n",
       "          [-1.0138e-01, -5.6610e-02,  5.0415e-02,  ..., -1.5637e-01,\n",
       "           -4.2450e-02,  2.3816e-01],\n",
       "          [-5.0537e-01,  9.8511e-02,  1.1951e-01,  ...,  7.5317e-02,\n",
       "           -2.6880e-01,  2.9736e-01]],\n",
       "\n",
       "         [[-3.3951e-04, -1.4496e-02, -4.7112e-03,  ...,  6.1798e-04,\n",
       "           -2.1942e-02, -1.0757e-03],\n",
       "          [ 4.2603e-01, -1.2012e-01, -2.7539e-01,  ...,  4.4647e-02,\n",
       "           -4.8096e-01, -3.0249e-01],\n",
       "          [ 5.2393e-01, -1.8726e-01,  1.6858e-01,  ...,  2.3718e-01,\n",
       "            1.7487e-02,  3.1860e-01],\n",
       "          ...,\n",
       "          [-3.1201e-01,  5.5029e-01, -3.8184e-01,  ...,  2.8613e-01,\n",
       "            5.1318e-01,  2.9785e-01],\n",
       "          [-5.4346e-01, -3.6938e-01,  1.1224e-01,  ...,  3.1958e-01,\n",
       "            2.0557e-01,  2.7661e-01],\n",
       "          [-3.3398e-01,  2.1667e-02, -2.6587e-01,  ...,  3.7988e-01,\n",
       "           -5.1953e-01, -1.1377e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.5945e-02, -2.7466e-03, -2.3193e-03,  ..., -1.0864e-02,\n",
       "           -4.4708e-03,  3.5858e-04],\n",
       "          [-3.9380e-01, -7.8491e-02, -2.5830e-01,  ...,  2.5146e-01,\n",
       "            1.9421e-01,  8.8672e-01],\n",
       "          [-1.1121e-01, -3.7048e-02, -2.6611e-02,  ..., -5.9570e-01,\n",
       "           -9.5410e-01, -4.3335e-02],\n",
       "          ...,\n",
       "          [-4.1602e-01,  5.6689e-01,  9.0454e-02,  ..., -5.7281e-02,\n",
       "           -2.0020e-01,  1.6089e-01],\n",
       "          [-4.2041e-01,  5.6543e-01,  1.7969e-01,  ..., -4.1595e-02,\n",
       "           -2.2058e-01,  1.9336e-01],\n",
       "          [-4.4141e-01,  5.1270e-01,  1.0522e-01,  ..., -1.7242e-02,\n",
       "           -1.6235e-01,  1.9897e-01]],\n",
       "\n",
       "         [[ 1.3351e-02, -5.1613e-03,  8.4305e-03,  ...,  1.6403e-03,\n",
       "            4.2191e-03, -4.2915e-03],\n",
       "          [-6.5283e-01,  3.6499e-01,  5.0926e-03,  ..., -3.8257e-01,\n",
       "           -4.9097e-01, -4.1992e-01],\n",
       "          [ 6.8237e-02, -4.2725e-01,  1.0713e+00,  ..., -5.9570e-01,\n",
       "            1.3398e+00,  8.8477e-01],\n",
       "          ...,\n",
       "          [ 1.4966e-01,  1.5527e-01, -4.2651e-01,  ...,  5.8643e-01,\n",
       "            5.6738e-01,  1.2598e-01],\n",
       "          [ 1.3782e-01,  1.7163e-01, -4.3970e-01,  ...,  6.3818e-01,\n",
       "            4.4385e-01,  1.9165e-01],\n",
       "          [ 2.4023e-01,  2.0032e-01, -4.7095e-01,  ...,  5.8105e-01,\n",
       "            4.9170e-01,  2.5366e-01]],\n",
       "\n",
       "         [[-4.3221e-03,  1.0887e-02, -1.8127e-02,  ...,  1.2047e-02,\n",
       "            1.3733e-04,  1.0460e-02],\n",
       "          [ 3.1763e-01, -3.0591e-01, -7.4036e-02,  ...,  2.9321e-01,\n",
       "           -2.2009e-01,  2.7075e-01],\n",
       "          [-5.2930e-01,  1.9470e-01,  1.2549e-01,  ...,  3.5889e-01,\n",
       "            2.3682e-01,  3.7817e-01],\n",
       "          ...,\n",
       "          [ 8.4167e-02,  4.3701e-01,  2.1289e-01,  ...,  3.3350e-01,\n",
       "            2.5195e-01,  2.7905e-01],\n",
       "          [ 4.0955e-02,  4.8950e-01,  2.3779e-01,  ...,  3.1665e-01,\n",
       "            1.3025e-01,  2.0947e-01],\n",
       "          [-2.2064e-02,  4.6704e-01,  1.8945e-01,  ...,  3.3008e-01,\n",
       "            1.2524e-01,  2.2314e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.3755e-03, -7.2632e-03,  8.5983e-03,  ..., -8.7261e-05,\n",
       "           -5.4169e-04, -1.0063e-02],\n",
       "          [-3.4961e-01,  3.4717e-01,  5.0635e-01,  ..., -2.8101e-01,\n",
       "           -2.0972e-01,  1.6858e-01],\n",
       "          [ 8.9355e-01, -1.6614e-01, -3.8208e-01,  ...,  9.3994e-01,\n",
       "           -2.2388e-01, -1.1218e-01],\n",
       "          ...,\n",
       "          [-1.9958e-01, -2.4622e-01,  1.0215e+00,  ..., -3.2471e-01,\n",
       "           -1.9202e-01,  1.2537e-01],\n",
       "          [-2.2644e-01, -1.7773e-01,  1.0381e+00,  ..., -3.3911e-01,\n",
       "           -2.6904e-01,  7.0068e-02],\n",
       "          [-1.6040e-01, -1.2634e-01,  1.0605e+00,  ..., -2.6050e-01,\n",
       "           -2.5903e-01,  1.3672e-01]],\n",
       "\n",
       "         [[-8.8501e-03,  1.1063e-04,  1.6861e-03,  ...,  1.8646e-02,\n",
       "           -1.4740e-02,  3.3455e-03],\n",
       "          [-1.7834e-01, -2.4829e-01,  3.6548e-01,  ..., -3.6694e-01,\n",
       "            1.7212e-01, -2.3315e-02],\n",
       "          [ 7.5781e-01, -7.3242e-02,  3.9771e-01,  ..., -1.6687e-01,\n",
       "            8.8770e-01, -6.3477e-01],\n",
       "          ...,\n",
       "          [-5.2783e-01,  5.7007e-02, -9.6375e-02,  ..., -7.9956e-03,\n",
       "            3.0371e-01,  6.9153e-02],\n",
       "          [-6.1084e-01, -5.7983e-04, -2.3026e-02,  ...,  3.3386e-02,\n",
       "            3.0444e-01,  1.3306e-01],\n",
       "          [-5.5322e-01,  4.5319e-03,  1.3275e-03,  ...,  8.3435e-02,\n",
       "            2.6660e-01,  1.5930e-01]],\n",
       "\n",
       "         [[-3.3951e-04, -1.4496e-02, -4.7112e-03,  ...,  6.1798e-04,\n",
       "           -2.1942e-02, -1.0757e-03],\n",
       "          [ 4.2603e-01, -1.2012e-01, -2.7539e-01,  ...,  4.4647e-02,\n",
       "           -4.8096e-01, -3.0249e-01],\n",
       "          [ 6.4062e-01, -1.0889e-01,  2.7271e-01,  ...,  1.6211e-01,\n",
       "            9.4055e-02,  3.2104e-01],\n",
       "          ...,\n",
       "          [-5.3772e-02, -1.9006e-01, -1.7651e-01,  ...,  4.3457e-02,\n",
       "           -2.0630e-01,  1.8542e-01],\n",
       "          [-8.9111e-02, -2.9321e-01, -1.9434e-01,  ...,  1.2622e-01,\n",
       "           -1.1194e-01,  1.3611e-01],\n",
       "          [-5.3040e-02, -2.2510e-01, -1.7224e-01,  ...,  1.5344e-01,\n",
       "           -1.7969e-01,  8.8318e-02]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.4424e-02,  3.3569e-03,  8.3008e-03,  ...,  1.6040e-01,\n",
       "           -6.8741e-03,  9.6558e-02],\n",
       "          [-1.6484e+00,  1.3135e-01, -5.1465e-01,  ...,  1.3652e+00,\n",
       "           -1.3428e-02,  8.4351e-02],\n",
       "          [-7.9980e-01,  5.6836e-01, -4.8828e-02,  ...,  4.4141e+00,\n",
       "            2.0820e+00, -9.0674e-01],\n",
       "          ...,\n",
       "          [ 2.0938e+00,  4.0381e-01,  1.0684e+00,  ...,  2.2695e+00,\n",
       "            5.4688e-01, -1.3867e+00],\n",
       "          [ 1.1768e+00, -1.2598e+00,  6.6504e-01,  ...,  3.5254e+00,\n",
       "            1.8877e+00,  1.6748e-01],\n",
       "          [-3.4912e-01, -1.3828e+00, -1.8054e-01,  ...,  4.3047e+00,\n",
       "            1.7188e+00,  4.1089e-01]],\n",
       "\n",
       "         [[ 5.0659e-03, -2.1393e-02,  1.7700e-02,  ..., -1.4514e-01,\n",
       "            1.7517e-02,  4.6460e-01],\n",
       "          [-1.1221e+00,  7.7051e-01, -2.6147e-01,  ..., -5.4590e-01,\n",
       "           -1.4531e+00, -5.8008e-01],\n",
       "          [-2.1655e-01,  3.2520e-01,  4.4067e-01,  ...,  4.6045e-01,\n",
       "            7.8613e-01, -1.1973e+00],\n",
       "          ...,\n",
       "          [ 9.3018e-02,  3.5815e-01,  5.3467e-02,  ...,  6.4307e-01,\n",
       "            6.4502e-01,  1.4141e+00],\n",
       "          [ 3.5986e-01, -2.2107e-01,  3.8721e-01,  ...,  6.9336e-01,\n",
       "           -1.9453e+00,  1.2002e+00],\n",
       "          [ 1.8042e-01,  1.5039e-01,  6.5430e-01,  ...,  1.4883e+00,\n",
       "           -2.4561e-01, -1.3496e+00]],\n",
       "\n",
       "         [[-4.5166e-02,  5.6885e-02,  1.8921e-03,  ..., -1.1981e-01,\n",
       "            2.6074e-01, -1.6003e-01],\n",
       "          [ 1.7500e+00, -6.1621e-01,  1.0342e+00,  ...,  3.9673e-02,\n",
       "           -7.4951e-01,  2.5244e-01],\n",
       "          [-8.0762e-01, -9.6680e-02,  1.4551e-01,  ..., -1.0557e+00,\n",
       "            2.4878e-01, -5.6055e-01],\n",
       "          ...,\n",
       "          [-4.2773e+00, -6.1035e-01, -1.7578e-01,  ..., -2.8809e-01,\n",
       "           -2.0654e-01,  3.2886e-01],\n",
       "          [-7.5684e-01,  2.9150e-01,  4.6631e-01,  ..., -2.0691e-01,\n",
       "           -4.0771e-02, -1.2573e-01],\n",
       "          [ 2.8125e+00,  1.4258e+00,  2.4316e+00,  ..., -1.7615e-01,\n",
       "           -1.4834e+00, -3.7109e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 8.4839e-03,  3.2501e-03,  1.1337e-02,  ..., -1.3782e-01,\n",
       "           -1.6675e-01,  2.5439e-01],\n",
       "          [-4.6509e-02, -6.7505e-02, -9.4849e-02,  ..., -1.5625e+00,\n",
       "           -1.1934e+00, -5.7959e-01],\n",
       "          [ 7.4951e-01, -1.4514e-01,  5.0586e-01,  ..., -2.2930e+00,\n",
       "            2.5879e-02, -2.2412e-01],\n",
       "          ...,\n",
       "          [ 4.2114e-01,  1.9775e-02,  3.2178e-01,  ...,  2.8906e+00,\n",
       "           -1.2354e+00, -3.7207e+00],\n",
       "          [-6.4648e-01,  1.5173e-01,  1.2585e-01,  ...,  1.2227e+00,\n",
       "           -8.0225e-01, -2.0332e+00],\n",
       "          [-9.0918e-01,  2.5195e-01, -5.2393e-01,  ...,  7.6660e-01,\n",
       "           -1.5928e+00, -3.0957e+00]],\n",
       "\n",
       "         [[-1.4954e-03,  1.2512e-02, -1.7548e-02,  ..., -1.6809e-01,\n",
       "            5.7861e-02, -1.7786e-01],\n",
       "          [-1.6797e-01,  3.0786e-01,  2.7661e-01,  ..., -5.3369e-01,\n",
       "            8.2422e-01, -1.9502e+00],\n",
       "          [-8.4290e-02,  4.6753e-02,  6.0913e-02,  ...,  2.2871e+00,\n",
       "            3.6597e-01,  1.4121e+00],\n",
       "          ...,\n",
       "          [ 3.1641e-01,  3.4937e-01, -2.5635e-01,  ...,  5.2783e-01,\n",
       "            7.9492e-01, -7.7441e-01],\n",
       "          [ 3.2861e-01,  4.1162e-01, -3.4302e-01,  ...,  8.6975e-02,\n",
       "           -3.5498e-01,  1.1602e+00],\n",
       "          [ 3.4985e-01,  1.5308e-01, -1.3147e-01,  ...,  1.5488e+00,\n",
       "           -1.7031e+00,  2.4766e+00]],\n",
       "\n",
       "         [[ 5.1270e-03,  2.8809e-02, -1.1353e-02,  ...,  9.0454e-02,\n",
       "            7.3120e-02,  1.6693e-02],\n",
       "          [-2.4824e+00, -1.8721e+00,  9.7656e-04,  ...,  7.8320e-01,\n",
       "            4.2920e-01,  7.0898e-01],\n",
       "          [-2.9395e+00,  1.1338e+00, -8.9014e-01,  ...,  7.0898e-01,\n",
       "           -1.7590e-01,  1.0000e+00],\n",
       "          ...,\n",
       "          [ 1.0938e+00, -1.2695e+00,  1.1914e+00,  ...,  3.0591e-01,\n",
       "           -2.1255e-02,  8.0615e-01],\n",
       "          [ 1.6338e+00, -9.5337e-02,  1.4297e+00,  ...,  1.2148e+00,\n",
       "           -4.3066e-01,  9.0137e-01],\n",
       "          [ 7.7734e-01,  9.9316e-01,  2.8125e+00,  ...,  1.1885e+00,\n",
       "            7.6660e-01,  7.0605e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.4424e-02,  3.3569e-03,  8.3008e-03,  ...,  1.6040e-01,\n",
       "           -6.8741e-03,  9.6558e-02],\n",
       "          [-1.6484e+00,  1.3135e-01, -5.1465e-01,  ...,  1.3652e+00,\n",
       "           -1.3428e-02,  8.4351e-02],\n",
       "          [-4.4800e-01,  9.7656e-04,  7.2266e-02,  ...,  3.8730e+00,\n",
       "            2.5366e-01,  1.1597e-01],\n",
       "          ...,\n",
       "          [ 4.9854e-01, -4.9170e-01,  2.9572e-02,  ...,  5.0469e+00,\n",
       "            8.3789e-01,  1.0371e+00],\n",
       "          [ 7.8955e-01, -6.0547e-01, -1.2549e-01,  ...,  4.9531e+00,\n",
       "            9.5459e-01,  1.1260e+00],\n",
       "          [ 3.2959e-01, -3.2861e-01, -2.4927e-01,  ...,  4.9844e+00,\n",
       "            9.9170e-01,  9.2090e-01]],\n",
       "\n",
       "         [[ 5.0659e-03, -2.1393e-02,  1.7700e-02,  ..., -1.4514e-01,\n",
       "            1.7517e-02,  4.6460e-01],\n",
       "          [-1.1221e+00,  7.7051e-01, -2.6147e-01,  ..., -5.4590e-01,\n",
       "           -1.4531e+00, -5.8008e-01],\n",
       "          [-3.4448e-01,  4.6387e-01,  2.1289e-01,  ...,  1.0605e+00,\n",
       "            5.7422e-01, -7.5586e-01],\n",
       "          ...,\n",
       "          [ 4.5215e-01,  8.8379e-01,  7.7576e-02,  ...,  2.2832e+00,\n",
       "           -6.1719e-01, -9.0332e-01],\n",
       "          [ 2.7734e-01,  9.3848e-01,  2.4182e-01,  ...,  2.2324e+00,\n",
       "           -7.7539e-01, -7.3242e-01],\n",
       "          [-8.0688e-02,  3.7915e-01,  2.7441e-01,  ...,  2.2773e+00,\n",
       "           -6.8799e-01, -8.4375e-01]],\n",
       "\n",
       "         [[-4.5166e-02,  5.6885e-02,  1.8921e-03,  ..., -1.1981e-01,\n",
       "            2.6074e-01, -1.6003e-01],\n",
       "          [ 1.7500e+00, -6.1621e-01,  1.0342e+00,  ...,  3.9673e-02,\n",
       "           -7.4951e-01,  2.5244e-01],\n",
       "          [-1.2529e+00, -7.2021e-01,  1.5381e-01,  ..., -1.5703e+00,\n",
       "           -3.6499e-01, -1.5479e+00],\n",
       "          ...,\n",
       "          [-3.6992e+00, -7.9688e-01, -1.0156e+00,  ..., -1.9775e-01,\n",
       "           -5.1074e-01, -9.3164e-01],\n",
       "          [-9.1602e-01, -1.1267e-01,  8.2373e-01,  ..., -1.1914e-01,\n",
       "           -6.4014e-01, -9.2773e-01],\n",
       "          [ 2.7520e+00,  8.6377e-01,  2.3027e+00,  ..., -5.2429e-02,\n",
       "           -4.9170e-01, -1.0273e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 8.4839e-03,  3.2501e-03,  1.1337e-02,  ..., -1.3782e-01,\n",
       "           -1.6675e-01,  2.5439e-01],\n",
       "          [-4.6509e-02, -6.7505e-02, -9.4849e-02,  ..., -1.5625e+00,\n",
       "           -1.1934e+00, -5.7959e-01],\n",
       "          [ 1.0895e-01,  4.1534e-02,  3.6914e-01,  ..., -1.9316e+00,\n",
       "            7.6367e-01, -9.8633e-01],\n",
       "          ...,\n",
       "          [ 2.9614e-01,  3.5034e-02, -2.5586e-01,  ...,  1.5098e+00,\n",
       "           -1.5117e+00, -1.6787e+00],\n",
       "          [-8.4473e-01,  2.0862e-01, -4.8682e-01,  ...,  1.4785e+00,\n",
       "           -1.6084e+00, -1.5947e+00],\n",
       "          [-1.0996e+00,  2.1899e-01, -4.7388e-01,  ...,  1.4287e+00,\n",
       "           -1.5273e+00, -1.7607e+00]],\n",
       "\n",
       "         [[-1.4954e-03,  1.2512e-02, -1.7548e-02,  ..., -1.6809e-01,\n",
       "            5.7861e-02, -1.7786e-01],\n",
       "          [-1.6797e-01,  3.0786e-01,  2.7661e-01,  ..., -5.3369e-01,\n",
       "            8.2422e-01, -1.9502e+00],\n",
       "          [-9.5459e-02,  3.4375e-01,  3.2129e-01,  ...,  1.5830e+00,\n",
       "            1.7114e-01,  6.4551e-01],\n",
       "          ...,\n",
       "          [-3.6743e-02,  1.4648e-01, -9.7656e-02,  ...,  5.8203e-01,\n",
       "           -1.9453e+00,  1.9805e+00],\n",
       "          [ 1.8225e-01,  4.2896e-01, -3.9978e-02,  ...,  6.2305e-01,\n",
       "           -1.8262e+00,  2.0078e+00],\n",
       "          [ 2.2241e-01,  4.1724e-01,  8.4351e-02,  ...,  5.7129e-01,\n",
       "           -1.8369e+00,  2.0059e+00]],\n",
       "\n",
       "         [[ 5.1270e-03,  2.8809e-02, -1.1353e-02,  ...,  9.0454e-02,\n",
       "            7.3120e-02,  1.6693e-02],\n",
       "          [-2.4824e+00, -1.8721e+00,  9.7656e-04,  ...,  7.8320e-01,\n",
       "            4.2920e-01,  7.0898e-01],\n",
       "          [-2.4102e+00,  8.9697e-01, -1.0664e+00,  ...,  1.1650e+00,\n",
       "           -2.5610e-01,  1.8477e+00],\n",
       "          ...,\n",
       "          [ 2.0098e+00, -9.5312e-01,  1.5869e-01,  ...,  2.7285e+00,\n",
       "            8.6475e-01,  3.7109e-01],\n",
       "          [ 2.3047e+00, -5.0293e-01,  1.3291e+00,  ...,  2.5840e+00,\n",
       "            8.8623e-01,  5.0195e-01],\n",
       "          [ 5.8545e-01,  1.4795e-01,  2.1035e+00,  ...,  2.5293e+00,\n",
       "            8.4961e-01,  3.4424e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 3.9001e-02,  8.8501e-04,  7.6218e-03,  ...,  6.0234e-03,\n",
       "            1.8265e-02, -3.0975e-03],\n",
       "          [-6.8359e-01,  3.7354e-01,  2.3422e-02,  ...,  9.7961e-03,\n",
       "           -1.0291e-01,  9.4666e-02],\n",
       "          [ 2.5317e-01,  2.0935e-02,  2.4084e-01,  ..., -1.1267e-01,\n",
       "            3.9819e-01,  7.9575e-03],\n",
       "          ...,\n",
       "          [ 4.6704e-01, -2.6562e-01,  3.4271e-02,  ...,  1.1340e-01,\n",
       "            1.8042e-01,  1.6565e-01],\n",
       "          [-1.1926e-01, -3.4644e-01, -4.0503e-01,  ..., -2.0691e-02,\n",
       "            1.2756e-01,  1.1194e-01],\n",
       "          [ 3.7695e-01, -6.7505e-02, -3.8208e-01,  ...,  2.8027e-01,\n",
       "           -2.8027e-01, -7.7637e-01]],\n",
       "\n",
       "         [[-2.1896e-03,  1.2360e-03, -5.6305e-03,  ..., -7.5607e-03,\n",
       "           -7.4768e-04, -1.3840e-02],\n",
       "          [-1.6748e-01, -3.6224e-02, -1.0767e-01,  ...,  7.0410e-01,\n",
       "            2.9419e-01, -1.5027e-01],\n",
       "          [-1.6187e-01, -2.5537e-01,  1.1682e-01,  ...,  8.7280e-03,\n",
       "           -7.2144e-02, -4.1382e-01],\n",
       "          ...,\n",
       "          [-5.9814e-01,  1.3574e-01, -2.7222e-01,  ..., -5.7281e-02,\n",
       "           -7.7881e-01, -5.9766e-01],\n",
       "          [-3.8696e-02, -2.4646e-01, -5.1660e-01,  ..., -1.0693e-01,\n",
       "           -1.1285e-01,  7.2510e-01],\n",
       "          [ 1.9104e-01, -1.8176e-01,  3.0176e-01,  ..., -1.0706e-01,\n",
       "           -7.9102e-01,  1.2520e+00]],\n",
       "\n",
       "         [[ 1.1200e-02, -1.3458e-02,  6.3400e-03,  ...,  6.5613e-04,\n",
       "           -3.5210e-03,  7.1106e-03],\n",
       "          [-2.5073e-01,  4.5563e-02,  5.3662e-01,  ..., -1.4380e-01,\n",
       "           -2.9834e-01, -3.3862e-01],\n",
       "          [ 3.8452e-01, -2.6318e-01,  6.8909e-02,  ...,  8.9941e-01,\n",
       "            5.5634e-02, -7.0801e-03],\n",
       "          ...,\n",
       "          [-7.3059e-02,  2.7856e-01, -8.6914e-01,  ..., -1.6541e-02,\n",
       "           -1.1780e-02,  5.8008e-01],\n",
       "          [-4.2700e-01, -6.8359e-02,  8.1787e-02,  ...,  1.5625e-02,\n",
       "           -4.3262e-01,  3.2178e-01],\n",
       "          [-3.6621e-01, -3.8232e-01,  5.4346e-01,  ..., -3.5156e-01,\n",
       "           -1.5649e-01,  6.7139e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.5671e-02, -8.0795e-03,  2.5291e-03,  ...,  7.2289e-03,\n",
       "            4.8523e-02, -9.1553e-04],\n",
       "          [ 4.5923e-01, -5.4492e-01, -5.5518e-01,  ...,  2.8906e-01,\n",
       "           -6.0352e-01,  4.7583e-01],\n",
       "          [-9.4922e-01,  1.5906e-01,  1.1914e-01,  ...,  5.4248e-01,\n",
       "           -2.0874e-01, -1.9897e-01],\n",
       "          ...,\n",
       "          [ 5.2246e-02,  1.9324e-01,  1.8042e-01,  ..., -4.3945e-01,\n",
       "           -6.3867e-01, -1.6357e-01],\n",
       "          [-1.9031e-01,  2.2717e-01, -4.3188e-01,  ..., -1.9202e-01,\n",
       "           -7.2021e-02, -1.2012e-01],\n",
       "          [ 1.3904e-01, -2.1924e-01, -5.8594e-03,  ..., -1.4844e-01,\n",
       "           -6.1035e-01, -1.3757e-01]],\n",
       "\n",
       "         [[ 1.3809e-03, -1.2856e-02, -1.2321e-02,  ..., -2.7370e-03,\n",
       "            1.0925e-02,  1.4793e-02],\n",
       "          [ 1.5259e-01,  1.0156e-01,  6.3818e-01,  ...,  2.4329e-01,\n",
       "           -9.8291e-01,  5.5328e-02],\n",
       "          [-3.8525e-01,  4.1064e-01,  1.5210e-01,  ...,  1.3350e+00,\n",
       "            1.3867e+00,  4.6973e-01],\n",
       "          ...,\n",
       "          [ 8.2764e-02,  3.4399e-01,  6.6357e-01,  ..., -4.6533e-01,\n",
       "           -7.8735e-02, -1.3171e-01],\n",
       "          [ 5.0391e-01, -2.6880e-01,  1.2378e-01,  ...,  3.9355e-01,\n",
       "           -2.8418e-01, -3.8989e-01],\n",
       "          [ 1.0291e-01, -1.2476e-01, -6.0986e-01,  ..., -8.9355e-01,\n",
       "            4.7803e-01, -2.4817e-01]],\n",
       "\n",
       "         [[ 7.1640e-03, -1.4755e-02,  9.7733e-03,  ...,  6.3782e-03,\n",
       "           -1.7395e-02, -1.5656e-02],\n",
       "          [-3.3844e-02,  5.5054e-02,  1.5686e-01,  ..., -4.9170e-01,\n",
       "            7.9834e-02,  1.3281e-01],\n",
       "          [ 7.2632e-02, -2.8223e-01,  9.5947e-02,  ..., -1.2915e-01,\n",
       "            5.6732e-02,  1.9055e-01],\n",
       "          ...,\n",
       "          [ 4.9487e-01,  1.8201e-01, -5.8203e-01,  ..., -5.9375e-01,\n",
       "           -2.3718e-01,  1.3660e-01],\n",
       "          [ 5.0439e-01,  4.9805e-01,  1.8005e-01,  ..., -8.7842e-01,\n",
       "            1.2573e-01,  1.1627e-02],\n",
       "          [-6.3416e-02,  4.1333e-01, -4.6851e-01,  ..., -4.3384e-01,\n",
       "           -2.4158e-01,  6.8652e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.9001e-02,  8.8501e-04,  7.6218e-03,  ...,  6.0234e-03,\n",
       "            1.8265e-02, -3.0975e-03],\n",
       "          [-6.8359e-01,  3.7354e-01,  2.3422e-02,  ...,  9.7961e-03,\n",
       "           -1.0291e-01,  9.4666e-02],\n",
       "          [ 1.0730e-01,  3.4204e-01,  8.5876e-02,  ...,  6.0028e-02,\n",
       "            2.4072e-01, -1.1530e-01],\n",
       "          ...,\n",
       "          [ 3.2397e-01, -2.1729e-01, -4.9316e-02,  ...,  5.3467e-01,\n",
       "            4.5654e-02,  1.0849e-02],\n",
       "          [ 3.3276e-01, -2.4243e-01,  3.5339e-02,  ...,  6.0596e-01,\n",
       "           -7.0801e-03, -6.8604e-02],\n",
       "          [ 3.6523e-01, -1.8494e-01,  1.3379e-01,  ...,  5.6396e-01,\n",
       "            3.6194e-02, -1.9763e-01]],\n",
       "\n",
       "         [[-2.1896e-03,  1.2360e-03, -5.6305e-03,  ..., -7.5607e-03,\n",
       "           -7.4768e-04, -1.3840e-02],\n",
       "          [-1.6748e-01, -3.6224e-02, -1.0767e-01,  ...,  7.0410e-01,\n",
       "            2.9419e-01, -1.5027e-01],\n",
       "          [ 6.5625e-01, -5.6055e-01,  4.2896e-01,  ...,  5.2832e-01,\n",
       "            2.8687e-01, -1.3037e-01],\n",
       "          ...,\n",
       "          [ 5.0354e-03, -1.7920e-01,  6.0645e-01,  ..., -1.4038e-01,\n",
       "           -9.4482e-01,  7.0654e-01],\n",
       "          [ 8.9355e-02, -2.8662e-01,  6.5479e-01,  ..., -9.8633e-02,\n",
       "           -9.5850e-01,  7.3535e-01],\n",
       "          [-1.4526e-02, -2.8833e-01,  5.9082e-01,  ..., -1.1218e-01,\n",
       "           -9.9072e-01,  7.6172e-01]],\n",
       "\n",
       "         [[ 1.1200e-02, -1.3458e-02,  6.3400e-03,  ...,  6.5613e-04,\n",
       "           -3.5210e-03,  7.1106e-03],\n",
       "          [-2.5073e-01,  4.5563e-02,  5.3662e-01,  ..., -1.4380e-01,\n",
       "           -2.9834e-01, -3.3862e-01],\n",
       "          [ 3.5107e-01, -4.6411e-01,  1.0236e-01,  ..., -7.0215e-01,\n",
       "            1.5127e+00,  3.4229e-01],\n",
       "          ...,\n",
       "          [-1.9238e-01, -1.9214e-01, -1.5649e-01,  ...,  1.7944e-01,\n",
       "           -1.8219e-02, -8.2581e-02],\n",
       "          [-1.7383e-01, -1.8604e-01, -9.1553e-02,  ...,  1.1597e-01,\n",
       "           -2.5955e-02, -1.4954e-01],\n",
       "          [-1.1487e-01, -2.1399e-01, -1.1096e-01,  ...,  1.4038e-01,\n",
       "           -1.0132e-02, -1.1230e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.5671e-02, -8.0795e-03,  2.5291e-03,  ...,  7.2289e-03,\n",
       "            4.8523e-02, -9.1553e-04],\n",
       "          [ 4.5923e-01, -5.4492e-01, -5.5518e-01,  ...,  2.8906e-01,\n",
       "           -6.0352e-01,  4.7583e-01],\n",
       "          [ 6.7627e-01, -6.8311e-01,  2.2009e-01,  ...,  8.7891e-01,\n",
       "           -1.0527e+00,  9.9731e-02],\n",
       "          ...,\n",
       "          [-2.6562e-01, -1.0364e-01,  2.3938e-01,  ...,  8.0566e-01,\n",
       "            2.8882e-01,  4.2432e-01],\n",
       "          [-2.1667e-01, -1.2354e-01,  1.0834e-01,  ...,  8.5449e-01,\n",
       "            3.5986e-01,  3.9600e-01],\n",
       "          [-1.5857e-01, -7.0801e-02,  9.5947e-02,  ...,  8.1787e-01,\n",
       "            3.1689e-01,  4.3604e-01]],\n",
       "\n",
       "         [[ 1.3809e-03, -1.2856e-02, -1.2321e-02,  ..., -2.7370e-03,\n",
       "            1.0925e-02,  1.4793e-02],\n",
       "          [ 1.5259e-01,  1.0156e-01,  6.3818e-01,  ...,  2.4329e-01,\n",
       "           -9.8291e-01,  5.5328e-02],\n",
       "          [ 3.0518e-04,  5.8594e-02,  1.1035e+00,  ...,  4.7559e-01,\n",
       "            3.3765e-01,  9.3848e-01],\n",
       "          ...,\n",
       "          [ 2.6294e-01, -8.6816e-01, -2.0093e-01,  ..., -3.5474e-01,\n",
       "            8.1494e-01,  8.8379e-01],\n",
       "          [ 3.0786e-01, -7.7734e-01, -2.4194e-01,  ..., -3.1592e-01,\n",
       "            8.3838e-01,  9.0381e-01],\n",
       "          [ 3.1714e-01, -8.4521e-01, -2.3413e-01,  ..., -2.3926e-01,\n",
       "            8.9990e-01,  9.1064e-01]],\n",
       "\n",
       "         [[ 7.1640e-03, -1.4755e-02,  9.7733e-03,  ...,  6.3782e-03,\n",
       "           -1.7395e-02, -1.5656e-02],\n",
       "          [-3.3844e-02,  5.5054e-02,  1.5686e-01,  ..., -4.9170e-01,\n",
       "            7.9834e-02,  1.3281e-01],\n",
       "          [-2.4487e-01, -5.7037e-02,  4.1284e-01,  ..., -2.8516e-01,\n",
       "           -1.7126e-01,  5.0879e-01],\n",
       "          ...,\n",
       "          [-5.0439e-01,  1.8103e-01, -9.1064e-02,  ..., -1.0217e-01,\n",
       "           -1.2787e-02,  5.0439e-01],\n",
       "          [-5.1953e-01,  2.0557e-01, -1.0449e-01,  ..., -1.4844e-01,\n",
       "            4.7569e-03,  4.7534e-01],\n",
       "          [-5.0830e-01,  2.3975e-01, -1.0938e-01,  ..., -8.5083e-02,\n",
       "            8.8257e-02,  4.4946e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.6591e-02,  4.4373e-02,  3.7201e-02,  ..., -7.7820e-02,\n",
       "           -3.5449e-01,  1.2256e+00],\n",
       "          [-6.8115e-02, -9.0332e-02, -9.1553e-01,  ...,  1.6885e+00,\n",
       "           -3.3350e-01, -4.4922e+00],\n",
       "          [-5.4346e-01, -1.3164e+00,  6.4697e-03,  ...,  8.8135e-01,\n",
       "           -8.7891e-01, -3.9531e+00],\n",
       "          ...,\n",
       "          [-6.1035e-02,  1.8115e-01,  1.0508e+00,  ...,  1.0869e+00,\n",
       "           -5.1367e-01, -4.9922e+00],\n",
       "          [ 5.3809e-01, -2.3987e-01,  1.7975e-02,  ...,  6.8457e-01,\n",
       "           -1.0186e+00, -6.6172e+00],\n",
       "          [ 2.5742e-02, -1.9055e-01,  9.2102e-02,  ...,  5.6494e-01,\n",
       "           -8.5693e-01, -4.3945e+00]],\n",
       "\n",
       "         [[-1.0284e-02, -1.8906e-02, -1.6876e-02,  ...,  2.0081e-01,\n",
       "           -1.0547e+00, -6.3293e-02],\n",
       "          [ 1.4734e-01, -3.4790e-03, -6.1829e-02,  ..., -1.1006e+00,\n",
       "            1.8965e+00, -1.8145e+00],\n",
       "          [-1.4673e-01,  1.2695e-01, -1.3354e-01,  ..., -4.4971e-01,\n",
       "            1.0801e+00, -9.5312e-01],\n",
       "          ...,\n",
       "          [-1.8311e-03, -2.0142e-03,  1.2695e-01,  ..., -1.6602e-01,\n",
       "            3.7207e+00, -1.4912e+00],\n",
       "          [-7.0984e-02, -2.6642e-02, -7.2266e-02,  ..., -1.0449e+00,\n",
       "            2.8906e+00,  1.9980e+00],\n",
       "          [ 8.8013e-02, -2.8015e-02, -5.2368e-02,  ...,  8.7585e-03,\n",
       "            2.3340e+00,  1.6162e+00]],\n",
       "\n",
       "         [[-4.3335e-03,  3.8208e-02, -3.9124e-02,  ...,  2.2253e-01,\n",
       "           -1.9568e-01,  3.7415e-02],\n",
       "          [ 8.7988e-01, -4.3213e-01, -1.1787e+00,  ..., -8.8818e-01,\n",
       "           -4.3457e-02,  4.1943e-01],\n",
       "          [-9.4824e-01,  3.1348e-01, -1.0439e+00,  ..., -1.3447e+00,\n",
       "           -9.6777e-01,  1.1934e+00],\n",
       "          ...,\n",
       "          [ 1.2842e-01, -1.3672e+00,  1.0938e+00,  ..., -9.5020e-01,\n",
       "            3.4985e-01,  2.7026e-01],\n",
       "          [ 5.9082e-01,  2.9150e-01,  5.0781e-01,  ..., -1.6338e+00,\n",
       "           -6.6455e-01, -1.1224e-04],\n",
       "          [ 6.0889e-01,  9.6484e-01,  3.7842e-01,  ..., -6.3037e-01,\n",
       "           -1.4658e+00, -4.4531e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.1727e-03,  2.0691e-02, -2.8366e-02,  ...,  1.0022e-01,\n",
       "           -1.6589e-01,  1.5564e-02],\n",
       "          [ 5.0703e+00,  4.0039e-02, -6.5332e-01,  ...,  6.8896e-01,\n",
       "           -3.9453e-01,  5.6348e-01],\n",
       "          [ 4.7539e+00,  1.9736e+00, -3.3477e+00,  ...,  4.4922e-01,\n",
       "           -1.9495e-01, -2.2791e-01],\n",
       "          ...,\n",
       "          [-4.2305e+00, -5.2734e-02,  1.1445e+00,  ..., -8.5840e-01,\n",
       "           -5.0439e-01,  1.3718e-02],\n",
       "          [-5.9727e+00,  3.1895e+00,  2.8281e+00,  ...,  4.5801e-01,\n",
       "           -3.7842e-01, -3.1799e-02],\n",
       "          [-1.4814e+00,  3.5703e+00,  3.2656e+00,  ..., -5.5225e-01,\n",
       "           -4.9536e-01,  7.7588e-01]],\n",
       "\n",
       "         [[ 6.8604e-02, -5.3101e-02,  4.5166e-02,  ..., -1.9641e-01,\n",
       "            3.7354e-02, -5.6689e-01],\n",
       "          [-1.2188e+00,  1.8730e+00,  2.1167e-01,  ...,  1.4082e+00,\n",
       "           -7.1777e-01,  1.8232e+00],\n",
       "          [ 1.9795e+00,  4.1211e+00,  1.7246e+00,  ..., -2.4194e-01,\n",
       "           -1.1289e+00, -9.7070e-01],\n",
       "          ...,\n",
       "          [ 1.6367e+00,  1.4893e+00,  2.8467e-01,  ..., -5.1758e-01,\n",
       "           -7.5684e-01, -2.5146e-01],\n",
       "          [ 6.2695e-01,  1.7266e+00, -1.3379e+00,  ..., -9.8145e-01,\n",
       "           -8.4961e-01,  1.6484e+00],\n",
       "          [-1.6689e+00,  7.7148e-02, -1.7852e+00,  ..., -7.3193e-01,\n",
       "           -7.9102e-01,  1.4658e+00]],\n",
       "\n",
       "         [[ 3.5889e-02, -3.9429e-02, -1.1795e-02,  ..., -9.0698e-02,\n",
       "            6.0181e-02,  4.9072e-02],\n",
       "          [-3.4258e+00,  2.2891e+00, -3.7012e-01,  ..., -6.6113e-01,\n",
       "           -4.6082e-02,  5.2979e-01],\n",
       "          [-2.0723e+00,  1.3809e+00, -1.8652e+00,  ...,  4.1138e-02,\n",
       "           -8.4326e-01, -7.4316e-01],\n",
       "          ...,\n",
       "          [ 3.7305e+00,  2.0742e+00,  1.4258e-01,  ...,  9.9805e-01,\n",
       "            2.8979e-01, -8.5547e-01],\n",
       "          [ 1.7041e+00,  2.0273e+00,  1.6855e+00,  ..., -6.2891e-01,\n",
       "            6.5820e-01, -1.0918e-02],\n",
       "          [-1.2842e+00,  2.1348e+00,  2.1914e+00,  ..., -2.9941e+00,\n",
       "           -2.2217e-01,  2.8589e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.6591e-02,  4.4373e-02,  3.7201e-02,  ..., -7.7820e-02,\n",
       "           -3.5449e-01,  1.2256e+00],\n",
       "          [-6.8115e-02, -9.0332e-02, -9.1553e-01,  ...,  1.6885e+00,\n",
       "           -3.3350e-01, -4.4922e+00],\n",
       "          [-6.9922e-01, -1.2021e+00, -5.4565e-02,  ...,  1.0029e+00,\n",
       "           -5.1953e-01, -3.9648e+00],\n",
       "          ...,\n",
       "          [-5.7678e-02, -3.2056e-01,  7.3181e-02,  ...,  8.8623e-01,\n",
       "           -2.4160e+00, -5.5547e+00],\n",
       "          [-8.3496e-02, -2.9590e-01,  8.2581e-02,  ...,  1.0830e+00,\n",
       "           -2.4570e+00, -5.6523e+00],\n",
       "          [-1.4624e-01,  2.6855e-03,  1.2164e-01,  ...,  1.4023e+00,\n",
       "           -2.3555e+00, -5.4648e+00]],\n",
       "\n",
       "         [[-1.0284e-02, -1.8906e-02, -1.6876e-02,  ...,  2.0081e-01,\n",
       "           -1.0547e+00, -6.3293e-02],\n",
       "          [ 1.4734e-01, -3.4790e-03, -6.1829e-02,  ..., -1.1006e+00,\n",
       "            1.8965e+00, -1.8145e+00],\n",
       "          [ 7.0862e-02, -9.8022e-02, -5.1270e-03,  ..., -6.0107e-01,\n",
       "            2.2871e+00, -4.4312e-01],\n",
       "          ...,\n",
       "          [-1.6650e-01,  4.2041e-01,  2.7026e-01,  ...,  1.1611e+00,\n",
       "            2.2285e+00,  1.4912e+00],\n",
       "          [ 1.4114e-03,  2.4976e-01,  6.2622e-02,  ...,  1.2432e+00,\n",
       "            2.1738e+00,  1.4756e+00],\n",
       "          [ 9.4238e-02, -6.2134e-02, -1.4746e-01,  ...,  1.2529e+00,\n",
       "            2.1758e+00,  1.5742e+00]],\n",
       "\n",
       "         [[-4.3335e-03,  3.8208e-02, -3.9124e-02,  ...,  2.2253e-01,\n",
       "           -1.9568e-01,  3.7415e-02],\n",
       "          [ 8.7988e-01, -4.3213e-01, -1.1787e+00,  ..., -8.8818e-01,\n",
       "           -4.3457e-02,  4.1943e-01],\n",
       "          [-8.1152e-01,  8.1885e-01, -1.3135e+00,  ..., -3.4155e-01,\n",
       "           -3.5425e-01,  4.2529e-01],\n",
       "          ...,\n",
       "          [-8.7451e-01, -1.1084e-01,  8.2153e-02,  ..., -8.6523e-01,\n",
       "           -1.0645e-01, -5.0488e-01],\n",
       "          [ 3.5742e-01,  4.9268e-01,  3.0078e-01,  ..., -7.4170e-01,\n",
       "           -2.5195e-01, -4.8218e-01],\n",
       "          [ 1.2354e+00,  9.3213e-01,  2.7173e-01,  ..., -6.7529e-01,\n",
       "           -1.7310e-01, -4.2285e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.1727e-03,  2.0691e-02, -2.8366e-02,  ...,  1.0022e-01,\n",
       "           -1.6589e-01,  1.5564e-02],\n",
       "          [ 5.0703e+00,  4.0039e-02, -6.5332e-01,  ...,  6.8896e-01,\n",
       "           -3.9453e-01,  5.6348e-01],\n",
       "          [ 4.3398e+00,  2.0859e+00, -2.5547e+00,  ...,  7.3389e-01,\n",
       "           -4.3433e-01, -2.5635e-01],\n",
       "          ...,\n",
       "          [-4.7461e+00, -6.9727e-01,  1.5820e+00,  ...,  3.7695e-01,\n",
       "           -2.2144e-01,  1.2471e+00],\n",
       "          [-5.2070e+00,  1.4160e+00,  2.9961e+00,  ...,  3.5498e-01,\n",
       "           -1.8835e-01,  1.1523e+00],\n",
       "          [-9.1016e-01,  2.6328e+00,  3.2441e+00,  ...,  3.0078e-01,\n",
       "           -1.3293e-01,  1.2012e+00]],\n",
       "\n",
       "         [[ 6.8604e-02, -5.3101e-02,  4.5166e-02,  ..., -1.9641e-01,\n",
       "            3.7354e-02, -5.6689e-01],\n",
       "          [-1.2188e+00,  1.8730e+00,  2.1167e-01,  ...,  1.4082e+00,\n",
       "           -7.1777e-01,  1.8232e+00],\n",
       "          [ 9.2725e-01,  2.8652e+00,  1.4209e+00,  ...,  8.2373e-01,\n",
       "           -1.7979e+00,  6.6162e-01],\n",
       "          ...,\n",
       "          [ 2.6094e+00,  1.9834e+00, -6.3086e-01,  ..., -3.5889e-01,\n",
       "           -7.8516e-01,  3.9746e-01],\n",
       "          [ 1.7944e-01,  1.5742e+00, -1.4502e+00,  ..., -3.8892e-01,\n",
       "           -7.4658e-01,  5.3223e-01],\n",
       "          [-2.2891e+00,  1.6455e-01, -1.8594e+00,  ..., -3.7891e-01,\n",
       "           -7.5342e-01,  4.8901e-01]],\n",
       "\n",
       "         [[ 3.5889e-02, -3.9429e-02, -1.1795e-02,  ..., -9.0698e-02,\n",
       "            6.0181e-02,  4.9072e-02],\n",
       "          [-3.4258e+00,  2.2891e+00, -3.7012e-01,  ..., -6.6113e-01,\n",
       "           -4.6082e-02,  5.2979e-01],\n",
       "          [-1.0156e+00,  1.7861e+00, -2.4883e+00,  ...,  1.7529e-01,\n",
       "           -9.0820e-01, -9.7363e-01],\n",
       "          ...,\n",
       "          [ 2.4961e+00,  6.6992e-01,  4.2236e-01,  ..., -1.8867e+00,\n",
       "            1.6553e-01,  1.4136e-01],\n",
       "          [ 1.3672e+00,  1.8066e+00,  1.7109e+00,  ..., -1.8926e+00,\n",
       "            2.2583e-01,  3.2275e-01],\n",
       "          [-9.5459e-01,  1.8447e+00,  2.4297e+00,  ..., -1.9229e+00,\n",
       "            1.1084e-01,  3.2812e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-1.9197e-03, -2.1553e-03, -2.1332e-02,  ...,  1.5326e-03,\n",
       "            8.2703e-03, -1.0986e-02],\n",
       "          [-2.9956e-01, -2.7759e-01, -7.0996e-01,  ...,  2.7612e-01,\n",
       "           -9.2871e-01, -9.9060e-02],\n",
       "          [-1.1475e-01, -5.6488e-02,  2.9688e-01,  ..., -3.1445e-01,\n",
       "           -1.6760e-01,  1.0889e+00],\n",
       "          ...,\n",
       "          [-6.1133e-01,  2.2388e-01,  6.0596e-01,  ..., -4.7095e-01,\n",
       "           -3.4302e-01,  7.4414e-01],\n",
       "          [-7.5500e-02,  4.9292e-01,  6.1377e-01,  ...,  2.3178e-02,\n",
       "            2.0996e-01, -4.8218e-02],\n",
       "          [ 2.7612e-01,  7.9932e-01, -4.8676e-02,  ..., -3.9734e-02,\n",
       "            1.0236e-01,  2.1655e-01]],\n",
       "\n",
       "         [[ 2.0081e-02, -6.8665e-05,  4.3640e-03,  ...,  1.1414e-02,\n",
       "           -4.0398e-03, -3.6591e-02],\n",
       "          [-3.1445e-01, -2.4707e-01,  3.8281e-01,  ...,  1.7688e-01,\n",
       "           -1.4392e-01, -5.2917e-02],\n",
       "          [ 2.5732e-01,  9.0918e-01,  1.4688e+00,  ..., -1.2549e-01,\n",
       "            4.2871e-01,  3.7256e-01],\n",
       "          ...,\n",
       "          [ 6.7285e-01,  5.0586e-01, -5.1660e-01,  ...,  4.5685e-02,\n",
       "           -3.9697e-01,  7.0312e-01],\n",
       "          [-5.8929e-02,  9.3750e-02, -4.0100e-02,  ..., -4.4336e-01,\n",
       "           -4.4617e-02, -4.6875e-01],\n",
       "          [-5.5029e-01,  2.9321e-01, -5.1605e-02,  ..., -2.6929e-01,\n",
       "            1.7114e-01, -3.8818e-01]],\n",
       "\n",
       "         [[ 9.1410e-04,  5.3558e-03,  4.5853e-03,  ..., -1.1826e-04,\n",
       "           -2.0035e-02, -1.1391e-02],\n",
       "          [-3.4424e-01, -2.5146e-01, -2.0203e-01,  ...,  9.5459e-02,\n",
       "           -1.9263e-01, -2.1106e-01],\n",
       "          [-3.7628e-02, -7.0020e-01, -1.7859e-01,  ...,  5.3809e-01,\n",
       "            1.5674e-01, -5.2490e-03],\n",
       "          ...,\n",
       "          [-3.5083e-01,  1.2646e+00,  4.2627e-01,  ..., -8.0872e-03,\n",
       "           -9.8206e-02,  4.9219e-01],\n",
       "          [-4.2920e-01,  5.6006e-01,  6.1816e-01,  ...,  3.6304e-01,\n",
       "           -5.9814e-01,  1.6187e-01],\n",
       "          [ 4.4092e-01, -2.6062e-02, -1.1279e-01,  ...,  3.2642e-01,\n",
       "           -2.4200e-02,  2.7393e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.0169e-03,  6.5002e-03, -9.9182e-05,  ..., -6.6261e-03,\n",
       "           -2.1400e-03, -7.8278e-03],\n",
       "          [ 2.0007e-01,  1.2903e-01, -6.3379e-01,  ..., -9.2773e-01,\n",
       "            1.4417e-01, -5.4834e-01],\n",
       "          [-7.6843e-02, -2.3849e-02, -1.2708e-01,  ..., -3.4253e-01,\n",
       "           -2.0361e-01,  6.7432e-01],\n",
       "          ...,\n",
       "          [ 6.3281e-01,  5.7471e-01,  2.9028e-01,  ...,  4.1113e-01,\n",
       "           -1.7163e-01,  4.6167e-01],\n",
       "          [ 8.5144e-02, -1.5405e-01,  2.2827e-01,  ...,  1.4514e-01,\n",
       "           -3.1934e-01,  2.8394e-01],\n",
       "          [ 5.8447e-01, -2.1387e-01,  4.5630e-01,  ...,  1.4917e-01,\n",
       "           -2.6123e-01,  3.9111e-01]],\n",
       "\n",
       "         [[ 7.1526e-04,  1.2848e-02, -3.8071e-03,  ...,  8.0109e-03,\n",
       "           -1.4221e-02, -8.9493e-03],\n",
       "          [-2.0605e-01,  6.0498e-01,  4.1040e-01,  ..., -8.1543e-01,\n",
       "            9.6375e-02, -7.1594e-02],\n",
       "          [ 9.1095e-03, -2.4316e-01, -3.7817e-01,  ..., -5.1709e-01,\n",
       "           -5.8899e-02, -3.8306e-01],\n",
       "          ...,\n",
       "          [ 1.5710e-01, -9.5886e-02,  7.0923e-02,  ...,  4.0967e-01,\n",
       "            6.1279e-01,  1.3596e-02],\n",
       "          [ 1.8848e-01, -8.7814e-03, -3.9307e-01,  ..., -1.7197e-02,\n",
       "           -1.8628e-01,  2.1155e-01],\n",
       "          [ 2.7393e-01, -6.7017e-02, -3.4521e-01,  ...,  3.7134e-01,\n",
       "            2.2974e-01,  1.7297e-01]],\n",
       "\n",
       "         [[-1.2436e-02,  8.9798e-03, -3.4828e-03,  ...,  8.1482e-03,\n",
       "           -7.5760e-03, -2.8591e-03],\n",
       "          [ 5.8838e-01,  3.6426e-01, -8.6572e-01,  ..., -1.6565e-01,\n",
       "           -2.6196e-01, -4.5197e-02],\n",
       "          [ 6.1133e-01,  3.1934e-01,  3.3594e-01,  ...,  4.7949e-01,\n",
       "            1.0352e-01,  3.3301e-01],\n",
       "          ...,\n",
       "          [-4.9609e-01,  4.4214e-01,  4.8779e-01,  ...,  2.4280e-01,\n",
       "            1.4587e-01,  3.3691e-01],\n",
       "          [-8.7646e-01,  4.4922e-01,  1.3770e-01,  ..., -3.8599e-01,\n",
       "            2.6636e-01, -9.5032e-02],\n",
       "          [ 2.3828e-01,  5.9229e-01,  7.5928e-02,  ..., -3.2104e-01,\n",
       "           -1.6504e-01,  3.8037e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.9197e-03, -2.1553e-03, -2.1332e-02,  ...,  1.5326e-03,\n",
       "            8.2703e-03, -1.0986e-02],\n",
       "          [-2.9956e-01, -2.7759e-01, -7.0996e-01,  ...,  2.7612e-01,\n",
       "           -9.2871e-01, -9.9060e-02],\n",
       "          [-6.7822e-01,  1.5442e-01, -4.0283e-01,  ..., -2.0300e-01,\n",
       "           -5.4541e-01,  3.5840e-01],\n",
       "          ...,\n",
       "          [ 1.0370e-01,  6.9434e-01,  2.5684e-01,  ...,  1.2976e-01,\n",
       "            1.2793e-01,  2.5757e-01],\n",
       "          [ 1.4124e-01,  6.2256e-01,  2.9492e-01,  ...,  1.3037e-01,\n",
       "            4.9164e-02,  2.4036e-01],\n",
       "          [ 1.4600e-01,  6.4990e-01,  3.1055e-01,  ...,  9.5886e-02,\n",
       "            7.4219e-02,  1.7725e-01]],\n",
       "\n",
       "         [[ 2.0081e-02, -6.8665e-05,  4.3640e-03,  ...,  1.1414e-02,\n",
       "           -4.0398e-03, -3.6591e-02],\n",
       "          [-3.1445e-01, -2.4707e-01,  3.8281e-01,  ...,  1.7688e-01,\n",
       "           -1.4392e-01, -5.2917e-02],\n",
       "          [-1.3086e-01, -9.1064e-01,  1.3633e+00,  ..., -9.1943e-01,\n",
       "            1.0156e+00,  5.3076e-01],\n",
       "          ...,\n",
       "          [-4.8926e-01,  8.5840e-01, -3.2593e-01,  ..., -8.0200e-02,\n",
       "            2.5464e-01, -6.2988e-01],\n",
       "          [-4.5947e-01,  8.5352e-01, -2.9102e-01,  ..., -2.3193e-03,\n",
       "            2.1313e-01, -5.8789e-01],\n",
       "          [-6.4600e-01,  8.7158e-01, -3.1323e-01,  ..., -3.3875e-02,\n",
       "            2.9980e-01, -5.2344e-01]],\n",
       "\n",
       "         [[ 9.1410e-04,  5.3558e-03,  4.5853e-03,  ..., -1.1826e-04,\n",
       "           -2.0035e-02, -1.1391e-02],\n",
       "          [-3.4424e-01, -2.5146e-01, -2.0203e-01,  ...,  9.5459e-02,\n",
       "           -1.9263e-01, -2.1106e-01],\n",
       "          [ 1.1890e-01, -7.4707e-01, -3.2544e-01,  ..., -2.1460e-01,\n",
       "            4.2896e-01, -2.5830e-01],\n",
       "          ...,\n",
       "          [ 4.1040e-01,  2.6440e-01,  7.4158e-02,  ...,  6.1572e-01,\n",
       "            1.9861e-01,  1.4648e-01],\n",
       "          [ 4.5996e-01,  2.9834e-01,  7.1899e-02,  ...,  5.7812e-01,\n",
       "            2.4878e-01,  1.4844e-01],\n",
       "          [ 5.4492e-01,  2.7710e-01,  1.5210e-01,  ...,  5.7812e-01,\n",
       "            2.3779e-01,  1.6516e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.0169e-03,  6.5002e-03, -9.9182e-05,  ..., -6.6261e-03,\n",
       "           -2.1400e-03, -7.8278e-03],\n",
       "          [ 2.0007e-01,  1.2903e-01, -6.3379e-01,  ..., -9.2773e-01,\n",
       "            1.4417e-01, -5.4834e-01],\n",
       "          [-7.7881e-02, -2.5488e-01, -7.4268e-01,  ...,  5.6580e-02,\n",
       "           -1.5088e-01, -1.4417e-01],\n",
       "          ...,\n",
       "          [ 3.8794e-01, -8.3887e-01,  4.7046e-01,  ...,  2.5586e-01,\n",
       "           -2.6147e-01, -3.1891e-03],\n",
       "          [ 4.1357e-01, -8.4570e-01,  4.6973e-01,  ...,  2.3486e-01,\n",
       "           -2.7881e-01, -9.2896e-02],\n",
       "          [ 4.8193e-01, -9.1553e-01,  4.0332e-01,  ...,  2.6660e-01,\n",
       "           -2.0605e-01, -7.0312e-02]],\n",
       "\n",
       "         [[ 7.1526e-04,  1.2848e-02, -3.8071e-03,  ...,  8.0109e-03,\n",
       "           -1.4221e-02, -8.9493e-03],\n",
       "          [-2.0605e-01,  6.0498e-01,  4.1040e-01,  ..., -8.1543e-01,\n",
       "            9.6375e-02, -7.1594e-02],\n",
       "          [ 1.5613e-01, -3.6841e-01, -3.2739e-01,  ..., -1.1621e+00,\n",
       "            4.6295e-02, -3.5059e-01],\n",
       "          ...,\n",
       "          [ 2.5391e-01, -4.9561e-01, -3.3447e-01,  ...,  3.9282e-01,\n",
       "            1.9971e-01, -4.2139e-01],\n",
       "          [ 2.8491e-01, -4.7510e-01, -3.2129e-01,  ...,  4.2603e-01,\n",
       "            2.0227e-01, -3.2227e-01],\n",
       "          [ 2.9590e-01, -3.5718e-01, -2.9590e-01,  ...,  4.8145e-01,\n",
       "            2.0532e-01, -3.3276e-01]],\n",
       "\n",
       "         [[-1.2436e-02,  8.9798e-03, -3.4828e-03,  ...,  8.1482e-03,\n",
       "           -7.5760e-03, -2.8591e-03],\n",
       "          [ 5.8838e-01,  3.6426e-01, -8.6572e-01,  ..., -1.6565e-01,\n",
       "           -2.6196e-01, -4.5197e-02],\n",
       "          [ 2.0105e-01,  2.3499e-01,  4.0015e-01,  ..., -9.8572e-03,\n",
       "           -4.5703e-01,  1.2085e-01],\n",
       "          ...,\n",
       "          [ 3.2812e-01,  3.8910e-02,  3.9624e-01,  ..., -2.6123e-01,\n",
       "            1.5881e-01,  5.1660e-01],\n",
       "          [ 4.2236e-01, -3.1281e-02,  3.1445e-01,  ..., -2.5342e-01,\n",
       "            1.5759e-01,  5.6201e-01],\n",
       "          [ 4.0723e-01,  9.5978e-03,  3.8623e-01,  ..., -3.2080e-01,\n",
       "            1.3281e-01,  5.7031e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 8.0795e-03, -2.7847e-02, -3.5461e-02,  ..., -3.5980e-02,\n",
       "           -1.4336e+00,  4.9902e-01],\n",
       "          [-2.2168e+00,  5.5127e-01,  1.3086e+00,  ..., -6.6406e-01,\n",
       "            1.6680e+00, -2.0020e-02],\n",
       "          [-2.6113e+00, -4.2145e-02,  9.0723e-01,  ..., -6.4844e-01,\n",
       "            2.8770e+00,  3.0518e-02],\n",
       "          ...,\n",
       "          [ 6.9775e-01, -4.6387e-02, -6.3965e-01,  ..., -2.1680e+00,\n",
       "            4.5703e+00, -2.0605e+00],\n",
       "          [ 1.3184e+00,  1.1094e+00,  7.3340e-01,  ...,  7.5537e-01,\n",
       "            3.2910e+00,  1.2734e+00],\n",
       "          [ 2.3560e-02,  6.9629e-01,  7.0898e-01,  ...,  8.3057e-01,\n",
       "            2.1973e+00,  1.5898e+00]],\n",
       "\n",
       "         [[ 6.4087e-03, -1.3062e-02, -1.6052e-02,  ...,  3.4741e-01,\n",
       "            2.8000e-03, -4.3182e-02],\n",
       "          [ 2.7759e-01,  5.2734e-01,  5.8057e-01,  ..., -8.7830e-02,\n",
       "            3.5278e-02, -1.0303e+00],\n",
       "          [ 5.9717e-01,  1.4709e-01, -1.0723e+00,  ...,  1.8633e+00,\n",
       "            1.5161e-01,  6.3818e-01],\n",
       "          ...,\n",
       "          [-1.0439e+00,  9.9951e-01,  9.1650e-01,  ...,  7.6758e-01,\n",
       "           -7.1338e-01,  2.3750e+00],\n",
       "          [ 7.4829e-02,  4.8730e-01,  4.7168e-01,  ..., -5.2197e-01,\n",
       "           -1.5459e+00,  1.7070e+00],\n",
       "          [ 7.2070e-01,  1.2207e-02, -3.0420e-01,  ..., -4.9194e-02,\n",
       "           -1.5547e+00,  7.2205e-02]],\n",
       "\n",
       "         [[ 2.2903e-02, -1.0376e-03,  6.2256e-02,  ...,  7.8979e-02,\n",
       "           -1.5137e-01,  9.7595e-02],\n",
       "          [-4.7607e-01, -2.2253e-01, -2.8540e-01,  ..., -2.5439e-01,\n",
       "           -6.1865e-01,  1.9592e-01],\n",
       "          [-9.5361e-01, -1.6382e-01, -1.2715e+00,  ..., -2.6055e+00,\n",
       "           -9.6289e-01, -2.2180e-01],\n",
       "          ...,\n",
       "          [ 1.1426e+00, -2.6758e-01,  6.6504e-01,  ..., -9.0186e-01,\n",
       "           -7.7246e-01,  1.4229e+00],\n",
       "          [-1.7163e-01,  9.9805e-01,  6.7676e-01,  ..., -1.2979e+00,\n",
       "           -1.4893e+00,  9.5020e-01],\n",
       "          [-1.6953e+00,  4.8755e-01,  4.6069e-01,  ..., -1.5107e+00,\n",
       "            2.7734e-01,  4.0625e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.9917e-02,  3.8452e-03,  3.4180e-02,  ..., -1.1536e-01,\n",
       "           -2.5488e-01, -7.4951e-02],\n",
       "          [-2.3242e+00,  1.2949e+00,  9.7949e-01,  ...,  2.1960e-01,\n",
       "            2.6807e-01, -8.8574e-01],\n",
       "          [-3.3066e+00,  2.7852e+00, -2.3770e+00,  ...,  4.8242e-01,\n",
       "            1.1318e+00, -8.4082e-01],\n",
       "          ...,\n",
       "          [ 1.3838e+00,  8.7207e-01, -1.3115e+00,  ...,  4.4434e-01,\n",
       "            1.8867e+00, -1.2471e+00],\n",
       "          [ 2.3105e+00, -1.5898e+00,  3.4961e-01,  ...,  6.6064e-01,\n",
       "           -2.5732e-01, -1.7148e+00],\n",
       "          [ 6.3379e-01, -1.1748e+00,  9.4482e-01,  ..., -1.5063e-01,\n",
       "            1.6172e+00,  2.4390e-01]],\n",
       "\n",
       "         [[-2.9404e-02, -2.0935e-02, -2.3270e-02,  ..., -4.8438e-01,\n",
       "            1.3538e-01,  6.5576e-01],\n",
       "          [ 8.0566e-02,  8.2520e-02, -7.6465e-01,  ...,  6.7871e-01,\n",
       "            5.2686e-01,  6.2988e-02],\n",
       "          [-1.9668e+00, -1.4746e+00, -1.3594e+00,  ...,  9.5898e-01,\n",
       "            6.1865e-01, -1.8867e+00],\n",
       "          ...,\n",
       "          [ 3.1885e-01, -6.4551e-01,  8.2910e-01,  ...,  1.2715e+00,\n",
       "            2.2051e+00, -1.9263e-01],\n",
       "          [ 5.5273e-01, -1.3877e+00,  1.4941e+00,  ...,  1.6152e+00,\n",
       "            2.3906e+00, -2.6270e+00],\n",
       "          [ 1.5430e+00, -1.7012e+00,  1.0781e+00,  ...,  9.2627e-01,\n",
       "            2.5215e+00, -2.1641e+00]],\n",
       "\n",
       "         [[-1.9394e-02, -2.7954e-02,  6.1035e-03,  ...,  3.9062e-01,\n",
       "            1.7480e+00, -3.0078e-01],\n",
       "          [ 1.4668e+00, -6.5820e-01,  1.1934e+00,  ...,  8.1201e-01,\n",
       "           -3.0391e+00,  2.6445e+00],\n",
       "          [ 1.1104e+00, -6.2256e-02, -2.1704e-01,  ...,  1.6406e-01,\n",
       "           -3.1738e+00,  2.3477e+00],\n",
       "          ...,\n",
       "          [-1.4395e+00, -5.2539e-01, -5.7812e-01,  ..., -3.1641e-01,\n",
       "           -2.5430e+00, -9.4629e-01],\n",
       "          [-1.2217e+00, -2.0039e+00, -3.2397e-01,  ..., -1.7402e+00,\n",
       "           -2.9902e+00,  1.1699e+00],\n",
       "          [-2.6221e-01, -2.0410e-01,  1.6113e-01,  ..., -1.5244e+00,\n",
       "           -5.0898e+00,  8.9600e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 8.0795e-03, -2.7847e-02, -3.5461e-02,  ..., -3.5980e-02,\n",
       "           -1.4336e+00,  4.9902e-01],\n",
       "          [-2.2168e+00,  5.5127e-01,  1.3086e+00,  ..., -6.6406e-01,\n",
       "            1.6680e+00, -2.0020e-02],\n",
       "          [-3.1953e+00,  8.4839e-02,  2.9492e-01,  ...,  9.3896e-01,\n",
       "            2.5645e+00, -2.1606e-02],\n",
       "          ...,\n",
       "          [ 4.8804e-01,  5.8398e-01, -6.8555e-01,  ...,  9.0234e-01,\n",
       "            1.6152e+00,  6.7676e-01],\n",
       "          [ 5.4004e-01,  1.0518e+00, -1.3049e-01,  ...,  9.5215e-01,\n",
       "            1.5811e+00,  5.4736e-01],\n",
       "          [ 2.5610e-01,  8.4424e-01,  4.6094e-01,  ...,  9.1504e-01,\n",
       "            1.5557e+00,  6.4941e-01]],\n",
       "\n",
       "         [[ 6.4087e-03, -1.3062e-02, -1.6052e-02,  ...,  3.4741e-01,\n",
       "            2.8000e-03, -4.3182e-02],\n",
       "          [ 2.7759e-01,  5.2734e-01,  5.8057e-01,  ..., -8.7830e-02,\n",
       "            3.5278e-02, -1.0303e+00],\n",
       "          [-3.1787e-01,  1.4102e+00, -1.3506e+00,  ...,  7.1289e-01,\n",
       "           -1.0371e+00,  1.4248e+00],\n",
       "          ...,\n",
       "          [-5.6030e-02, -1.8738e-01,  4.5020e-01,  ...,  3.2617e-01,\n",
       "           -2.4668e+00,  1.0391e+00],\n",
       "          [ 2.1912e-01, -1.2781e-01,  2.1399e-01,  ...,  4.0381e-01,\n",
       "           -2.6152e+00,  1.0605e+00],\n",
       "          [ 3.4302e-01, -9.9548e-02, -1.0034e-01,  ...,  4.2578e-01,\n",
       "           -2.5156e+00,  1.1934e+00]],\n",
       "\n",
       "         [[ 2.2903e-02, -1.0376e-03,  6.2256e-02,  ...,  7.8979e-02,\n",
       "           -1.5137e-01,  9.7595e-02],\n",
       "          [-4.7607e-01, -2.2253e-01, -2.8540e-01,  ..., -2.5439e-01,\n",
       "           -6.1865e-01,  1.9592e-01],\n",
       "          [ 2.3267e-01, -4.3488e-02, -1.5107e+00,  ..., -2.4746e+00,\n",
       "            1.9531e-01,  2.2998e-01],\n",
       "          ...,\n",
       "          [ 1.4170e+00, -4.9048e-01, -2.1631e-01,  ..., -6.3574e-01,\n",
       "           -6.4160e-01,  7.6562e-01],\n",
       "          [ 6.8164e-01,  1.0291e-01, -2.2873e-02,  ..., -6.0498e-01,\n",
       "           -8.5596e-01,  6.2451e-01],\n",
       "          [-6.9824e-01,  5.9766e-01,  1.9751e-01,  ..., -6.1133e-01,\n",
       "           -7.1777e-01,  5.8789e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.9917e-02,  3.8452e-03,  3.4180e-02,  ..., -1.1536e-01,\n",
       "           -2.5488e-01, -7.4951e-02],\n",
       "          [-2.3242e+00,  1.2949e+00,  9.7949e-01,  ...,  2.1960e-01,\n",
       "            2.6807e-01, -8.8574e-01],\n",
       "          [-2.8867e+00,  3.3965e+00, -2.3066e+00,  ..., -6.4160e-01,\n",
       "            1.0156e+00, -3.6768e-01],\n",
       "          ...,\n",
       "          [ 1.5742e+00,  1.0674e+00,  1.6504e-01,  ..., -5.8203e-01,\n",
       "            1.3940e-01, -1.2559e+00],\n",
       "          [ 2.0645e+00,  2.9028e-01,  1.1357e+00,  ..., -5.7715e-01,\n",
       "            2.2705e-01, -1.3789e+00],\n",
       "          [ 6.5430e-01, -6.9727e-01,  1.7705e+00,  ..., -5.8984e-01,\n",
       "            2.9053e-01, -1.2734e+00]],\n",
       "\n",
       "         [[-2.9404e-02, -2.0935e-02, -2.3270e-02,  ..., -4.8438e-01,\n",
       "            1.3538e-01,  6.5576e-01],\n",
       "          [ 8.0566e-02,  8.2520e-02, -7.6465e-01,  ...,  6.7871e-01,\n",
       "            5.2686e-01,  6.2988e-02],\n",
       "          [-2.0723e+00, -9.7852e-01, -1.6582e+00,  ...,  1.9482e-01,\n",
       "            2.4512e-01, -3.4644e-01],\n",
       "          ...,\n",
       "          [-1.0020e+00,  5.9082e-02,  4.4897e-01,  ...,  3.0322e-01,\n",
       "            2.6016e+00, -2.3125e+00],\n",
       "          [ 7.0190e-02, -7.8662e-01,  7.4609e-01,  ...,  3.3154e-01,\n",
       "            2.7188e+00, -2.3008e+00],\n",
       "          [ 1.0381e+00, -1.1260e+00,  6.8018e-01,  ...,  3.1445e-01,\n",
       "            2.5781e+00, -2.3438e+00]],\n",
       "\n",
       "         [[-1.9394e-02, -2.7954e-02,  6.1035e-03,  ...,  3.9062e-01,\n",
       "            1.7480e+00, -3.0078e-01],\n",
       "          [ 1.4668e+00, -6.5820e-01,  1.1934e+00,  ...,  8.1201e-01,\n",
       "           -3.0391e+00,  2.6445e+00],\n",
       "          [ 1.5684e+00, -5.9668e-01, -4.7827e-01,  ...,  4.6021e-01,\n",
       "           -3.2617e+00,  1.9375e+00],\n",
       "          ...,\n",
       "          [-5.9424e-01, -6.5820e-01, -9.3848e-01,  ..., -1.4824e+00,\n",
       "           -3.8281e+00,  6.2939e-01],\n",
       "          [-1.2061e+00, -7.4121e-01, -2.7905e-01,  ..., -1.4092e+00,\n",
       "           -3.7734e+00,  6.8311e-01],\n",
       "          [-7.3047e-01, -3.9160e-01,  5.0293e-01,  ..., -1.2705e+00,\n",
       "           -3.7266e+00,  6.3281e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 2.7435e-02,  1.1650e-02, -4.6021e-02,  ...,  4.2328e-02,\n",
       "           -2.5940e-02, -5.6335e-02],\n",
       "          [ 1.3403e-01,  3.2074e-02, -1.4026e-01,  ..., -5.9113e-02,\n",
       "           -1.0394e-01,  5.8545e-01],\n",
       "          [ 1.6943e-01, -3.9581e-02,  1.3843e-01,  ...,  1.5698e-01,\n",
       "           -4.6509e-01,  1.8250e-01],\n",
       "          ...,\n",
       "          [ 4.2822e-01, -9.1187e-02, -9.8999e-02,  ..., -1.1670e-01,\n",
       "            2.8076e-02, -2.6025e-01],\n",
       "          [-1.5112e-01,  1.0278e-01, -1.4929e-01,  ..., -2.4475e-01,\n",
       "            2.2205e-01, -4.0454e-01],\n",
       "          [-1.9592e-01,  1.6785e-01, -6.6040e-02,  ...,  1.2134e-01,\n",
       "           -6.9031e-02,  2.8247e-01]],\n",
       "\n",
       "         [[-1.4725e-02,  1.4404e-02, -8.5068e-04,  ...,  7.6828e-03,\n",
       "            5.2795e-03,  4.0817e-04],\n",
       "          [-3.0975e-02, -1.3641e-02, -7.3364e-02,  ..., -5.3613e-01,\n",
       "            1.2988e-01,  1.4185e-01],\n",
       "          [ 4.9951e-01,  3.2520e-01,  2.3633e-01,  ...,  7.4365e-01,\n",
       "            1.1650e+00, -8.1848e-02],\n",
       "          ...,\n",
       "          [ 7.7881e-02,  2.1130e-01,  3.6914e-01,  ..., -7.1338e-01,\n",
       "            5.4688e-02, -3.9209e-01],\n",
       "          [ 3.5205e-01,  4.8853e-01,  5.6396e-01,  ..., -3.7280e-01,\n",
       "            2.2937e-01, -6.9775e-01],\n",
       "          [-1.8103e-01, -1.7786e-01, -3.0518e-01,  ..., -2.0044e-01,\n",
       "            2.1741e-01, -9.5850e-01]],\n",
       "\n",
       "         [[ 3.6201e-03, -5.1689e-03, -9.5367e-04,  ..., -1.1177e-02,\n",
       "            2.4139e-02, -5.7106e-03],\n",
       "          [-3.6328e-01, -1.1719e-02, -3.1714e-01,  ..., -1.2573e-01,\n",
       "            3.0078e-01,  7.0496e-02],\n",
       "          [-8.4375e-01,  6.1798e-02, -1.5717e-02,  ..., -9.6191e-02,\n",
       "           -9.7290e-02, -4.1284e-01],\n",
       "          ...,\n",
       "          [-1.8420e-01, -1.5894e-01,  1.5540e-01,  ..., -6.0645e-01,\n",
       "           -4.6729e-01, -7.9712e-02],\n",
       "          [-1.5784e-01, -6.6406e-01, -1.4252e-02,  ..., -8.2825e-02,\n",
       "           -2.1240e-01,  2.1631e-01],\n",
       "          [-3.8257e-01, -4.5312e-01, -1.2537e-01,  ..., -5.6458e-02,\n",
       "           -4.3066e-01,  2.7002e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.8234e-02, -8.7891e-03,  7.5150e-03,  ...,  7.3719e-04,\n",
       "           -8.0681e-04,  4.1428e-03],\n",
       "          [-2.4634e-01, -9.7351e-02,  1.0455e-01,  ...,  2.8711e-01,\n",
       "           -1.5601e-01,  2.0166e-01],\n",
       "          [-3.1445e-01,  3.9404e-01, -4.6362e-01,  ...,  2.1472e-01,\n",
       "           -5.2783e-01,  7.5732e-01],\n",
       "          ...,\n",
       "          [ 4.5776e-01,  2.9346e-01, -3.1909e-01,  ..., -2.0923e-01,\n",
       "           -6.9458e-02,  2.2339e-01],\n",
       "          [-3.5303e-01, -4.8706e-01, -2.9199e-01,  ..., -9.7275e-05,\n",
       "           -6.5381e-01,  1.5820e-01],\n",
       "          [-7.1436e-01,  1.5210e-01, -5.1910e-02,  ..., -8.6487e-02,\n",
       "           -1.9604e-01,  1.0779e-01]],\n",
       "\n",
       "         [[-1.1513e-02, -3.6201e-03, -2.4853e-03,  ...,  5.1003e-03,\n",
       "            2.1286e-02, -2.4033e-03],\n",
       "          [-7.7332e-02,  3.7964e-01,  3.8672e-01,  ..., -1.1920e-01,\n",
       "           -5.4016e-02,  3.1836e-01],\n",
       "          [-5.8441e-02, -1.4771e-01,  7.0496e-02,  ..., -2.4872e-02,\n",
       "            7.9150e-01,  1.4160e-01],\n",
       "          ...,\n",
       "          [-3.8354e-01,  3.1250e-02,  1.9653e-02,  ..., -1.1487e-01,\n",
       "           -1.5088e-01, -5.2673e-02],\n",
       "          [ 4.2206e-02,  3.5156e-01,  1.3599e-01,  ..., -2.6807e-01,\n",
       "            8.6914e-01,  4.8730e-01],\n",
       "          [-2.0044e-01,  4.1699e-01, -1.7114e-01,  ..., -2.1985e-01,\n",
       "           -1.9312e-01,  6.5063e-02]],\n",
       "\n",
       "         [[-5.3215e-03,  6.1378e-03,  2.0691e-02,  ..., -3.9612e-02,\n",
       "           -1.9104e-02,  2.5497e-02],\n",
       "          [-1.0150e-01,  7.5391e-01,  2.9395e-01,  ...,  1.0730e-01,\n",
       "            1.5198e-01, -1.1584e-01],\n",
       "          [-3.4326e-01, -5.6152e-02,  3.5596e-01,  ...,  9.7021e-01,\n",
       "            2.1484e-01, -6.2622e-02],\n",
       "          ...,\n",
       "          [ 2.5977e-01,  2.0386e-01, -6.0400e-01,  ...,  3.0005e-01,\n",
       "           -2.0581e-01,  1.6089e-01],\n",
       "          [ 3.7781e-02,  3.7817e-01, -2.3718e-01,  ...,  5.1367e-01,\n",
       "           -2.2034e-01,  1.0870e-01],\n",
       "          [ 2.5562e-01,  1.4740e-02, -5.1453e-02,  ...,  1.8127e-01,\n",
       "           -1.3965e-01,  2.0862e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.7435e-02,  1.1650e-02, -4.6021e-02,  ...,  4.2328e-02,\n",
       "           -2.5940e-02, -5.6335e-02],\n",
       "          [ 1.3403e-01,  3.2074e-02, -1.4026e-01,  ..., -5.9113e-02,\n",
       "           -1.0394e-01,  5.8545e-01],\n",
       "          [ 2.0044e-01, -5.3271e-01,  4.1235e-01,  ..., -1.7981e-01,\n",
       "           -2.1533e-01,  2.3352e-01],\n",
       "          ...,\n",
       "          [-2.2791e-01,  3.2043e-02, -3.4033e-01,  ...,  7.8125e-03,\n",
       "           -1.3916e-01,  3.3569e-04],\n",
       "          [-2.3560e-01, -1.0536e-02, -3.7769e-01,  ..., -1.7456e-02,\n",
       "           -2.2778e-01,  1.0834e-02],\n",
       "          [-2.5562e-01,  2.9755e-02, -4.3433e-01,  ..., -6.5552e-02,\n",
       "           -1.8286e-01, -3.9001e-02]],\n",
       "\n",
       "         [[-1.4725e-02,  1.4404e-02, -8.5068e-04,  ...,  7.6828e-03,\n",
       "            5.2795e-03,  4.0817e-04],\n",
       "          [-3.0975e-02, -1.3641e-02, -7.3364e-02,  ..., -5.3613e-01,\n",
       "            1.2988e-01,  1.4185e-01],\n",
       "          [ 4.7974e-02,  1.4355e+00,  2.7148e-01,  ...,  5.3418e-01,\n",
       "           -2.9395e-01, -1.7102e-01],\n",
       "          ...,\n",
       "          [ 4.3628e-01,  2.5464e-01,  1.1139e-01,  ...,  3.5522e-02,\n",
       "            5.0830e-01, -5.4639e-01],\n",
       "          [ 4.5044e-01,  1.9983e-01,  7.9224e-02,  ..., -6.9641e-02,\n",
       "            5.6396e-01, -5.5420e-01],\n",
       "          [ 4.2163e-01,  2.5732e-01,  1.3342e-01,  ..., -4.0283e-03,\n",
       "            6.2842e-01, -5.8398e-01]],\n",
       "\n",
       "         [[ 3.6201e-03, -5.1689e-03, -9.5367e-04,  ..., -1.1177e-02,\n",
       "            2.4139e-02, -5.7106e-03],\n",
       "          [-3.6328e-01, -1.1719e-02, -3.1714e-01,  ..., -1.2573e-01,\n",
       "            3.0078e-01,  7.0496e-02],\n",
       "          [-3.2910e-01, -2.4951e-01,  2.8613e-01,  ...,  2.3364e-01,\n",
       "           -9.7046e-02, -2.5220e-01],\n",
       "          ...,\n",
       "          [ 5.1636e-02, -4.5850e-01, -7.0508e-01,  ..., -4.9048e-01,\n",
       "            1.2000e-01,  3.5107e-01],\n",
       "          [ 1.4572e-02, -4.0894e-01, -5.9229e-01,  ..., -5.4492e-01,\n",
       "            9.9426e-02,  2.7466e-01],\n",
       "          [ 3.5553e-02, -4.8389e-01, -6.0400e-01,  ..., -4.8193e-01,\n",
       "            9.8083e-02,  1.5637e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.8234e-02, -8.7891e-03,  7.5150e-03,  ...,  7.3719e-04,\n",
       "           -8.0681e-04,  4.1428e-03],\n",
       "          [-2.4634e-01, -9.7351e-02,  1.0455e-01,  ...,  2.8711e-01,\n",
       "           -1.5601e-01,  2.0166e-01],\n",
       "          [-6.5918e-01,  6.8359e-01,  8.9417e-03,  ...,  1.6345e-01,\n",
       "           -5.5664e-01, -9.6313e-02],\n",
       "          ...,\n",
       "          [-4.8511e-01,  3.7262e-02, -2.8711e-01,  ..., -2.9419e-02,\n",
       "           -1.9928e-02, -9.9609e-02],\n",
       "          [-4.8560e-01,  7.7148e-02, -2.8394e-01,  ..., -2.0828e-03,\n",
       "           -1.3379e-01, -1.8921e-01],\n",
       "          [-5.0586e-01,  7.5195e-02, -2.2607e-01,  ..., -2.3331e-02,\n",
       "           -2.7161e-02, -1.0382e-01]],\n",
       "\n",
       "         [[-1.1513e-02, -3.6201e-03, -2.4853e-03,  ...,  5.1003e-03,\n",
       "            2.1286e-02, -2.4033e-03],\n",
       "          [-7.7332e-02,  3.7964e-01,  3.8672e-01,  ..., -1.1920e-01,\n",
       "           -5.4016e-02,  3.1836e-01],\n",
       "          [ 8.6914e-02, -1.3049e-01,  3.5303e-01,  ..., -6.2500e-01,\n",
       "            4.2920e-01,  7.3096e-01],\n",
       "          ...,\n",
       "          [ 3.3081e-02,  2.1216e-01, -1.9104e-01,  ..., -1.0345e-01,\n",
       "            1.8018e-01,  2.4756e-01],\n",
       "          [ 9.3018e-02,  2.0142e-01, -1.8457e-01,  ..., -1.5222e-01,\n",
       "            1.6516e-01,  2.0923e-01],\n",
       "          [ 3.0228e-02,  1.8457e-01, -1.2396e-01,  ..., -1.6699e-01,\n",
       "            1.0547e-01,  2.0190e-01]],\n",
       "\n",
       "         [[-5.3215e-03,  6.1378e-03,  2.0691e-02,  ..., -3.9612e-02,\n",
       "           -1.9104e-02,  2.5497e-02],\n",
       "          [-1.0150e-01,  7.5391e-01,  2.9395e-01,  ...,  1.0730e-01,\n",
       "            1.5198e-01, -1.1584e-01],\n",
       "          [ 3.2983e-01,  1.6455e-01, -9.2896e-02,  ..., -4.7180e-02,\n",
       "            8.3643e-01, -4.7168e-01],\n",
       "          ...,\n",
       "          [ 2.4426e-01,  7.0923e-02,  7.4072e-01,  ...,  2.2974e-01,\n",
       "           -1.3206e-02,  1.9080e-01],\n",
       "          [ 2.7661e-01,  8.4778e-02,  7.8809e-01,  ...,  2.2693e-01,\n",
       "           -1.9989e-02,  1.7969e-01],\n",
       "          [ 2.7905e-01,  1.2695e-01,  7.8027e-01,  ...,  2.5366e-01,\n",
       "           -3.5797e-02,  9.5581e-02]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.4302e-02, -7.3853e-03,  1.0742e-02,  ...,  1.5137e-02,\n",
       "           -3.1006e-01,  3.4155e-01],\n",
       "          [-4.4897e-01, -4.8804e-01,  1.0767e-01,  ...,  9.0039e-01,\n",
       "           -6.3672e-01, -1.4023e+00],\n",
       "          [ 5.8887e-01, -1.5352e+00,  3.7280e-01,  ...,  1.7852e+00,\n",
       "           -4.9414e-01,  5.5908e-01],\n",
       "          ...,\n",
       "          [ 8.7207e-01,  1.5312e+00, -1.0488e+00,  ..., -4.1919e-01,\n",
       "            2.7988e+00, -1.3984e+00],\n",
       "          [ 5.2832e-01,  1.5674e-01, -3.1641e-01,  ..., -4.2139e-01,\n",
       "           -7.8247e-02, -9.2822e-01],\n",
       "          [-1.1455e+00, -3.3643e-01, -2.4512e-01,  ..., -5.9082e-01,\n",
       "            2.4219e-01, -2.1914e+00]],\n",
       "\n",
       "         [[ 6.3324e-03,  6.4850e-04, -2.0477e-02,  ..., -5.6934e-01,\n",
       "            2.2934e-02,  1.2488e-01],\n",
       "          [ 1.1194e-01,  2.8290e-02, -2.8149e-01,  ...,  2.0039e+00,\n",
       "           -2.3535e-01, -1.0713e+00],\n",
       "          [ 1.1346e-01,  5.2368e-02,  7.8125e-03,  ...,  7.7197e-01,\n",
       "           -2.6055e+00, -1.7207e+00],\n",
       "          ...,\n",
       "          [ 3.4106e-01,  1.6125e-01, -2.5977e-01,  ...,  6.6699e-01,\n",
       "           -2.6807e-01,  5.7539e+00],\n",
       "          [-1.9165e-01,  7.3853e-02, -2.7563e-01,  ...,  3.6719e-01,\n",
       "           -1.3916e+00,  4.4482e-01],\n",
       "          [ 4.3579e-02,  2.1655e-01,  1.9434e-01,  ..., -7.3096e-01,\n",
       "           -3.8359e+00,  2.2441e+00]],\n",
       "\n",
       "         [[ 1.2451e-02, -1.2756e-02, -1.4038e-02,  ..., -2.2156e-02,\n",
       "           -3.7598e-01,  5.2832e-01],\n",
       "          [ 2.4785e+00,  1.3242e+00, -1.4160e-01,  ..., -9.7070e-01,\n",
       "           -5.2832e-01, -9.4385e-01],\n",
       "          [ 3.4570e+00,  2.0020e+00,  1.1465e+00,  ..., -3.1738e-01,\n",
       "            7.9346e-01,  7.0850e-01],\n",
       "          ...,\n",
       "          [-1.6484e+00,  9.9365e-01,  6.1133e-01,  ...,  1.8174e+00,\n",
       "            2.2925e-01,  2.6001e-02],\n",
       "          [-2.6836e+00,  1.8848e+00, -1.9775e+00,  ...,  1.0488e+00,\n",
       "           -1.7407e-01,  3.2275e-01],\n",
       "          [-7.9980e-01,  2.6191e+00, -3.3066e+00,  ..., -2.8784e-01,\n",
       "           -8.6523e-01,  1.3223e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0765e-02, -6.8665e-04,  6.9733e-03,  ..., -4.5117e-01,\n",
       "            1.7896e-01, -5.9692e-02],\n",
       "          [-1.5601e-01, -1.1804e-01, -2.4463e-01,  ..., -8.6328e-01,\n",
       "           -1.9990e+00, -8.9844e-01],\n",
       "          [-2.6245e-01,  5.5664e-02, -2.6611e-01,  ...,  1.1904e+00,\n",
       "           -1.6201e+00,  1.2812e+00],\n",
       "          ...,\n",
       "          [-2.1399e-01,  4.9756e-01, -3.3691e-01,  ...,  1.9961e+00,\n",
       "            4.8594e+00, -1.0225e+00],\n",
       "          [-9.2087e-03,  8.4717e-02,  2.3694e-01,  ...,  3.0273e+00,\n",
       "            1.5977e+00, -6.2988e-01],\n",
       "          [-3.5767e-02,  8.8379e-02,  1.5808e-01,  ...,  4.5664e+00,\n",
       "            3.5879e+00, -4.6729e-01]],\n",
       "\n",
       "         [[-1.2329e-02, -3.8361e-02, -4.5395e-04,  ...,  5.5695e-02,\n",
       "           -7.1594e-02,  1.1543e+00],\n",
       "          [ 9.7607e-01,  5.7959e-01, -5.6494e-01,  ...,  1.3848e+00,\n",
       "            2.9834e-01, -2.8086e+00],\n",
       "          [ 3.6094e+00, -2.9824e+00, -2.6074e+00,  ..., -1.8057e+00,\n",
       "           -1.9639e+00, -1.3789e+00],\n",
       "          ...,\n",
       "          [-3.1797e+00, -2.6025e-01,  1.3086e-01,  ...,  2.4854e-01,\n",
       "            1.2793e+00, -2.7520e+00],\n",
       "          [-7.5488e-01,  4.0771e-01,  7.9834e-01,  ...,  6.7725e-01,\n",
       "            1.7749e-01, -1.8691e+00],\n",
       "          [ 3.4277e-01, -1.1045e+00,  1.9023e+00,  ...,  3.5400e-01,\n",
       "           -8.5107e-01, -2.1016e+00]],\n",
       "\n",
       "         [[-3.1708e-02,  3.9886e-02, -1.1734e-02,  ...,  3.5858e-03,\n",
       "            2.3682e-02,  3.3752e-02],\n",
       "          [ 2.4473e+00,  1.5078e+00, -9.7998e-01,  ...,  1.2129e+00,\n",
       "           -6.7871e-01, -4.5947e-01],\n",
       "          [ 2.2461e+00,  1.9541e+00, -2.4102e+00,  ..., -1.0059e+00,\n",
       "            2.2715e+00,  7.9102e-01],\n",
       "          ...,\n",
       "          [-1.9297e+00,  5.0391e-01,  1.0811e+00,  ...,  1.6230e+00,\n",
       "            3.9185e-01, -5.4297e-01],\n",
       "          [-4.5959e-02,  8.4375e-01,  2.5177e-02,  ...,  6.5723e-01,\n",
       "            4.7314e-01,  1.2783e+00],\n",
       "          [ 8.1299e-01,  1.0605e+00,  6.2598e-01,  ...,  1.2061e+00,\n",
       "            3.1953e+00,  9.6094e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.4302e-02, -7.3853e-03,  1.0742e-02,  ...,  1.5137e-02,\n",
       "           -3.1006e-01,  3.4155e-01],\n",
       "          [-4.4897e-01, -4.8804e-01,  1.0767e-01,  ...,  9.0039e-01,\n",
       "           -6.3672e-01, -1.4023e+00],\n",
       "          [ 9.4141e-01, -9.8926e-01,  2.8735e-01,  ...,  2.2949e+00,\n",
       "           -1.0107e+00,  3.3457e+00],\n",
       "          ...,\n",
       "          [ 2.1191e+00,  8.3691e-01, -1.4873e+00,  ..., -1.3223e+00,\n",
       "           -3.2007e-01, -2.4316e+00],\n",
       "          [ 1.0811e+00, -1.1493e-01, -1.1016e+00,  ..., -1.2627e+00,\n",
       "           -5.0342e-01, -2.4688e+00],\n",
       "          [-8.8916e-01, -9.3555e-01, -1.9824e-01,  ..., -1.4492e+00,\n",
       "           -4.4507e-01, -2.0879e+00]],\n",
       "\n",
       "         [[ 6.3324e-03,  6.4850e-04, -2.0477e-02,  ..., -5.6934e-01,\n",
       "            2.2934e-02,  1.2488e-01],\n",
       "          [ 1.1194e-01,  2.8290e-02, -2.8149e-01,  ...,  2.0039e+00,\n",
       "           -2.3535e-01, -1.0713e+00],\n",
       "          [ 4.0497e-02, -2.1704e-01, -1.3782e-01,  ...,  9.1260e-01,\n",
       "           -2.8848e+00, -2.3945e+00],\n",
       "          ...,\n",
       "          [-3.8757e-02,  9.2651e-02,  2.3132e-01,  ...,  2.1777e-01,\n",
       "           -4.3477e+00,  3.5645e+00],\n",
       "          [-9.2102e-02,  2.1400e-03,  7.9102e-02,  ..., -1.8539e-02,\n",
       "           -4.2578e+00,  3.6816e+00],\n",
       "          [-6.0425e-02, -7.1777e-02, -1.2878e-01,  ..., -1.0059e-01,\n",
       "           -4.2969e+00,  3.6152e+00]],\n",
       "\n",
       "         [[ 1.2451e-02, -1.2756e-02, -1.4038e-02,  ..., -2.2156e-02,\n",
       "           -3.7598e-01,  5.2832e-01],\n",
       "          [ 2.4785e+00,  1.3242e+00, -1.4160e-01,  ..., -9.7070e-01,\n",
       "           -5.2832e-01, -9.4385e-01],\n",
       "          [ 3.4082e+00,  2.5723e+00,  9.2773e-01,  ...,  3.5205e-01,\n",
       "            1.3594e+00,  1.4316e+00],\n",
       "          ...,\n",
       "          [-1.6211e+00,  1.8311e-01,  2.9590e-01,  ..., -7.6514e-01,\n",
       "           -5.3174e-01,  1.4756e+00],\n",
       "          [-2.6074e+00,  1.0596e+00, -1.9629e+00,  ..., -8.2520e-01,\n",
       "           -5.8398e-01,  1.4629e+00],\n",
       "          [-1.2119e+00,  1.2188e+00, -3.4766e+00,  ..., -8.2080e-01,\n",
       "           -4.8340e-01,  1.4727e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0765e-02, -6.8665e-04,  6.9733e-03,  ..., -4.5117e-01,\n",
       "            1.7896e-01, -5.9692e-02],\n",
       "          [-1.5601e-01, -1.1804e-01, -2.4463e-01,  ..., -8.6328e-01,\n",
       "           -1.9990e+00, -8.9844e-01],\n",
       "          [-3.3472e-01,  5.8740e-01, -1.8542e-01,  ...,  2.7368e-01,\n",
       "           -1.1934e+00, -4.4922e-01],\n",
       "          ...,\n",
       "          [ 4.8523e-02,  1.0620e-02, -1.6846e-01,  ...,  3.4727e+00,\n",
       "            3.1914e+00, -7.7979e-01],\n",
       "          [ 8.8623e-02,  2.0203e-01, -6.4209e-02,  ...,  3.6797e+00,\n",
       "            3.2207e+00, -7.4756e-01],\n",
       "          [ 3.2440e-02,  2.2595e-01,  1.0687e-01,  ...,  3.6230e+00,\n",
       "            3.2617e+00, -7.6074e-01]],\n",
       "\n",
       "         [[-1.2329e-02, -3.8361e-02, -4.5395e-04,  ...,  5.5695e-02,\n",
       "           -7.1594e-02,  1.1543e+00],\n",
       "          [ 9.7607e-01,  5.7959e-01, -5.6494e-01,  ...,  1.3848e+00,\n",
       "            2.9834e-01, -2.8086e+00],\n",
       "          [ 2.0664e+00, -2.3438e+00, -1.1992e+00,  ..., -1.5479e+00,\n",
       "           -2.1426e+00, -3.6016e+00],\n",
       "          ...,\n",
       "          [-3.4590e+00,  1.0566e+00,  6.9385e-01,  ..., -4.6021e-01,\n",
       "           -5.1660e-01, -1.2822e+00],\n",
       "          [-2.7461e+00,  1.1963e-01,  1.7051e+00,  ..., -5.4199e-01,\n",
       "           -4.9316e-01, -1.3242e+00],\n",
       "          [ 3.8379e-01, -8.8379e-01,  2.0586e+00,  ..., -6.9434e-01,\n",
       "           -5.2441e-01, -1.3652e+00]],\n",
       "\n",
       "         [[-3.1708e-02,  3.9886e-02, -1.1734e-02,  ...,  3.5858e-03,\n",
       "            2.3682e-02,  3.3752e-02],\n",
       "          [ 2.4473e+00,  1.5078e+00, -9.7998e-01,  ...,  1.2129e+00,\n",
       "           -6.7871e-01, -4.5947e-01],\n",
       "          [ 2.0859e+00,  2.3887e+00, -2.6289e+00,  ..., -4.3481e-01,\n",
       "            1.6436e+00,  7.8711e-01],\n",
       "          ...,\n",
       "          [-1.0400e+00,  8.1055e-01,  5.5273e-01,  ...,  1.9434e+00,\n",
       "            2.5508e+00, -2.2510e-01],\n",
       "          [-6.7969e-01,  1.1631e+00,  5.6836e-01,  ...,  2.1465e+00,\n",
       "            2.7305e+00, -2.0654e-01],\n",
       "          [ 2.5439e-01,  9.0625e-01,  4.1187e-01,  ...,  2.0547e+00,\n",
       "            2.8711e+00, -1.2744e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 8.2474e-03, -1.9875e-03,  1.5594e-02,  ...,  4.7638e-02,\n",
       "            1.4061e-02,  2.7130e-02],\n",
       "          [ 5.3125e-01,  7.8711e-01, -6.3721e-02,  ..., -3.6304e-01,\n",
       "            1.3135e-01,  5.4150e-01],\n",
       "          [ 1.8066e-01,  2.2412e-01,  1.2280e-01,  ...,  5.4199e-01,\n",
       "           -4.1895e-01, -2.8125e-01],\n",
       "          ...,\n",
       "          [-8.0664e-01,  2.6074e-01, -1.1865e-01,  ...,  4.6844e-02,\n",
       "            1.9226e-01,  5.8441e-02],\n",
       "          [-1.0830e+00,  3.7915e-01, -5.9570e-01,  ..., -5.4395e-01,\n",
       "            1.1536e-01, -3.0487e-02],\n",
       "          [-3.8696e-01,  3.4363e-02, -2.4451e-01,  ...,  1.3660e-01,\n",
       "            2.8711e-01,  7.0215e-01]],\n",
       "\n",
       "         [[ 1.6235e-02,  1.6602e-02,  1.6346e-03,  ...,  5.8975e-03,\n",
       "           -1.2283e-03, -2.6855e-03],\n",
       "          [-2.5269e-01,  7.6758e-01, -8.2861e-01,  ..., -1.1455e+00,\n",
       "           -1.5225e+00,  5.1953e-01],\n",
       "          [ 2.4524e-01, -2.0801e-01,  1.7090e-01,  ...,  3.9502e-01,\n",
       "           -4.1113e-01,  4.7021e-01],\n",
       "          ...,\n",
       "          [-4.3750e-01,  5.4297e-01, -1.0059e+00,  ...,  6.7236e-01,\n",
       "           -7.2217e-01,  1.3857e+00],\n",
       "          [ 5.6885e-01, -4.5996e-01, -6.7432e-01,  ..., -8.5352e-01,\n",
       "           -1.2041e+00,  1.1162e+00],\n",
       "          [-9.4116e-02,  6.0352e-01,  3.5620e-01,  ...,  1.8005e-01,\n",
       "            7.6123e-01, -3.1079e-01]],\n",
       "\n",
       "         [[-8.3313e-03, -2.0248e-02, -2.4586e-03,  ...,  1.1414e-02,\n",
       "            1.4122e-02,  4.3188e-01],\n",
       "          [ 1.1639e-01,  5.9961e-01,  5.9967e-02,  ...,  5.7495e-02,\n",
       "            7.4316e-01, -1.2490e+00],\n",
       "          [-1.3342e-01,  2.5342e-01,  9.8389e-02,  ..., -1.4624e-01,\n",
       "           -6.8298e-02,  2.7808e-01],\n",
       "          ...,\n",
       "          [ 1.0244e+00,  5.7910e-01, -3.1763e-01,  ..., -4.8218e-01,\n",
       "           -6.3135e-01, -8.4082e-01],\n",
       "          [-9.6924e-02,  2.8027e-01,  3.7939e-01,  ...,  3.2251e-01,\n",
       "           -4.0723e-01, -8.6963e-01],\n",
       "          [-1.0999e-01,  1.1334e-01, -1.1316e-01,  ...,  3.9062e-01,\n",
       "            1.2891e-01, -6.1035e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.1658e-02, -3.1433e-03, -1.2390e-02,  ..., -1.3596e-02,\n",
       "           -1.0460e-02, -2.0584e-02],\n",
       "          [-1.6187e-01, -3.1836e-01, -1.9199e+00,  ..., -7.8027e-01,\n",
       "            5.8105e-01,  9.4849e-02],\n",
       "          [-1.1035e+00,  1.8389e+00, -5.1807e-01,  ..., -1.1113e+00,\n",
       "            3.0273e-01, -8.7109e-01],\n",
       "          ...,\n",
       "          [-3.2275e-01, -2.1814e-01, -1.9580e-01,  ..., -8.3740e-02,\n",
       "           -5.7678e-03, -4.8853e-01],\n",
       "          [-9.6558e-02, -3.1982e-01, -5.9875e-02,  ...,  3.7451e-01,\n",
       "            4.3164e-01,  4.2822e-01],\n",
       "          [ 4.4775e-01,  3.0029e-01,  2.3804e-01,  ...,  1.0986e-02,\n",
       "            2.8320e-01,  1.5479e-01]],\n",
       "\n",
       "         [[-9.3231e-03,  2.0142e-02, -2.8702e-02,  ..., -2.9434e-02,\n",
       "           -4.2725e-02, -7.6218e-03],\n",
       "          [ 8.1201e-01,  1.9385e-01,  4.8755e-01,  ...,  6.2646e-01,\n",
       "            4.7302e-04,  1.6016e-01],\n",
       "          [ 5.5273e-01, -2.6904e-01,  4.8145e-01,  ...,  8.2178e-01,\n",
       "           -5.5176e-01, -1.0864e-01],\n",
       "          ...,\n",
       "          [ 8.9600e-01, -4.3311e-01, -2.1301e-01,  ...,  3.0151e-01,\n",
       "           -7.9932e-01,  2.1240e-01],\n",
       "          [ 7.8906e-01, -7.7246e-01,  1.4678e+00,  ...,  1.0381e+00,\n",
       "            3.1567e-01,  3.7036e-01],\n",
       "          [ 5.6250e-01, -6.8311e-01,  4.9170e-01,  ...,  1.1353e-01,\n",
       "            2.7161e-03,  5.4736e-01]],\n",
       "\n",
       "         [[-2.4109e-03,  1.4191e-03,  2.0081e-02,  ...,  7.3090e-03,\n",
       "           -1.8234e-02,  3.4515e-02],\n",
       "          [-1.2000e-01, -6.4941e-01,  1.5161e-01,  ..., -4.1602e-01,\n",
       "            1.4832e-01, -2.1790e-01],\n",
       "          [-3.4943e-02, -1.5356e-01, -2.1729e-01,  ...,  3.5425e-01,\n",
       "            4.5337e-01, -4.4458e-01],\n",
       "          ...,\n",
       "          [-1.4111e-01, -1.9470e-01, -3.3228e-01,  ...,  6.5430e-02,\n",
       "           -2.6807e-01, -7.1533e-01],\n",
       "          [-6.3867e-01, -2.6929e-01,  7.3975e-02,  ...,  1.6528e-01,\n",
       "           -6.3672e-01,  2.4780e-02],\n",
       "          [-2.6440e-01, -6.2549e-01,  1.0095e-01,  ...,  6.5308e-03,\n",
       "            1.5820e-01,  6.6284e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 8.2474e-03, -1.9875e-03,  1.5594e-02,  ...,  4.7638e-02,\n",
       "            1.4061e-02,  2.7130e-02],\n",
       "          [ 5.3125e-01,  7.8711e-01, -6.3721e-02,  ..., -3.6304e-01,\n",
       "            1.3135e-01,  5.4150e-01],\n",
       "          [ 9.3555e-01,  3.6182e-01,  4.6826e-01,  ..., -1.0371e+00,\n",
       "           -1.6089e-01, -1.0752e+00],\n",
       "          ...,\n",
       "          [-1.6785e-01,  3.1079e-01, -2.5848e-02,  ...,  1.7041e-01,\n",
       "           -1.3257e-01,  1.5210e-01],\n",
       "          [-1.8945e-01,  2.7856e-01, -8.0383e-02,  ...,  1.7493e-01,\n",
       "           -3.8147e-04,  1.3574e-01],\n",
       "          [-1.5869e-01,  2.8442e-01, -6.7688e-02,  ...,  1.8298e-01,\n",
       "           -5.9845e-02,  1.6602e-01]],\n",
       "\n",
       "         [[ 1.6235e-02,  1.6602e-02,  1.6346e-03,  ...,  5.8975e-03,\n",
       "           -1.2283e-03, -2.6855e-03],\n",
       "          [-2.5269e-01,  7.6758e-01, -8.2861e-01,  ..., -1.1455e+00,\n",
       "           -1.5225e+00,  5.1953e-01],\n",
       "          [ 1.1719e+00, -8.2764e-02,  1.6914e+00,  ...,  7.6855e-01,\n",
       "            3.6469e-03, -9.1370e-02],\n",
       "          ...,\n",
       "          [ 3.7842e-02,  6.6016e-01, -6.3965e-01,  ..., -5.5713e-01,\n",
       "            1.4072e+00,  7.1143e-01],\n",
       "          [ 4.7729e-02,  5.9131e-01, -6.8750e-01,  ..., -5.7275e-01,\n",
       "            1.3750e+00,  6.5918e-01],\n",
       "          [-1.5747e-02,  5.0098e-01, -7.3096e-01,  ..., -6.2012e-01,\n",
       "            1.4746e+00,  5.7471e-01]],\n",
       "\n",
       "         [[-8.3313e-03, -2.0248e-02, -2.4586e-03,  ...,  1.1414e-02,\n",
       "            1.4122e-02,  4.3188e-01],\n",
       "          [ 1.1639e-01,  5.9961e-01,  5.9967e-02,  ...,  5.7495e-02,\n",
       "            7.4316e-01, -1.2490e+00],\n",
       "          [ 3.9575e-01,  5.1367e-01, -6.2598e-01,  ...,  5.2551e-02,\n",
       "            5.9387e-02, -3.5986e-01],\n",
       "          ...,\n",
       "          [ 6.0669e-02,  5.8545e-01,  1.4929e-01,  ...,  4.9072e-01,\n",
       "           -3.4766e-01, -8.6670e-01],\n",
       "          [ 2.8931e-02,  5.9668e-01,  1.7920e-01,  ...,  5.0488e-01,\n",
       "           -3.4448e-01, -9.1113e-01],\n",
       "          [ 5.8838e-02,  6.2402e-01,  1.8542e-01,  ...,  4.9756e-01,\n",
       "           -3.1348e-01, -8.2324e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.1658e-02, -3.1433e-03, -1.2390e-02,  ..., -1.3596e-02,\n",
       "           -1.0460e-02, -2.0584e-02],\n",
       "          [-1.6187e-01, -3.1836e-01, -1.9199e+00,  ..., -7.8027e-01,\n",
       "            5.8105e-01,  9.4849e-02],\n",
       "          [-4.3311e-01, -7.9834e-01, -1.1609e-01,  ..., -1.1768e+00,\n",
       "            1.2529e+00, -1.2012e+00],\n",
       "          ...,\n",
       "          [-9.5898e-01,  4.5898e-02,  9.1064e-02,  ..., -4.0527e-01,\n",
       "           -1.4941e-01,  6.0303e-01],\n",
       "          [-8.9062e-01,  8.7952e-02,  1.7090e-02,  ..., -2.8174e-01,\n",
       "           -2.1436e-01,  5.4492e-01],\n",
       "          [-9.6973e-01,  2.3877e-01, -2.0187e-02,  ..., -4.4678e-01,\n",
       "           -3.2300e-01,  6.6406e-01]],\n",
       "\n",
       "         [[-9.3231e-03,  2.0142e-02, -2.8702e-02,  ..., -2.9434e-02,\n",
       "           -4.2725e-02, -7.6218e-03],\n",
       "          [ 8.1201e-01,  1.9385e-01,  4.8755e-01,  ...,  6.2646e-01,\n",
       "            4.7302e-04,  1.6016e-01],\n",
       "          [-5.9814e-01,  4.5752e-01, -1.4294e-01,  ..., -3.6084e-01,\n",
       "            5.0000e-01,  1.5051e-01],\n",
       "          ...,\n",
       "          [-2.0850e-01, -7.2754e-01, -1.1401e-01,  ...,  3.0591e-01,\n",
       "            3.8025e-02,  6.9141e-01],\n",
       "          [-2.2998e-01, -6.8262e-01,  3.5553e-03,  ...,  4.3042e-01,\n",
       "            8.8684e-02,  7.6953e-01],\n",
       "          [-1.9812e-01, -7.0410e-01,  4.0497e-02,  ...,  4.1577e-01,\n",
       "            7.1777e-02,  7.2021e-01]],\n",
       "\n",
       "         [[-2.4109e-03,  1.4191e-03,  2.0081e-02,  ...,  7.3090e-03,\n",
       "           -1.8234e-02,  3.4515e-02],\n",
       "          [-1.2000e-01, -6.4941e-01,  1.5161e-01,  ..., -4.1602e-01,\n",
       "            1.4832e-01, -2.1790e-01],\n",
       "          [-3.9746e-01, -2.0068e-01, -1.1743e-01,  ...,  2.7612e-01,\n",
       "           -7.7820e-02,  1.6205e-02],\n",
       "          ...,\n",
       "          [-5.6445e-01, -2.8613e-01,  4.8438e-01,  ...,  6.4600e-01,\n",
       "            8.3801e-02, -3.7744e-01],\n",
       "          [-6.0352e-01, -2.9492e-01,  4.3408e-01,  ...,  6.3135e-01,\n",
       "            4.6539e-02, -3.3936e-01],\n",
       "          [-6.4258e-01, -2.5342e-01,  4.3848e-01,  ...,  6.4697e-01,\n",
       "            1.2012e-01, -2.8442e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-4.1443e-02, -9.8648e-03, -4.6753e-02,  ...,  8.8257e-02,\n",
       "            8.0322e-02, -3.4741e-01],\n",
       "          [ 2.5664e+00,  1.4199e+00,  9.1406e-01,  ...,  5.2930e-01,\n",
       "           -4.4995e-01,  6.4258e-01],\n",
       "          [ 3.9531e+00,  6.3281e-01,  1.3633e+00,  ...,  1.6504e+00,\n",
       "            1.2891e+00, -3.2471e-01],\n",
       "          ...,\n",
       "          [-2.3398e+00,  8.1006e-01, -9.7021e-01,  ..., -6.7188e-01,\n",
       "            5.7520e-01,  2.0825e-01],\n",
       "          [-3.5605e+00, -2.9956e-01,  1.0645e+00,  ..., -4.8096e-01,\n",
       "            1.7842e+00,  8.2178e-01],\n",
       "          [-1.1982e+00, -7.1924e-01,  9.3311e-01,  ..., -1.1523e+00,\n",
       "            4.0859e+00, -5.9082e-02]],\n",
       "\n",
       "         [[ 4.8981e-03, -2.1057e-02, -3.8391e-02,  ...,  6.0791e-01,\n",
       "           -4.6240e-01, -8.9453e-01],\n",
       "          [ 1.8799e+00,  2.3291e-01, -5.0977e-01,  ..., -7.5635e-01,\n",
       "           -1.8079e-01, -2.7637e-01],\n",
       "          [ 3.3711e+00, -1.3213e+00, -1.7441e+00,  ...,  1.9257e-02,\n",
       "           -3.0918e+00,  5.9961e-01],\n",
       "          ...,\n",
       "          [-1.5566e+00, -4.2725e-01,  3.4229e-01,  ...,  1.4150e+00,\n",
       "           -2.6641e+00, -6.2378e-02],\n",
       "          [-2.4941e+00, -2.2129e+00,  2.0996e+00,  ..., -2.5146e-01,\n",
       "           -2.0781e+00, -3.5449e-01],\n",
       "          [-1.7617e+00, -2.0254e+00,  2.3125e+00,  ...,  6.1621e-01,\n",
       "           -1.9414e+00, -4.9042e-02]],\n",
       "\n",
       "         [[-3.9062e-03,  1.0223e-02,  6.6895e-02,  ..., -4.3774e-01,\n",
       "           -4.1040e-01,  5.1514e-01],\n",
       "          [-6.7773e-01, -1.8359e+00,  8.3984e-01,  ..., -1.9717e+00,\n",
       "           -2.5801e+00,  4.6460e-01],\n",
       "          [ 6.1426e-01, -2.2305e+00, -6.9678e-01,  ..., -3.3340e+00,\n",
       "           -1.8896e-01,  4.5312e-01],\n",
       "          ...,\n",
       "          [ 1.4238e+00, -2.7383e+00, -9.3359e-01,  ..., -1.2363e+00,\n",
       "           -5.7129e-01, -2.5781e+00],\n",
       "          [ 1.2134e-01, -2.3359e+00,  5.3857e-01,  ..., -2.3340e+00,\n",
       "           -2.9668e+00, -3.3203e-01],\n",
       "          [-1.7939e+00, -2.8320e-02,  8.2910e-01,  ..., -2.5605e+00,\n",
       "           -2.6914e+00, -1.3691e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.5073e-03, -3.1052e-02,  1.3077e-02,  ...,  3.0688e-01,\n",
       "           -1.9824e-01,  8.6523e-01],\n",
       "          [-1.9067e-01, -8.2947e-02,  1.1133e-01,  ...,  2.3535e+00,\n",
       "           -8.4766e-01, -5.8008e-01],\n",
       "          [ 1.8262e-01, -5.4779e-02, -1.8372e-01,  ...,  1.5742e+00,\n",
       "           -7.0410e-01,  3.2422e-01],\n",
       "          ...,\n",
       "          [ 2.3291e-01,  6.6406e-02, -1.2024e-02,  ..., -2.8242e+00,\n",
       "            3.4521e-01, -3.6250e+00],\n",
       "          [ 4.5703e-01, -4.0344e-02, -2.9053e-01,  ..., -3.1836e-01,\n",
       "           -2.8301e+00, -8.9990e-01],\n",
       "          [-1.6711e-01, -1.6431e-01, -9.4849e-02,  ..., -1.5098e+00,\n",
       "           -3.3828e+00, -7.5586e-01]],\n",
       "\n",
       "         [[-3.3630e-02, -3.3905e-02,  3.2349e-02,  ...,  1.2323e-01,\n",
       "            2.3755e-01,  9.2102e-02],\n",
       "          [-2.9688e+00,  3.4316e+00, -1.9424e+00,  ...,  1.1641e+00,\n",
       "            3.6914e-01,  8.5986e-01],\n",
       "          [-8.2734e+00,  2.0859e+00,  1.6074e+00,  ...,  1.1934e+00,\n",
       "            4.9121e-01,  1.5491e-01],\n",
       "          ...,\n",
       "          [ 1.4805e+00,  5.5078e+00,  1.0547e-01,  ...,  6.9629e-01,\n",
       "           -1.6846e-01, -1.2741e-03],\n",
       "          [ 6.4961e+00,  4.0898e+00, -2.2871e+00,  ...,  4.5508e-01,\n",
       "            2.8174e-01,  5.1611e-01],\n",
       "          [ 6.2383e+00, -2.0703e-01, -4.4453e+00,  ...,  1.7744e+00,\n",
       "           -8.2626e-03, -1.0376e-01]],\n",
       "\n",
       "         [[-1.1597e-02, -1.5137e-02,  1.4885e-02,  ..., -3.0249e-01,\n",
       "            2.5977e-01, -1.7920e-01],\n",
       "          [ 1.5596e+00,  1.7859e-01, -4.3506e-01,  ...,  4.9072e-01,\n",
       "           -7.6953e-01, -9.1211e-01],\n",
       "          [ 4.6641e+00, -2.0039e+00, -2.8184e+00,  ...,  4.7900e-01,\n",
       "            5.0977e-01, -2.2832e+00],\n",
       "          ...,\n",
       "          [-1.1602e+00,  5.9619e-01,  3.4961e+00,  ...,  1.0098e+00,\n",
       "           -2.9316e+00,  4.0430e-01],\n",
       "          [-1.8291e+00, -1.6797e-01,  1.0527e+00,  ...,  2.0664e+00,\n",
       "           -1.5762e+00, -1.0425e-01],\n",
       "          [-6.5918e-03,  8.6243e-02,  3.9380e-01,  ...,  5.9131e-01,\n",
       "           -1.9297e+00,  1.3379e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.1443e-02, -9.8648e-03, -4.6753e-02,  ...,  8.8257e-02,\n",
       "            8.0322e-02, -3.4741e-01],\n",
       "          [ 2.5664e+00,  1.4199e+00,  9.1406e-01,  ...,  5.2930e-01,\n",
       "           -4.4995e-01,  6.4258e-01],\n",
       "          [ 3.9941e+00,  7.5049e-01,  1.2168e+00,  ...,  2.4121e+00,\n",
       "            4.9170e-01, -7.2949e-01],\n",
       "          ...,\n",
       "          [-1.4805e+00,  2.1172e+00, -7.5342e-01,  ...,  6.3110e-02,\n",
       "            3.3027e+00,  1.9861e-01],\n",
       "          [-3.1855e+00,  6.2012e-01, -5.3162e-02,  ...,  9.0271e-02,\n",
       "            3.1484e+00,  7.2144e-02],\n",
       "          [-1.8721e+00, -1.2285e+00,  7.7881e-01,  ...,  1.9043e-01,\n",
       "            3.2773e+00, -9.5581e-02]],\n",
       "\n",
       "         [[ 4.8981e-03, -2.1057e-02, -3.8391e-02,  ...,  6.0791e-01,\n",
       "           -4.6240e-01, -8.9453e-01],\n",
       "          [ 1.8799e+00,  2.3291e-01, -5.0977e-01,  ..., -7.5635e-01,\n",
       "           -1.8079e-01, -2.7637e-01],\n",
       "          [ 3.0312e+00, -9.0576e-01, -1.8633e+00,  ..., -2.6099e-01,\n",
       "           -2.3027e+00, -1.6406e+00],\n",
       "          ...,\n",
       "          [-3.6133e-02, -7.1582e-01, -1.2207e-01,  ..., -7.5635e-01,\n",
       "           -2.5566e+00, -6.6992e-01],\n",
       "          [-1.5566e+00, -1.6406e+00,  1.0371e+00,  ..., -5.9668e-01,\n",
       "           -2.4473e+00, -7.3633e-01],\n",
       "          [-1.6445e+00, -1.5361e+00,  1.7148e+00,  ..., -5.1904e-01,\n",
       "           -2.5566e+00, -7.8809e-01]],\n",
       "\n",
       "         [[-3.9062e-03,  1.0223e-02,  6.6895e-02,  ..., -4.3774e-01,\n",
       "           -4.1040e-01,  5.1514e-01],\n",
       "          [-6.7773e-01, -1.8359e+00,  8.3984e-01,  ..., -1.9717e+00,\n",
       "           -2.5801e+00,  4.6460e-01],\n",
       "          [ 1.0986e+00, -1.6191e+00, -2.5024e-03,  ..., -2.7441e+00,\n",
       "            3.3838e-01,  1.0967e+00],\n",
       "          ...,\n",
       "          [ 2.6484e+00, -2.0723e+00, -5.3760e-01,  ..., -1.8535e+00,\n",
       "           -3.0684e+00,  6.4648e-01],\n",
       "          [ 1.1584e-01, -1.4238e+00,  2.7100e-01,  ..., -1.8467e+00,\n",
       "           -2.9883e+00,  5.3516e-01],\n",
       "          [-2.4297e+00,  1.5381e-01,  7.1680e-01,  ..., -1.8760e+00,\n",
       "           -2.9961e+00,  5.4492e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.5073e-03, -3.1052e-02,  1.3077e-02,  ...,  3.0688e-01,\n",
       "           -1.9824e-01,  8.6523e-01],\n",
       "          [-1.9067e-01, -8.2947e-02,  1.1133e-01,  ...,  2.3535e+00,\n",
       "           -8.4766e-01, -5.8008e-01],\n",
       "          [ 1.3037e-01,  6.0791e-02, -2.3047e-01,  ...,  1.3457e+00,\n",
       "           -1.3164e+00, -3.0298e-01],\n",
       "          ...,\n",
       "          [-4.2480e-01, -1.0504e-01,  6.1621e-01,  ...,  1.3757e-01,\n",
       "           -2.0430e+00,  1.3189e-03],\n",
       "          [-1.2573e-01,  5.0110e-02,  3.6816e-01,  ...,  1.9031e-01,\n",
       "           -2.1777e+00, -6.7505e-02],\n",
       "          [ 2.2095e-01,  1.5942e-01,  7.1289e-02,  ...,  1.9153e-01,\n",
       "           -2.2773e+00, -1.7004e-01]],\n",
       "\n",
       "         [[-3.3630e-02, -3.3905e-02,  3.2349e-02,  ...,  1.2323e-01,\n",
       "            2.3755e-01,  9.2102e-02],\n",
       "          [-2.9688e+00,  3.4316e+00, -1.9424e+00,  ...,  1.1641e+00,\n",
       "            3.6914e-01,  8.5986e-01],\n",
       "          [-7.3867e+00,  1.5264e+00,  2.2969e+00,  ...,  7.9688e-01,\n",
       "           -1.0811e+00, -4.3921e-01],\n",
       "          ...,\n",
       "          [ 3.7109e-01,  4.2109e+00,  6.3184e-01,  ...,  1.4268e+00,\n",
       "            3.5986e-01,  3.6401e-01],\n",
       "          [ 4.8984e+00,  3.3535e+00, -1.8818e+00,  ...,  1.5215e+00,\n",
       "            4.8999e-01,  2.8809e-01],\n",
       "          [ 5.1680e+00,  3.8086e-01, -3.5000e+00,  ...,  1.6211e+00,\n",
       "            4.1650e-01,  4.2017e-01]],\n",
       "\n",
       "         [[-1.1597e-02, -1.5137e-02,  1.4885e-02,  ..., -3.0249e-01,\n",
       "            2.5977e-01, -1.7920e-01],\n",
       "          [ 1.5596e+00,  1.7859e-01, -4.3506e-01,  ...,  4.9072e-01,\n",
       "           -7.6953e-01, -9.1211e-01],\n",
       "          [ 4.7539e+00, -2.1211e+00, -2.6133e+00,  ...,  6.8457e-01,\n",
       "            6.0986e-01, -1.8555e+00],\n",
       "          ...,\n",
       "          [ 1.4038e-01,  5.7715e-01,  9.1064e-02,  ...,  1.3145e+00,\n",
       "           -5.7520e-01,  2.8857e-01],\n",
       "          [-6.4697e-01,  7.9285e-02,  5.2686e-01,  ...,  1.4072e+00,\n",
       "           -5.9424e-01,  1.1499e-01],\n",
       "          [-7.1045e-01, -3.0884e-01,  8.0127e-01,  ...,  1.4238e+00,\n",
       "           -5.5957e-01,  2.3401e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-1.4244e-02, -3.5591e-03, -2.8610e-02,  ...,  3.4454e-02,\n",
       "            2.0950e-02,  1.5480e-02],\n",
       "          [-4.5557e-01, -1.4929e-01,  6.3135e-01,  ..., -4.0186e-01,\n",
       "           -4.1626e-02, -5.2637e-01],\n",
       "          [ 2.0538e-02,  4.9805e-01, -5.0244e-01,  ...,  1.1934e+00,\n",
       "            1.5576e-01, -5.9052e-02],\n",
       "          ...,\n",
       "          [ 1.5857e-01, -5.7031e-01, -3.2764e-01,  ...,  2.2498e-01,\n",
       "            6.8726e-02, -7.9590e-01],\n",
       "          [ 7.7441e-01, -2.1143e-01,  2.4365e-01,  ...,  1.6431e-01,\n",
       "           -1.5576e-01, -8.3496e-01],\n",
       "          [ 1.2073e-01,  6.7993e-02, -1.7773e-01,  ..., -5.2393e-01,\n",
       "           -2.3853e-01, -8.8770e-01]],\n",
       "\n",
       "         [[ 8.1635e-03, -1.2733e-02, -5.9700e-03,  ..., -6.7368e-03,\n",
       "            1.1650e-02, -1.9974e-02],\n",
       "          [-4.9530e-02,  6.6345e-02,  3.6987e-01,  ...,  5.8105e-01,\n",
       "           -1.3412e-02,  9.5215e-02],\n",
       "          [ 2.2766e-02,  6.1279e-02, -5.3925e-02,  ...,  3.5767e-01,\n",
       "           -7.8857e-01,  6.4355e-01],\n",
       "          ...,\n",
       "          [-8.8379e-01, -1.0449e+00,  7.8320e-01,  ..., -3.1226e-01,\n",
       "           -1.0754e-01,  4.6436e-01],\n",
       "          [-9.9707e-01, -6.2256e-02,  1.0596e+00,  ..., -2.0422e-01,\n",
       "           -1.6125e-01,  3.3301e-01],\n",
       "          [-3.9502e-01,  9.2163e-03,  1.8005e-02,  ...,  7.5378e-02,\n",
       "           -1.9421e-01,  4.4922e-01]],\n",
       "\n",
       "         [[ 1.2650e-02, -2.8687e-02,  1.8005e-02,  ...,  9.6817e-03,\n",
       "           -1.4595e-02,  2.0313e-03],\n",
       "          [-4.3762e-02, -3.1152e-01, -1.1530e-01,  ...,  6.0852e-02,\n",
       "           -8.6914e-02, -3.1555e-02],\n",
       "          [ 1.6980e-01,  1.2573e-01, -3.1860e-01,  ..., -5.4395e-01,\n",
       "           -2.8247e-01, -6.3354e-02],\n",
       "          ...,\n",
       "          [ 2.9373e-02, -3.5498e-01, -9.1003e-02,  ...,  6.8176e-02,\n",
       "           -2.4194e-01,  5.4810e-02],\n",
       "          [ 3.7695e-01, -1.2573e-01, -3.4839e-01,  ...,  7.1411e-02,\n",
       "           -3.9868e-01,  5.4657e-02],\n",
       "          [-2.6489e-01,  2.7710e-01,  2.3193e-01,  ...,  5.0629e-02,\n",
       "            2.7832e-01,  2.0190e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-7.7896e-03,  3.0556e-03, -2.6894e-03,  ...,  8.5907e-03,\n",
       "            8.4534e-03, -8.7204e-03],\n",
       "          [ 7.1875e-01,  1.6586e-02, -1.1621e-01,  ...,  6.4697e-02,\n",
       "            3.5889e-01, -2.4170e-01],\n",
       "          [ 1.3457e+00,  4.5923e-01, -2.0645e+00,  ...,  1.2109e+00,\n",
       "           -8.5498e-01,  6.1084e-01],\n",
       "          ...,\n",
       "          [-3.9087e-01, -1.9263e-01, -2.5439e-01,  ...,  7.1960e-02,\n",
       "           -3.8232e-01, -3.8666e-02],\n",
       "          [-3.3350e-01, -2.4414e-01, -5.0732e-01,  ..., -3.8525e-01,\n",
       "            4.5581e-01,  1.3892e-01],\n",
       "          [-4.8767e-02, -3.9233e-01, -3.8574e-01,  ...,  5.3906e-01,\n",
       "           -3.3179e-01, -1.8982e-01]],\n",
       "\n",
       "         [[-2.3102e-02,  3.6865e-02, -6.6833e-02,  ..., -1.1749e-02,\n",
       "            2.6550e-03, -1.0742e-02],\n",
       "          [-6.6943e-01,  3.7781e-02,  1.0312e+00,  ...,  1.1920e-01,\n",
       "            6.6357e-01,  5.0586e-01],\n",
       "          [-2.8662e-01, -7.2168e-01,  6.6943e-01,  ..., -4.6899e-01,\n",
       "            4.1968e-01, -2.1716e-01],\n",
       "          ...,\n",
       "          [-2.6416e-01,  3.5840e-01,  8.8330e-01,  ..., -4.5654e-01,\n",
       "            4.5703e-01,  1.5601e-01],\n",
       "          [ 3.0249e-01, -2.3132e-01,  1.2207e+00,  ..., -9.9316e-01,\n",
       "           -6.7090e-01,  4.9744e-02],\n",
       "          [ 1.4355e-01,  2.2095e-02,  1.4258e+00,  ..., -1.1798e-01,\n",
       "           -3.9746e-01, -3.6719e-01]],\n",
       "\n",
       "         [[ 1.4328e-02,  1.8265e-02,  3.0136e-03,  ...,  2.1530e-02,\n",
       "            5.6381e-03, -6.3744e-03],\n",
       "          [ 7.0410e-01,  4.8889e-02,  4.2236e-01,  ...,  4.7827e-01,\n",
       "           -2.6831e-01,  2.6270e-01],\n",
       "          [-9.3994e-02, -6.5967e-01, -1.1391e-02,  ...,  5.1318e-01,\n",
       "           -7.1924e-01,  2.2412e-01],\n",
       "          ...,\n",
       "          [-4.6069e-01,  2.4915e-01, -2.0728e-01,  ..., -4.2542e-02,\n",
       "           -2.7075e-01,  2.2217e-01],\n",
       "          [-4.1748e-01,  1.4717e-02, -1.0474e-01,  ...,  7.9224e-02,\n",
       "           -7.4805e-01,  3.0371e-01],\n",
       "          [-2.2913e-01, -3.9526e-01,  4.5703e-01,  ...,  1.5857e-01,\n",
       "           -4.1675e-01, -5.7129e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.4244e-02, -3.5591e-03, -2.8610e-02,  ...,  3.4454e-02,\n",
       "            2.0950e-02,  1.5480e-02],\n",
       "          [-4.5557e-01, -1.4929e-01,  6.3135e-01,  ..., -4.0186e-01,\n",
       "           -4.1626e-02, -5.2637e-01],\n",
       "          [ 3.7036e-01,  2.7817e-02,  9.4727e-02,  ..., -1.7712e-01,\n",
       "           -4.5776e-01,  2.9736e-01],\n",
       "          ...,\n",
       "          [-3.0518e-03, -2.7710e-01, -1.4941e-01,  ..., -2.3120e-01,\n",
       "           -2.2253e-01, -8.3105e-01],\n",
       "          [ 3.9825e-02, -1.8298e-01, -1.3660e-01,  ..., -1.9287e-01,\n",
       "           -1.8945e-01, -8.0273e-01],\n",
       "          [ 4.9683e-02, -2.0703e-01, -3.1586e-02,  ..., -2.6196e-01,\n",
       "           -1.3403e-01, -8.1348e-01]],\n",
       "\n",
       "         [[ 8.1635e-03, -1.2733e-02, -5.9700e-03,  ..., -6.7368e-03,\n",
       "            1.1650e-02, -1.9974e-02],\n",
       "          [-4.9530e-02,  6.6345e-02,  3.6987e-01,  ...,  5.8105e-01,\n",
       "           -1.3412e-02,  9.5215e-02],\n",
       "          [-8.0029e-01, -8.1543e-02,  4.4775e-01,  ...,  1.6861e-02,\n",
       "           -1.1514e+00,  3.2275e-01],\n",
       "          ...,\n",
       "          [-1.4954e-01,  1.5759e-01,  6.0596e-01,  ..., -7.1826e-01,\n",
       "            1.0029e+00, -3.7524e-01],\n",
       "          [-4.9072e-02,  1.9409e-01,  5.7373e-01,  ..., -7.7051e-01,\n",
       "            9.0088e-01, -3.3984e-01],\n",
       "          [ 6.2683e-02,  1.4758e-01,  4.2383e-01,  ..., -6.2256e-01,\n",
       "            8.8623e-01, -3.7354e-01]],\n",
       "\n",
       "         [[ 1.2650e-02, -2.8687e-02,  1.8005e-02,  ...,  9.6817e-03,\n",
       "           -1.4595e-02,  2.0313e-03],\n",
       "          [-4.3762e-02, -3.1152e-01, -1.1530e-01,  ...,  6.0852e-02,\n",
       "           -8.6914e-02, -3.1555e-02],\n",
       "          [ 3.1641e-01,  2.9614e-01, -5.2832e-01,  ..., -1.3512e-02,\n",
       "            1.0859e+00, -6.5576e-01],\n",
       "          ...,\n",
       "          [-1.6919e-01,  2.8833e-01,  1.1528e-02,  ...,  7.0374e-02,\n",
       "           -2.5955e-02,  4.4556e-03],\n",
       "          [-2.1765e-01,  3.4399e-01, -5.1819e-02,  ..., -7.8773e-04,\n",
       "           -1.0236e-01,  9.9487e-02],\n",
       "          [-1.8799e-01,  1.7432e-01, -6.1249e-02,  ...,  3.2776e-02,\n",
       "           -1.1780e-01,  1.0205e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-7.7896e-03,  3.0556e-03, -2.6894e-03,  ...,  8.5907e-03,\n",
       "            8.4534e-03, -8.7204e-03],\n",
       "          [ 7.1875e-01,  1.6586e-02, -1.1621e-01,  ...,  6.4697e-02,\n",
       "            3.5889e-01, -2.4170e-01],\n",
       "          [ 1.3027e+00,  1.0029e+00, -8.3203e-01,  ...,  1.2178e+00,\n",
       "            1.6028e-01,  1.1621e+00],\n",
       "          ...,\n",
       "          [-2.8345e-01, -5.9180e-01,  7.6965e-02,  ...,  1.4473e+00,\n",
       "            6.9397e-02,  4.7656e-01],\n",
       "          [-2.8589e-01, -5.6787e-01,  5.0201e-02,  ...,  1.3896e+00,\n",
       "           -7.7148e-02,  4.9292e-01],\n",
       "          [-3.6572e-01, -5.6250e-01,  2.9724e-02,  ...,  1.3320e+00,\n",
       "           -1.6870e-01,  4.9780e-01]],\n",
       "\n",
       "         [[-2.3102e-02,  3.6865e-02, -6.6833e-02,  ..., -1.1749e-02,\n",
       "            2.6550e-03, -1.0742e-02],\n",
       "          [-6.6943e-01,  3.7781e-02,  1.0312e+00,  ...,  1.1920e-01,\n",
       "            6.6357e-01,  5.0586e-01],\n",
       "          [-3.7329e-01, -1.5674e-01,  8.5498e-01,  ..., -2.0581e-01,\n",
       "            3.9368e-02, -3.7354e-02],\n",
       "          ...,\n",
       "          [ 3.3417e-02, -2.2241e-01,  1.1836e+00,  ..., -6.8604e-02,\n",
       "           -5.4150e-01,  3.4570e-01],\n",
       "          [ 9.7473e-02, -1.7944e-01,  1.2061e+00,  ..., -2.8992e-02,\n",
       "           -5.6104e-01,  3.0835e-01],\n",
       "          [ 8.0811e-02, -2.6611e-01,  1.1953e+00,  ..., -4.8615e-02,\n",
       "           -4.5239e-01,  3.2422e-01]],\n",
       "\n",
       "         [[ 1.4328e-02,  1.8265e-02,  3.0136e-03,  ...,  2.1530e-02,\n",
       "            5.6381e-03, -6.3744e-03],\n",
       "          [ 7.0410e-01,  4.8889e-02,  4.2236e-01,  ...,  4.7827e-01,\n",
       "           -2.6831e-01,  2.6270e-01],\n",
       "          [-5.1208e-02,  3.7012e-01,  7.0508e-01,  ..., -3.5156e-01,\n",
       "           -4.9561e-01,  5.8447e-01],\n",
       "          ...,\n",
       "          [-7.8552e-02, -3.5474e-01, -1.2207e-03,  ...,  5.9998e-02,\n",
       "           -4.4360e-01, -1.3757e-01],\n",
       "          [ 5.5733e-03, -4.0576e-01,  2.9846e-02,  ...,  4.3152e-02,\n",
       "           -4.5142e-01, -5.0690e-02],\n",
       "          [-3.7476e-02, -4.1187e-01,  5.4504e-02,  ..., -5.2185e-03,\n",
       "           -4.7412e-01, -2.1460e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.0956e-02,  2.1988e-02, -8.9722e-03,  ..., -4.0576e-01,\n",
       "            6.7993e-02, -6.7773e-01],\n",
       "          [-5.3613e-01,  6.2012e-02, -7.6953e-01,  ..., -1.5312e+00,\n",
       "            2.2339e-02, -1.4999e-02],\n",
       "          [-1.3086e+00, -1.8047e+00, -1.1514e+00,  ..., -1.5020e+00,\n",
       "           -1.5020e+00, -2.2144e-01],\n",
       "          ...,\n",
       "          [-5.2734e-01,  1.5479e-01,  1.3613e+00,  ...,  1.8838e+00,\n",
       "            1.7324e+00, -4.5459e-01],\n",
       "          [ 1.6885e+00, -1.0469e+00, -1.5254e+00,  ...,  5.4883e-01,\n",
       "            1.0195e+00,  2.3608e-01],\n",
       "          [ 2.5859e+00, -3.8403e-01, -1.8057e+00,  ...,  3.1465e+00,\n",
       "            1.0010e-01,  8.5010e-01]],\n",
       "\n",
       "         [[-1.2207e-01, -7.2021e-02, -6.4697e-02,  ..., -1.0437e-02,\n",
       "            1.8506e-01,  6.7200e-02],\n",
       "          [-6.0059e-02,  1.0303e+00,  1.2451e+00,  ..., -7.4951e-02,\n",
       "            1.5420e+00, -1.0449e+00],\n",
       "          [-7.1582e-01,  1.4316e+00,  1.3438e+00,  ...,  1.1188e-01,\n",
       "            6.4600e-01,  1.1055e+00],\n",
       "          ...,\n",
       "          [-2.8926e+00,  1.2344e+00, -5.8154e-01,  ...,  2.4429e-02,\n",
       "            4.1895e-01,  4.3320e+00],\n",
       "          [ 2.8955e-01,  7.7002e-01,  7.0459e-01,  ...,  1.6647e-02,\n",
       "            8.4570e-01,  7.2656e-01],\n",
       "          [ 2.5273e+00, -2.8066e+00,  2.5488e-01,  ...,  8.7842e-01,\n",
       "            3.8164e+00,  3.1586e-02]],\n",
       "\n",
       "         [[ 4.2038e-03,  3.7506e-02, -1.5366e-02,  ...,  1.4111e-01,\n",
       "           -1.1841e-01, -5.9601e-02],\n",
       "          [ 1.9121e+00, -9.8730e-01, -1.3184e-01,  ...,  5.4102e-01,\n",
       "            4.5947e-01,  3.5229e-01],\n",
       "          [ 6.7041e-01, -8.8965e-01, -8.3252e-01,  ..., -3.1602e+00,\n",
       "           -6.8945e-01,  1.0449e-01],\n",
       "          ...,\n",
       "          [-2.5430e+00, -2.2227e+00,  6.1914e-01,  ..., -6.7920e-01,\n",
       "           -8.6719e-01,  1.2585e-01],\n",
       "          [-1.5176e+00, -2.8271e-01,  8.7549e-01,  ..., -1.0469e+00,\n",
       "           -1.4023e+00, -1.6680e+00],\n",
       "          [ 1.2852e+00,  1.2803e+00,  9.1113e-01,  ..., -8.5547e-01,\n",
       "           -3.4277e+00,  1.9502e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3489e-02,  4.6310e-03, -2.5955e-02,  ...,  3.1543e-01,\n",
       "            1.5454e-01,  3.9368e-02],\n",
       "          [-1.1328e+00,  1.2236e+00, -3.2617e-01,  ...,  5.5225e-01,\n",
       "           -1.4883e+00,  1.8691e+00],\n",
       "          [ 1.8835e-01,  1.0557e+00, -4.2627e-01,  ..., -2.2656e-01,\n",
       "           -2.1133e+00,  1.0156e+00],\n",
       "          ...,\n",
       "          [ 7.4170e-01,  1.4219e+00,  1.7932e-01,  ..., -3.2227e+00,\n",
       "            4.9512e-01,  9.8633e-02],\n",
       "          [ 5.1904e-01,  6.6797e-01,  1.1768e+00,  ..., -3.3667e-01,\n",
       "            3.2080e-01,  1.5234e-01],\n",
       "          [-1.5352e+00,  1.5479e-01,  2.0859e+00,  ...,  2.6465e-01,\n",
       "            2.9004e-01, -9.2676e-01]],\n",
       "\n",
       "         [[-2.8503e-02, -3.8391e-02,  4.6806e-03,  ..., -4.7754e-01,\n",
       "           -4.8267e-01, -8.9697e-01],\n",
       "          [ 1.1055e+00, -1.2158e-01,  4.2993e-01,  ...,  5.7129e-02,\n",
       "           -1.2578e+00, -1.2578e+00],\n",
       "          [ 1.7021e+00,  2.1777e-01, -9.5850e-01,  ...,  1.4014e+00,\n",
       "            7.3145e-01, -1.8398e+00],\n",
       "          ...,\n",
       "          [-2.3398e+00, -7.4072e-01,  1.0229e-01,  ...,  2.8574e+00,\n",
       "           -2.1289e+00,  3.3545e-01],\n",
       "          [-2.8965e+00,  8.3789e-01,  1.6016e+00,  ...,  8.6963e-01,\n",
       "           -7.5293e-01,  2.7734e+00],\n",
       "          [ 2.8809e-02,  1.3320e+00,  7.6611e-01,  ...,  1.4375e+00,\n",
       "           -1.0391e+00,  1.7402e+00]],\n",
       "\n",
       "         [[-6.7444e-03, -1.9028e-02, -8.8730e-03,  ...,  3.7109e-01,\n",
       "            1.9395e+00,  1.8271e+00],\n",
       "          [-2.7539e-01, -1.5527e-01,  1.9165e-01,  ...,  9.0820e-01,\n",
       "           -3.3281e+00, -4.0820e+00],\n",
       "          [-3.4570e-01,  3.5645e-02, -1.8774e-01,  ...,  5.5811e-01,\n",
       "           -3.0977e+00, -5.5039e+00],\n",
       "          ...,\n",
       "          [-6.4453e-02,  5.4688e-02,  4.0234e-01,  ...,  1.0339e-01,\n",
       "           -4.4023e+00, -3.3262e+00],\n",
       "          [-1.9867e-02,  5.9473e-01, -2.9126e-01,  ..., -5.4785e-01,\n",
       "           -3.8730e+00, -3.5527e+00],\n",
       "          [-2.4097e-01,  3.0151e-01,  4.7211e-02,  ..., -1.7949e+00,\n",
       "           -4.4531e+00, -4.7969e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0956e-02,  2.1988e-02, -8.9722e-03,  ..., -4.0576e-01,\n",
       "            6.7993e-02, -6.7773e-01],\n",
       "          [-5.3613e-01,  6.2012e-02, -7.6953e-01,  ..., -1.5312e+00,\n",
       "            2.2339e-02, -1.4999e-02],\n",
       "          [-1.6250e+00, -1.5557e+00, -1.5781e+00,  ..., -1.6016e+00,\n",
       "           -1.8418e+00,  3.4058e-01],\n",
       "          ...,\n",
       "          [-8.6963e-01,  6.6553e-01, -2.8809e-01,  ...,  4.9634e-01,\n",
       "           -7.9395e-01,  9.1309e-02],\n",
       "          [ 1.3330e+00, -6.8604e-01, -1.0508e+00,  ...,  5.5859e-01,\n",
       "           -7.4854e-01,  1.1133e-01],\n",
       "          [ 2.4297e+00, -1.4707e+00, -1.5215e+00,  ...,  7.7148e-01,\n",
       "           -8.6035e-01,  1.9775e-01]],\n",
       "\n",
       "         [[-1.2207e-01, -7.2021e-02, -6.4697e-02,  ..., -1.0437e-02,\n",
       "            1.8506e-01,  6.7200e-02],\n",
       "          [-6.0059e-02,  1.0303e+00,  1.2451e+00,  ..., -7.4951e-02,\n",
       "            1.5420e+00, -1.0449e+00],\n",
       "          [-1.9854e+00,  1.7529e+00,  1.6816e+00,  ...,  9.9414e-01,\n",
       "            8.0420e-01,  5.8252e-01],\n",
       "          ...,\n",
       "          [-1.9219e+00,  1.1211e+00,  1.9885e-01,  ...,  5.7495e-02,\n",
       "            2.2070e+00,  3.0566e-01],\n",
       "          [-2.5537e-01,  4.3243e-02, -1.5857e-01,  ...,  3.6049e-03,\n",
       "            2.2988e+00,  3.9624e-01],\n",
       "          [ 1.8984e+00, -1.1729e+00, -3.9941e-01,  ...,  1.7676e-01,\n",
       "            2.2188e+00,  3.6987e-01]],\n",
       "\n",
       "         [[ 4.2038e-03,  3.7506e-02, -1.5366e-02,  ...,  1.4111e-01,\n",
       "           -1.1841e-01, -5.9601e-02],\n",
       "          [ 1.9121e+00, -9.8730e-01, -1.3184e-01,  ...,  5.4102e-01,\n",
       "            4.5947e-01,  3.5229e-01],\n",
       "          [ 7.8418e-01, -7.5098e-01, -8.8135e-01,  ..., -1.1406e+00,\n",
       "           -5.9277e-01, -8.5352e-01],\n",
       "          ...,\n",
       "          [-1.8926e+00, -1.4141e+00,  8.7012e-01,  ...,  1.0400e+00,\n",
       "           -2.3789e+00,  1.9697e+00],\n",
       "          [-4.2139e-01, -8.0713e-01,  9.5605e-01,  ...,  8.8916e-01,\n",
       "           -2.3301e+00,  1.9697e+00],\n",
       "          [ 1.4736e+00,  3.5840e-01,  7.0898e-01,  ...,  8.7598e-01,\n",
       "           -2.3691e+00,  1.9883e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3489e-02,  4.6310e-03, -2.5955e-02,  ...,  3.1543e-01,\n",
       "            1.5454e-01,  3.9368e-02],\n",
       "          [-1.1328e+00,  1.2236e+00, -3.2617e-01,  ...,  5.5225e-01,\n",
       "           -1.4883e+00,  1.8691e+00],\n",
       "          [ 1.1621e-01,  1.1006e+00, -7.2119e-01,  ...,  1.5684e+00,\n",
       "           -1.9434e+00,  2.8906e-01],\n",
       "          ...,\n",
       "          [ 6.2451e-01,  6.2646e-01,  2.8833e-01,  ..., -1.2920e+00,\n",
       "            4.5605e-01, -1.3721e+00],\n",
       "          [ 3.1152e-01,  8.0225e-01,  7.8613e-01,  ..., -1.1680e+00,\n",
       "            4.6826e-01, -1.3076e+00],\n",
       "          [-4.4629e-01,  3.9355e-01,  1.1035e+00,  ..., -1.4043e+00,\n",
       "            6.0791e-01, -1.3203e+00]],\n",
       "\n",
       "         [[-2.8503e-02, -3.8391e-02,  4.6806e-03,  ..., -4.7754e-01,\n",
       "           -4.8267e-01, -8.9697e-01],\n",
       "          [ 1.1055e+00, -1.2158e-01,  4.2993e-01,  ...,  5.7129e-02,\n",
       "           -1.2578e+00, -1.2578e+00],\n",
       "          [ 1.9961e+00,  1.6064e-01, -2.4585e-01,  ..., -1.2031e+00,\n",
       "            6.0205e-01, -1.5371e+00],\n",
       "          ...,\n",
       "          [-1.8008e+00, -1.1309e+00, -2.3584e-01,  ...,  9.1064e-01,\n",
       "           -1.5371e+00,  1.5781e+00],\n",
       "          [-1.7129e+00,  5.8496e-01,  6.0742e-01,  ...,  8.0566e-01,\n",
       "           -1.4629e+00,  1.6035e+00],\n",
       "          [-1.6797e-01,  1.7842e+00,  1.2334e+00,  ...,  9.8535e-01,\n",
       "           -1.5518e+00,  1.5898e+00]],\n",
       "\n",
       "         [[-6.7444e-03, -1.9028e-02, -8.8730e-03,  ...,  3.7109e-01,\n",
       "            1.9395e+00,  1.8271e+00],\n",
       "          [-2.7539e-01, -1.5527e-01,  1.9165e-01,  ...,  9.0820e-01,\n",
       "           -3.3281e+00, -4.0820e+00],\n",
       "          [-7.4902e-01,  2.8979e-01,  4.6387e-02,  ..., -8.9355e-01,\n",
       "           -2.2832e+00, -4.3984e+00],\n",
       "          ...,\n",
       "          [-7.9468e-02, -3.2739e-01,  9.1797e-01,  ..., -8.4229e-01,\n",
       "           -3.8535e+00, -4.3906e+00],\n",
       "          [-4.2407e-01, -1.7517e-01,  5.3027e-01,  ..., -7.4512e-01,\n",
       "           -3.8730e+00, -4.3125e+00],\n",
       "          [-3.1348e-01,  9.4727e-02, -8.3984e-02,  ..., -9.7607e-01,\n",
       "           -3.9336e+00, -4.2812e+00]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-0.0278,  0.0236, -0.0109,  ...,  0.0205, -0.0267, -0.0184],\n",
       "          [-0.1241,  0.4246, -0.0232,  ..., -0.0865, -0.1376,  0.0735],\n",
       "          [ 0.0519, -0.0964, -0.6255,  ..., -0.3059,  0.5684, -0.9434],\n",
       "          ...,\n",
       "          [-0.4661, -0.2886,  0.3557,  ...,  0.2590, -0.0541,  0.3157],\n",
       "          [-0.5044,  0.1730,  0.2021,  ...,  0.0452,  0.4424, -0.0308],\n",
       "          [-0.1704,  0.4492,  0.0629,  ...,  0.1780,  0.0580,  0.1484]],\n",
       "\n",
       "         [[ 0.0372, -0.0269, -0.0127,  ..., -0.0345, -0.0161,  0.0088],\n",
       "          [-0.0371,  0.0919, -0.7402,  ...,  0.4402, -0.5884, -0.0760],\n",
       "          [-0.9697,  0.4504, -0.7646,  ...,  0.6602, -0.5928, -0.4097],\n",
       "          ...,\n",
       "          [ 0.3928,  0.3984, -0.5132,  ...,  0.4612,  0.0723,  0.1362],\n",
       "          [-0.0385, -0.6460, -0.0663,  ...,  0.6265, -0.0361,  0.3076],\n",
       "          [ 0.4309,  0.2181,  0.6499,  ...,  0.2188,  0.1313, -0.1464]],\n",
       "\n",
       "         [[ 0.0051,  0.0073,  0.0228,  ...,  0.0080,  0.0050,  0.0143],\n",
       "          [ 0.0467, -0.0103, -0.0538,  ..., -0.1824,  0.0037, -0.2656],\n",
       "          [-0.6162,  0.6709, -0.5210,  ..., -0.2698, -0.2467, -0.7832],\n",
       "          ...,\n",
       "          [ 0.2229, -0.1067, -0.1075,  ..., -0.0345, -0.0958,  0.3706],\n",
       "          [-0.1406, -0.1021,  0.1312,  ...,  0.0520, -0.0182,  0.6719],\n",
       "          [ 0.0920,  0.0067,  0.1635,  ...,  0.1014,  0.1832, -0.2598]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0056,  0.0036, -0.0334,  ..., -0.0040,  0.0337,  0.0135],\n",
       "          [ 0.5415, -0.6455,  0.2001,  ..., -0.2229, -0.0965, -0.1531],\n",
       "          [-0.1644,  0.3081,  0.4170,  ..., -0.2827,  0.7344, -0.2595],\n",
       "          ...,\n",
       "          [ 0.3950,  0.8369,  0.4004,  ...,  0.2605, -1.1865,  0.1616],\n",
       "          [-0.4158,  0.2028,  0.5503,  ...,  0.2957, -0.7861,  0.0437],\n",
       "          [ 0.0092,  0.5195,  0.1105,  ...,  0.2385,  0.1250,  0.2812]],\n",
       "\n",
       "         [[-0.8867, -0.0598, -0.0521,  ..., -0.0756, -0.0498,  0.0448],\n",
       "          [ 0.8555, -0.4473,  0.1631,  ...,  0.4563,  0.4985,  0.1909],\n",
       "          [ 0.3264, -0.3655, -0.2769,  ..., -0.1479, -0.1598, -0.1738],\n",
       "          ...,\n",
       "          [ 0.2490,  0.1895, -0.3530,  ..., -0.1406, -0.6890,  0.3718],\n",
       "          [ 0.7759,  0.1644, -0.0638,  ..., -0.0914, -0.4663, -0.0293],\n",
       "          [ 0.7544,  0.4351, -0.2498,  ...,  0.0344,  0.2578,  0.0267]],\n",
       "\n",
       "         [[-0.0151, -0.0163,  0.0105,  ...,  0.0317, -0.0068,  0.0046],\n",
       "          [-0.3552,  0.6699, -0.9116,  ...,  1.1221, -1.2422,  0.1473],\n",
       "          [ 0.1348, -0.2559, -0.9556,  ...,  0.2494,  1.0020,  0.9355],\n",
       "          ...,\n",
       "          [-0.2700,  0.0911,  0.1772,  ...,  0.2856, -0.3469, -0.0277],\n",
       "          [-0.3411, -0.2076, -0.0166,  ...,  0.8696, -0.0830, -0.3098],\n",
       "          [-0.3926, -0.8472, -0.1182,  ...,  0.2749, -0.3594,  0.1239]]],\n",
       "\n",
       "\n",
       "        [[[-0.0278,  0.0236, -0.0109,  ...,  0.0205, -0.0267, -0.0184],\n",
       "          [-0.1241,  0.4246, -0.0232,  ..., -0.0865, -0.1376,  0.0735],\n",
       "          [ 0.4441,  0.7832,  1.1572,  ...,  0.0807,  0.5649,  0.3416],\n",
       "          ...,\n",
       "          [ 0.1299,  0.4607,  0.1836,  ...,  0.3374,  0.2415,  0.0854],\n",
       "          [ 0.3567,  0.3928,  0.0710,  ...,  0.3696,  0.2878,  0.1115],\n",
       "          [ 0.3489,  0.4434, -0.0479,  ...,  0.2881,  0.2749,  0.0632]],\n",
       "\n",
       "         [[ 0.0372, -0.0269, -0.0127,  ..., -0.0345, -0.0161,  0.0088],\n",
       "          [-0.0371,  0.0919, -0.7402,  ...,  0.4402, -0.5884, -0.0760],\n",
       "          [-1.4766, -0.4812, -0.9399,  ...,  0.6558,  0.5542,  0.2932],\n",
       "          ...,\n",
       "          [ 0.1409, -0.3108,  0.5347,  ...,  0.0195, -0.1096, -0.0662],\n",
       "          [ 0.1436, -0.2688,  0.5356,  ...,  0.0045, -0.1141,  0.0847],\n",
       "          [ 0.1648, -0.2717,  0.4751,  ...,  0.0395, -0.0494, -0.0342]],\n",
       "\n",
       "         [[ 0.0051,  0.0073,  0.0228,  ...,  0.0080,  0.0050,  0.0143],\n",
       "          [ 0.0467, -0.0103, -0.0538,  ..., -0.1824,  0.0037, -0.2656],\n",
       "          [ 0.3325, -0.5547,  0.2292,  ...,  0.3467, -0.5664,  0.0707],\n",
       "          ...,\n",
       "          [-0.0969,  0.6421,  0.0697,  ..., -0.0607, -0.2712,  0.1508],\n",
       "          [-0.1154,  0.6470,  0.1316,  ..., -0.0345, -0.2396,  0.1595],\n",
       "          [-0.0164,  0.5952,  0.0661,  ..., -0.0471, -0.1633,  0.1660]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0056,  0.0036, -0.0334,  ..., -0.0040,  0.0337,  0.0135],\n",
       "          [ 0.5415, -0.6455,  0.2001,  ..., -0.2229, -0.0965, -0.1531],\n",
       "          [-0.3428, -0.5190, -0.0677,  ..., -0.8765, -0.0232, -0.2717],\n",
       "          ...,\n",
       "          [-0.4001,  0.3650,  0.0761,  ..., -0.0251, -0.3945, -0.1758],\n",
       "          [-0.4602,  0.4688,  0.0299,  ..., -0.1279, -0.3923, -0.0977],\n",
       "          [-0.2749,  0.3789,  0.0177,  ..., -0.2681, -0.4250, -0.1509]],\n",
       "\n",
       "         [[-0.8867, -0.0598, -0.0521,  ..., -0.0756, -0.0498,  0.0448],\n",
       "          [ 0.8555, -0.4473,  0.1631,  ...,  0.4563,  0.4985,  0.1909],\n",
       "          [ 0.9043, -0.8770,  0.6616,  ..., -0.0900,  0.5591,  0.1656],\n",
       "          ...,\n",
       "          [ 1.0645,  0.1250,  0.1742,  ...,  0.3569, -0.3340, -0.2620],\n",
       "          [ 1.1064,  0.1769,  0.2429,  ...,  0.3794, -0.1986, -0.1897],\n",
       "          [ 1.1289,  0.1985,  0.1423,  ...,  0.4299, -0.1793, -0.2793]],\n",
       "\n",
       "         [[-0.0151, -0.0163,  0.0105,  ...,  0.0317, -0.0068,  0.0046],\n",
       "          [-0.3552,  0.6699, -0.9116,  ...,  1.1221, -1.2422,  0.1473],\n",
       "          [-1.2500, -1.1289,  0.4778,  ..., -0.9395,  1.9844,  1.4414],\n",
       "          ...,\n",
       "          [-0.6108, -1.2383,  0.4773,  ..., -0.3003, -0.2678,  0.5215],\n",
       "          [-0.6001, -1.0732,  0.4646,  ..., -0.4045, -0.3181,  0.6211],\n",
       "          [-0.5283, -1.0449,  0.4680,  ..., -0.3303, -0.2174,  0.6621]]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>)), (tensor([[[[-2.3972e-02,  8.2703e-02, -2.7725e-02,  ..., -2.6221e-01,\n",
       "           -4.4067e-02,  1.8506e-01],\n",
       "          [-1.6523e+00,  1.3711e+00, -2.3125e+00,  ...,  2.8394e-01,\n",
       "           -1.0078e+00,  1.9495e-01],\n",
       "          [-4.4766e+00, -1.6865e+00,  8.2129e-01,  ...,  5.4785e-01,\n",
       "           -4.3274e-02,  3.0029e-01],\n",
       "          ...,\n",
       "          [ 2.8184e+00,  1.2266e+00,  3.0273e-02,  ..., -3.5522e-01,\n",
       "            4.1040e-01,  3.8843e-01],\n",
       "          [ 2.7637e+00, -1.0283e+00,  1.9541e+00,  ..., -7.3535e-01,\n",
       "            1.5479e-01,  7.6172e-01],\n",
       "          [ 1.4277e+00, -3.2441e+00, -1.3086e+00,  ...,  6.1963e-01,\n",
       "           -5.3955e-01, -2.3819e-02]],\n",
       "\n",
       "         [[-1.3199e-03, -2.3178e-02,  7.3242e-04,  ..., -2.7905e-01,\n",
       "            2.9419e-01, -5.7715e-01],\n",
       "          [ 1.6191e+00, -1.2266e+00,  5.7617e-01,  ..., -1.1387e+00,\n",
       "           -1.5112e-01, -1.4736e+00],\n",
       "          [ 4.0195e+00, -2.1406e+00,  1.6016e+00,  ..., -3.1885e-01,\n",
       "            1.4980e+00, -2.1289e+00],\n",
       "          ...,\n",
       "          [-1.1758e+00, -3.1152e+00, -3.0410e+00,  ...,  5.8789e-01,\n",
       "           -1.0146e+00,  4.5312e-01],\n",
       "          [-2.7031e+00, -1.1943e+00, -2.9668e+00,  ..., -1.2100e+00,\n",
       "            1.5879e+00,  1.5830e+00],\n",
       "          [-1.8945e+00, -1.6006e+00, -1.2324e+00,  ...,  1.7615e-01,\n",
       "            2.7954e-01, -3.0615e-01]],\n",
       "\n",
       "         [[-5.3131e-02,  1.3321e-02,  9.5673e-03,  ...,  2.2388e-01,\n",
       "           -2.0959e-01,  3.9258e-01],\n",
       "          [-2.2969e+00,  2.7383e+00,  1.2861e+00,  ..., -3.8135e-01,\n",
       "           -2.8027e+00,  8.7061e-01],\n",
       "          [ 2.1250e+00,  2.8496e+00,  1.7783e+00,  ...,  9.3311e-01,\n",
       "           -1.0098e+00,  8.3984e-01],\n",
       "          ...,\n",
       "          [ 2.4824e+00,  3.3711e+00, -2.2246e+00,  ..., -2.4670e-01,\n",
       "            2.5391e-01,  9.6338e-01],\n",
       "          [ 1.7090e-02,  3.7129e+00, -2.1445e+00,  ..., -6.1865e-01,\n",
       "           -1.0889e+00,  1.6602e+00],\n",
       "          [-4.4062e+00,  1.6338e+00,  6.2109e-01,  ...,  4.2773e-01,\n",
       "           -4.8950e-01,  1.2861e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.8757e-02,  5.4749e-02,  1.9501e-02,  ..., -1.8384e-01,\n",
       "           -1.9775e+00,  7.4609e-01],\n",
       "          [-6.2109e-01, -5.3906e-01,  1.7822e-01,  ..., -7.1387e-01,\n",
       "            4.3398e+00, -1.8174e+00],\n",
       "          [-7.8857e-01, -6.2207e-01,  3.0469e-01,  ...,  5.3467e-01,\n",
       "            7.0469e+00,  5.0098e-01],\n",
       "          ...,\n",
       "          [ 1.5088e-01, -9.7217e-01, -1.0840e+00,  ..., -9.3750e-01,\n",
       "            4.1797e+00, -6.6357e-01],\n",
       "          [ 3.6694e-01, -8.6914e-01,  4.4678e-02,  ...,  5.5908e-01,\n",
       "            4.4023e+00, -1.1289e+00],\n",
       "          [ 4.9316e-02,  1.4160e-02,  7.2559e-01,  ...,  1.1074e+00,\n",
       "            6.1289e+00,  7.1387e-01]],\n",
       "\n",
       "         [[-1.3687e-02, -6.7200e-02, -6.8237e-02,  ...,  5.9082e-01,\n",
       "           -8.1641e-01, -2.4365e-01],\n",
       "          [ 2.3301e+00, -1.6582e+00,  5.5713e-01,  ...,  1.2578e+00,\n",
       "            3.4043e+00, -9.3555e-01],\n",
       "          [ 4.3125e+00, -4.0771e-01, -1.0293e+00,  ...,  1.1045e+00,\n",
       "            2.9160e+00, -1.2783e+00],\n",
       "          ...,\n",
       "          [-2.7012e+00, -1.6172e+00, -1.0488e+00,  ...,  1.5986e+00,\n",
       "            3.4531e+00,  1.8201e-01],\n",
       "          [-2.6992e+00,  3.5205e-01,  7.5732e-01,  ...,  2.3816e-01,\n",
       "            3.1953e+00,  5.1270e-01],\n",
       "          [-6.5430e-01,  2.2695e+00, -6.7480e-01,  ...,  5.3857e-01,\n",
       "            3.7734e+00,  6.7969e-01]],\n",
       "\n",
       "         [[ 8.4412e-02,  3.3020e-02,  1.7563e-02,  ..., -4.9487e-01,\n",
       "            1.1074e+00, -6.4844e-01],\n",
       "          [ 1.7207e+00, -4.4336e-01,  1.0166e+00,  ...,  2.5049e-01,\n",
       "           -9.0625e-01,  3.9868e-01],\n",
       "          [ 2.3438e+00,  1.3623e+00,  2.1953e+00,  ...,  3.3081e-02,\n",
       "           -2.1816e+00,  2.6416e-01],\n",
       "          ...,\n",
       "          [-3.1367e+00,  7.5000e-01, -2.8438e+00,  ...,  1.3662e+00,\n",
       "           -2.9121e+00,  8.0322e-01],\n",
       "          [-1.2588e+00, -9.0918e-01, -1.4434e+00,  ...,  1.7275e+00,\n",
       "           -2.3242e+00,  1.3164e+00],\n",
       "          [ 1.6162e-01, -5.2686e-01, -6.3574e-01,  ...,  1.6104e+00,\n",
       "           -2.0762e+00, -3.4375e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.3972e-02,  8.2703e-02, -2.7725e-02,  ..., -2.6221e-01,\n",
       "           -4.4067e-02,  1.8506e-01],\n",
       "          [-1.6523e+00,  1.3711e+00, -2.3125e+00,  ...,  2.8394e-01,\n",
       "           -1.0078e+00,  1.9495e-01],\n",
       "          [-4.8945e+00, -1.3848e+00,  5.9082e-01,  ...,  2.1753e-01,\n",
       "            4.8218e-02,  2.9712e-01],\n",
       "          ...,\n",
       "          [ 1.8867e+00,  2.2539e+00,  8.2227e-01,  ...,  1.5547e+00,\n",
       "            4.0405e-02, -1.7761e-01],\n",
       "          [ 2.6016e+00, -6.1951e-02,  1.6907e-01,  ...,  1.6094e+00,\n",
       "            1.5259e-01, -2.2864e-01],\n",
       "          [ 8.5547e-01, -2.2480e+00, -3.9746e-01,  ...,  1.5400e+00,\n",
       "            2.1594e-01, -4.0698e-01]],\n",
       "\n",
       "         [[-1.3199e-03, -2.3178e-02,  7.3242e-04,  ..., -2.7905e-01,\n",
       "            2.9419e-01, -5.7715e-01],\n",
       "          [ 1.6191e+00, -1.2266e+00,  5.7617e-01,  ..., -1.1387e+00,\n",
       "           -1.5112e-01, -1.4736e+00],\n",
       "          [ 4.0742e+00, -2.1660e+00,  1.8223e+00,  ..., -6.6113e-01,\n",
       "            1.8584e+00, -2.6465e+00],\n",
       "          ...,\n",
       "          [-1.0645e-01, -1.3008e+00, -1.7607e+00,  ..., -8.5742e-01,\n",
       "            1.6699e+00, -1.6089e-01],\n",
       "          [-1.8916e+00, -1.1045e+00, -2.2344e+00,  ..., -1.0947e+00,\n",
       "            1.7861e+00, -2.1362e-01],\n",
       "          [-1.8496e+00, -3.3887e-01, -1.8281e+00,  ..., -1.0107e+00,\n",
       "            1.8750e+00, -2.6953e-01]],\n",
       "\n",
       "         [[-5.3131e-02,  1.3321e-02,  9.5673e-03,  ...,  2.2388e-01,\n",
       "           -2.0959e-01,  3.9258e-01],\n",
       "          [-2.2969e+00,  2.7383e+00,  1.2861e+00,  ..., -3.8135e-01,\n",
       "           -2.8027e+00,  8.7061e-01],\n",
       "          [ 2.1016e+00,  3.2227e+00,  1.6494e+00,  ...,  2.6709e-01,\n",
       "           -5.0293e-01, -1.0266e-01],\n",
       "          ...,\n",
       "          [ 3.3516e+00,  3.2461e+00, -1.9531e+00,  ..., -2.5635e-01,\n",
       "           -2.0039e+00,  1.5176e+00],\n",
       "          [-2.8638e-01,  3.4746e+00, -1.3789e+00,  ..., -3.5107e-01,\n",
       "           -2.0449e+00,  1.4482e+00],\n",
       "          [-3.8164e+00,  1.5498e+00, -3.1836e-01,  ..., -2.8979e-01,\n",
       "           -1.9346e+00,  1.4873e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.8757e-02,  5.4749e-02,  1.9501e-02,  ..., -1.8384e-01,\n",
       "           -1.9775e+00,  7.4609e-01],\n",
       "          [-6.2109e-01, -5.3906e-01,  1.7822e-01,  ..., -7.1387e-01,\n",
       "            4.3398e+00, -1.8174e+00],\n",
       "          [-9.4873e-01, -5.1123e-01,  2.4634e-01,  ...,  3.5889e-01,\n",
       "            4.9844e+00, -2.2129e+00],\n",
       "          ...,\n",
       "          [ 9.5581e-02, -9.6094e-01, -1.0098e+00,  ...,  1.1836e+00,\n",
       "            4.6680e+00, -1.7114e-01],\n",
       "          [ 5.3516e-01, -9.1162e-01, -5.3809e-01,  ...,  1.1816e+00,\n",
       "            4.6602e+00, -2.1924e-01],\n",
       "          [ 4.0771e-01, -3.5132e-01,  1.7529e-01,  ...,  1.2998e+00,\n",
       "            4.6562e+00, -2.2461e-01]],\n",
       "\n",
       "         [[-1.3687e-02, -6.7200e-02, -6.8237e-02,  ...,  5.9082e-01,\n",
       "           -8.1641e-01, -2.4365e-01],\n",
       "          [ 2.3301e+00, -1.6582e+00,  5.5713e-01,  ...,  1.2578e+00,\n",
       "            3.4043e+00, -9.3555e-01],\n",
       "          [ 4.4883e+00, -2.9053e-01, -9.6143e-01,  ...,  2.3987e-02,\n",
       "            3.0312e+00, -9.3750e-02],\n",
       "          ...,\n",
       "          [-2.2305e+00, -1.3271e+00, -1.9531e-02,  ...,  6.8542e-02,\n",
       "            3.1738e+00,  1.2976e-01],\n",
       "          [-2.8320e+00,  5.4492e-01,  4.3701e-01,  ...,  1.4587e-01,\n",
       "            3.1680e+00,  1.5088e-01],\n",
       "          [-7.2949e-01,  2.0859e+00,  6.1523e-01,  ...,  9.8267e-02,\n",
       "            3.0723e+00,  2.6904e-01]],\n",
       "\n",
       "         [[ 8.4412e-02,  3.3020e-02,  1.7563e-02,  ..., -4.9487e-01,\n",
       "            1.1074e+00, -6.4844e-01],\n",
       "          [ 1.7207e+00, -4.4336e-01,  1.0166e+00,  ...,  2.5049e-01,\n",
       "           -9.0625e-01,  3.9868e-01],\n",
       "          [ 2.3613e+00,  1.0371e+00,  2.6387e+00,  ..., -1.6797e+00,\n",
       "           -1.8730e+00,  2.8540e-01],\n",
       "          ...,\n",
       "          [-1.8008e+00, -6.2939e-01, -1.4707e+00,  ...,  1.4814e+00,\n",
       "           -7.2705e-01,  4.8248e-02],\n",
       "          [-9.8145e-01, -1.1455e+00, -1.3301e+00,  ...,  1.4248e+00,\n",
       "           -8.1348e-01,  6.0150e-02],\n",
       "          [ 6.7822e-01, -8.2910e-01, -6.8457e-01,  ...,  1.4883e+00,\n",
       "           -7.9590e-01,  7.9346e-02]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-2.0294e-02, -4.8035e-02,  1.5442e-02,  ..., -5.3406e-02,\n",
       "           -2.6566e-02,  2.0554e-02],\n",
       "          [-3.0859e-01, -2.1912e-01,  6.0059e-01,  ..., -2.6099e-01,\n",
       "           -4.3701e-01, -1.3696e-01],\n",
       "          [-5.6445e-01, -5.0879e-01, -1.7017e-01,  ..., -3.0640e-01,\n",
       "            3.5571e-01,  1.7102e-01],\n",
       "          ...,\n",
       "          [ 1.9507e-01,  1.3318e-01,  5.7129e-01,  ...,  3.8477e-01,\n",
       "            4.9487e-01,  2.4353e-01],\n",
       "          [ 4.3488e-02,  6.1719e-01,  1.0107e+00,  ..., -7.7087e-02,\n",
       "            2.4048e-01,  9.1064e-01],\n",
       "          [ 1.4929e-01, -4.2334e-01, -2.6172e-01,  ...,  1.7346e-01,\n",
       "           -4.7180e-02,  7.1826e-01]],\n",
       "\n",
       "         [[ 6.6681e-03, -1.4503e-02,  3.6964e-03,  ...,  1.5701e-02,\n",
       "           -4.4403e-02, -8.3130e-02],\n",
       "          [ 4.3823e-02, -1.6464e-02,  4.3481e-01,  ...,  2.7783e-01,\n",
       "            2.5903e-01,  1.0068e+00],\n",
       "          [ 1.0332e+00, -3.2806e-02, -4.4629e-01,  ...,  2.9028e-01,\n",
       "            3.1372e-01,  5.1221e-01],\n",
       "          ...,\n",
       "          [-1.0999e-01, -1.6858e-01, -4.8120e-01,  ..., -3.4961e-01,\n",
       "           -1.9604e-01,  8.6133e-01],\n",
       "          [ 9.7595e-02, -4.1235e-01, -1.9214e-01,  ...,  2.2125e-04,\n",
       "            1.2036e-01,  1.0039e+00],\n",
       "          [ 1.1102e-01, -1.8286e-01, -3.3325e-01,  ..., -2.0801e-01,\n",
       "            1.8359e-01, -4.9072e-02]],\n",
       "\n",
       "         [[ 2.8809e-02, -1.5823e-02,  9.6924e-02,  ...,  7.5806e-02,\n",
       "           -1.8921e-02,  6.1554e-02],\n",
       "          [-1.9360e-01, -1.3794e-01,  1.6479e-02,  ..., -4.1016e-01,\n",
       "            3.6890e-01, -1.4075e-01],\n",
       "          [ 3.9453e-01, -6.8652e-01,  2.6221e-01,  ..., -3.8818e-01,\n",
       "            3.8147e-02,  6.7334e-01],\n",
       "          ...,\n",
       "          [ 6.1523e-01, -3.3105e-01, -5.5176e-01,  ..., -5.1807e-01,\n",
       "           -3.1299e-01,  3.8452e-03],\n",
       "          [ 3.7793e-01, -5.6250e-01, -1.9312e-01,  ..., -1.1211e+00,\n",
       "            4.9585e-01, -3.9014e-01],\n",
       "          [ 9.1370e-02,  2.0264e-02,  1.5674e-01,  ..., -4.2920e-01,\n",
       "            2.9199e-01,  5.6213e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.3666e-02,  5.7106e-03, -5.2643e-02,  ...,  7.6294e-02,\n",
       "            8.1329e-03,  5.6915e-02],\n",
       "          [ 1.3464e-01, -3.4521e-01,  3.2251e-01,  ...,  2.5635e-01,\n",
       "           -3.4497e-01,  5.0537e-01],\n",
       "          [-2.4695e-01,  1.2549e-01, -5.3613e-01,  ...,  2.0923e-01,\n",
       "           -1.2070e+00,  1.7517e-02],\n",
       "          ...,\n",
       "          [ 4.8267e-01, -1.8201e-01,  5.6494e-01,  ..., -3.4814e-01,\n",
       "           -1.4717e+00, -7.0459e-01],\n",
       "          [ 2.9761e-01, -2.1240e-01,  4.2725e-03,  ...,  5.0439e-01,\n",
       "           -4.1168e-02, -2.8540e-01],\n",
       "          [ 2.8174e-01,  2.0691e-01, -5.0879e-01,  ..., -1.3408e+00,\n",
       "            3.3545e-01,  1.5625e-01]],\n",
       "\n",
       "         [[ 2.7328e-02, -9.3079e-03, -2.9404e-02,  ...,  2.6953e-01,\n",
       "            2.6581e-02,  1.3657e-02],\n",
       "          [ 9.0234e-01,  1.4929e-01,  4.8431e-02,  ..., -1.4092e+00,\n",
       "           -2.5757e-01, -5.8301e-01],\n",
       "          [-1.6614e-01, -5.8691e-01,  4.9854e-01,  ...,  3.5474e-01,\n",
       "            4.5703e-01, -7.9688e-01],\n",
       "          ...,\n",
       "          [-1.1055e+00,  6.4844e-01,  4.2920e-01,  ..., -6.3135e-01,\n",
       "            6.1914e-01, -4.4116e-01],\n",
       "          [-6.2744e-02, -2.8296e-01,  5.8252e-01,  ..., -2.2412e-01,\n",
       "            1.4893e+00, -1.1387e+00],\n",
       "          [ 2.9102e-01, -6.4502e-01, -1.6907e-02,  ...,  3.5815e-01,\n",
       "            1.9861e-01, -2.6221e-01]],\n",
       "\n",
       "         [[-2.7740e-02, -2.7237e-02, -2.1545e-02,  ..., -6.4812e-03,\n",
       "           -2.6169e-02,  2.5009e-02],\n",
       "          [ 1.0767e-01, -5.7812e-01, -4.6082e-02,  ...,  2.9907e-01,\n",
       "           -4.3530e-01,  5.7080e-01],\n",
       "          [ 1.8457e-01, -2.0996e-02,  1.4404e-01,  ...,  1.5112e-01,\n",
       "            1.6113e-02, -1.5186e-01],\n",
       "          ...,\n",
       "          [-5.8887e-01,  5.8319e-02,  3.7378e-01,  ...,  2.3218e-01,\n",
       "            4.1455e-01,  4.8633e-01],\n",
       "          [ 4.8901e-01, -2.0972e-01, -1.5356e-01,  ...,  1.5274e-02,\n",
       "            2.5879e-01,  4.8523e-03],\n",
       "          [ 3.0688e-01, -3.2324e-01, -3.3252e-01,  ..., -3.7622e-01,\n",
       "            1.8555e-01,  4.3628e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.0294e-02, -4.8035e-02,  1.5442e-02,  ..., -5.3406e-02,\n",
       "           -2.6566e-02,  2.0554e-02],\n",
       "          [-3.0859e-01, -2.1912e-01,  6.0059e-01,  ..., -2.6099e-01,\n",
       "           -4.3701e-01, -1.3696e-01],\n",
       "          [-4.2822e-01, -3.1616e-01,  2.5806e-01,  ..., -4.3335e-01,\n",
       "            2.9449e-02,  2.8638e-01],\n",
       "          ...,\n",
       "          [-2.2998e-01, -5.4596e-02,  4.9902e-01,  ...,  4.0942e-01,\n",
       "           -2.8320e-01,  6.5625e-01],\n",
       "          [-3.3008e-01, -3.7506e-02,  4.7900e-01,  ...,  3.7476e-01,\n",
       "           -2.9956e-01,  6.9092e-01],\n",
       "          [-3.4009e-01, -2.2583e-03,  4.1504e-01,  ...,  4.1235e-01,\n",
       "           -4.5508e-01,  6.4111e-01]],\n",
       "\n",
       "         [[ 6.6681e-03, -1.4503e-02,  3.6964e-03,  ...,  1.5701e-02,\n",
       "           -4.4403e-02, -8.3130e-02],\n",
       "          [ 4.3823e-02, -1.6464e-02,  4.3481e-01,  ...,  2.7783e-01,\n",
       "            2.5903e-01,  1.0068e+00],\n",
       "          [ 1.8896e-01, -3.3325e-01,  1.4880e-01,  ..., -2.2058e-01,\n",
       "           -2.6642e-02,  6.6040e-02],\n",
       "          ...,\n",
       "          [ 7.7686e-01, -4.9878e-01, -3.5815e-01,  ..., -4.1016e-01,\n",
       "            3.1104e-01,  3.3911e-01],\n",
       "          [ 8.6279e-01, -5.6055e-01, -2.6587e-01,  ..., -3.8184e-01,\n",
       "            3.0859e-01,  3.6890e-01],\n",
       "          [ 9.6143e-01, -5.5469e-01, -3.1641e-01,  ..., -3.7744e-01,\n",
       "            3.7671e-01,  3.4595e-01]],\n",
       "\n",
       "         [[ 2.8809e-02, -1.5823e-02,  9.6924e-02,  ...,  7.5806e-02,\n",
       "           -1.8921e-02,  6.1554e-02],\n",
       "          [-1.9360e-01, -1.3794e-01,  1.6479e-02,  ..., -4.1016e-01,\n",
       "            3.6890e-01, -1.4075e-01],\n",
       "          [-2.2656e-01, -3.9917e-02, -6.2927e-02,  ..., -3.1958e-01,\n",
       "           -4.4580e-01,  4.0234e-01],\n",
       "          ...,\n",
       "          [ 6.5137e-01, -1.6321e-01,  9.1309e-01,  ..., -1.3379e+00,\n",
       "            2.9199e-01, -3.7500e-01],\n",
       "          [ 7.8662e-01, -1.4062e-01,  9.6582e-01,  ..., -1.2383e+00,\n",
       "            4.2065e-01, -3.0493e-01],\n",
       "          [ 8.9795e-01, -2.1973e-01,  1.0195e+00,  ..., -1.2471e+00,\n",
       "            3.8062e-01, -2.9248e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.3666e-02,  5.7106e-03, -5.2643e-02,  ...,  7.6294e-02,\n",
       "            8.1329e-03,  5.6915e-02],\n",
       "          [ 1.3464e-01, -3.4521e-01,  3.2251e-01,  ...,  2.5635e-01,\n",
       "           -3.4497e-01,  5.0537e-01],\n",
       "          [-3.2324e-01,  9.4873e-01, -1.9443e+00,  ...,  1.7617e+00,\n",
       "            1.8779e+00,  8.4326e-01],\n",
       "          ...,\n",
       "          [-5.7178e-01,  4.5319e-02, -6.6553e-01,  ..., -7.7246e-01,\n",
       "            3.0640e-01,  2.7393e-01],\n",
       "          [-5.6201e-01,  3.3447e-02, -6.0742e-01,  ..., -6.5039e-01,\n",
       "            2.2156e-01,  3.6914e-01],\n",
       "          [-5.3369e-01,  1.4374e-02, -6.6797e-01,  ..., -6.9141e-01,\n",
       "            1.0034e-01,  3.4497e-01]],\n",
       "\n",
       "         [[ 2.7328e-02, -9.3079e-03, -2.9404e-02,  ...,  2.6953e-01,\n",
       "            2.6581e-02,  1.3657e-02],\n",
       "          [ 9.0234e-01,  1.4929e-01,  4.8431e-02,  ..., -1.4092e+00,\n",
       "           -2.5757e-01, -5.8301e-01],\n",
       "          [ 7.1484e-01,  2.2314e-01,  2.1082e-01,  ...,  3.6475e-01,\n",
       "            4.6814e-02, -7.9443e-01],\n",
       "          ...,\n",
       "          [ 2.6367e-01, -4.4092e-01, -2.4866e-01,  ..., -6.2744e-01,\n",
       "            7.2900e-01, -7.2852e-01],\n",
       "          [ 2.9468e-01, -3.9893e-01, -1.4746e-01,  ..., -5.2979e-01,\n",
       "            6.2500e-01, -6.1328e-01],\n",
       "          [ 2.1814e-01, -3.1934e-01, -1.7383e-01,  ..., -6.2305e-01,\n",
       "            6.2402e-01, -6.5967e-01]],\n",
       "\n",
       "         [[-2.7740e-02, -2.7237e-02, -2.1545e-02,  ..., -6.4812e-03,\n",
       "           -2.6169e-02,  2.5009e-02],\n",
       "          [ 1.0767e-01, -5.7812e-01, -4.6082e-02,  ...,  2.9907e-01,\n",
       "           -4.3530e-01,  5.7080e-01],\n",
       "          [ 8.5205e-02,  9.7412e-02,  4.6729e-01,  ...,  4.6704e-01,\n",
       "           -3.3838e-01, -8.3936e-01],\n",
       "          ...,\n",
       "          [ 3.6194e-02,  2.5977e-01,  1.6296e-02,  ..., -9.2651e-02,\n",
       "            6.0059e-01,  1.0699e-01],\n",
       "          [ 1.1462e-01,  2.3743e-01, -4.1595e-02,  ..., -4.5532e-02,\n",
       "            5.0781e-01,  3.9276e-02],\n",
       "          [ 1.9006e-01,  1.7236e-01, -2.3193e-03,  ..., -2.4780e-02,\n",
       "            5.7471e-01,  1.0645e-01]]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**ipt.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./chatbot\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    logging_steps=3,\n",
    "    num_train_epochs=1,\n",
    "    gradient_checkpointing=True,\n",
    "    # adam_epsilon=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ds.select(range(6000)),\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cb64afb4b54d4fa281e2456ebac993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9765, 'grad_norm': 0.24770013988018036, 'learning_rate': 4.96e-05, 'epoch': 0.01}\n",
      "{'loss': 2.0277, 'grad_norm': 0.26451408863067627, 'learning_rate': 4.92e-05, 'epoch': 0.02}\n",
      "{'loss': 1.9526, 'grad_norm': 0.3056473433971405, 'learning_rate': 4.88e-05, 'epoch': 0.02}\n",
      "{'loss': 1.8342, 'grad_norm': 0.3194566071033478, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 1.9551, 'grad_norm': 0.6033316254615784, 'learning_rate': 4.8e-05, 'epoch': 0.04}\n",
      "{'loss': 1.8741, 'grad_norm': 0.4865131676197052, 'learning_rate': 4.76e-05, 'epoch': 0.05}\n",
      "{'loss': 1.8919, 'grad_norm': 0.46757039427757263, 'learning_rate': 4.72e-05, 'epoch': 0.06}\n",
      "{'loss': 1.9173, 'grad_norm': 0.5499241948127747, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.06}\n",
      "{'loss': 1.9032, 'grad_norm': 0.520094096660614, 'learning_rate': 4.64e-05, 'epoch': 0.07}\n",
      "{'loss': 1.9409, 'grad_norm': 0.6254320740699768, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 1.9087, 'grad_norm': 0.671566903591156, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.09}\n",
      "{'loss': 1.9071, 'grad_norm': 0.5912526845932007, 'learning_rate': 4.52e-05, 'epoch': 0.1}\n",
      "{'loss': 1.8284, 'grad_norm': 0.5724835991859436, 'learning_rate': 4.4800000000000005e-05, 'epoch': 0.1}\n",
      "{'loss': 1.7822, 'grad_norm': 0.6593788862228394, 'learning_rate': 4.44e-05, 'epoch': 0.11}\n",
      "{'loss': 1.8053, 'grad_norm': 0.5163369178771973, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.12}\n",
      "{'loss': 1.7646, 'grad_norm': 0.8600656986236572, 'learning_rate': 4.36e-05, 'epoch': 0.13}\n",
      "{'loss': 1.8197, 'grad_norm': 0.5071626305580139, 'learning_rate': 4.32e-05, 'epoch': 0.14}\n",
      "{'loss': 1.7318, 'grad_norm': 0.509921133518219, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.14}\n",
      "{'loss': 1.7321, 'grad_norm': 0.6726789474487305, 'learning_rate': 4.24e-05, 'epoch': 0.15}\n",
      "{'loss': 1.7803, 'grad_norm': 0.6415629982948303, 'learning_rate': 4.2e-05, 'epoch': 0.16}\n",
      "{'loss': 1.8941, 'grad_norm': 0.5538920760154724, 'learning_rate': 4.16e-05, 'epoch': 0.17}\n",
      "{'loss': 1.9288, 'grad_norm': 0.6367887258529663, 'learning_rate': 4.12e-05, 'epoch': 0.18}\n",
      "{'loss': 1.8755, 'grad_norm': 0.8213152885437012, 'learning_rate': 4.08e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\transformers\\trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3612\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\accelerate\\accelerator.py:2241\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraModel(\n",
       "  (model): PeftModelForCausalLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): LlamaForCausalLM(\n",
       "        (model): LlamaModel(\n",
       "          (embed_tokens): Embedding(128256, 3072)\n",
       "          (layers): ModuleList(\n",
       "            (0): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (1): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (2): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (3): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (4): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (5): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (6): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (7): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (8): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (9): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (10): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (11): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (12): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (13): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (14): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (15): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (16): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (17): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (18): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (19): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (20): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (21): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (22): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (23): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (24): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (25): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (26): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "            (27): LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'get_lora_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\peft\\tuners\\lora\\model.py:360\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LoraModel' object has no attribute 'get_lora_params'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\peft\\peft_model.py:787\u001b[0m, in \u001b[0;36mPeftModel.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PeftModelForCausalLM' object has no attribute 'get_lora_params'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\peft\\tuners\\lora\\model.py:360\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LoraModel' object has no attribute 'get_lora_params'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lora_parameters \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lora_params\u001b[49m()\n",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\peft\\tuners\\lora\\model.py:364\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# see #1892: prevent infinite recursion if class is not initialized\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\peft\\peft_model.py:791\u001b[0m, in \u001b[0;36mPeftModel.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# see #1892: prevent infinite recursion if class is not initialized\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\peft\\tuners\\lora\\model.py:364\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# see #1892: prevent infinite recursion if class is not initialized\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\G\\miniforge3\\envs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'get_lora_params'"
     ]
    }
   ],
   "source": [
    "lora_parameters = model.get_lora_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (1): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (2): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (3): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (4): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (5): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (6): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (7): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (8): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (9): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (10): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (11): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (12): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (13): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (14): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (15): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (16): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (17): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (18): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (19): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (20): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (21): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (22): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (23): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (24): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (25): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (26): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "          (27): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user\\n\\n你好你是谁？\\nQuestion: assistant\\n\\nAnswer: 我是AI助手，叫做LLaMA，我可以帮助回答各种问题，提供信息和建议。如果您需要帮助，请问我可以帮助您什么？'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipt = tokenizer(\"<|start_header_id|>user<|end_header_id|>\\n\\n{}\\nQuestion: {}<|eot_id|>\\n\".format(\"你好你是谁？\",\"\").strip() + \"<|start_header_id|>assistant<|end_header_id|>\\n\\nAnswer:\", return_tensors=\"pt\").to(model.device)\n",
    "tokenizer.decode(model.generate(**ipt, max_length=512, do_sample=True,pad_token_id=tokenizer.pad_token_id,eos_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True)\n",
    "# 需要明确指定pad_token_id 是tokenizer的pad_token_id 不然它会自动换成eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
